{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Multi-Head Attention from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Multi-Head Attention (MHA) is the core mechanism of the Transformer architecture. It enables the model to **jointly attend** to information from different representation subspaces at different positions.\n",
    "\n",
    "### Background: Why Multiple Heads?\n",
    "\n",
    "Single-head attention computes one set of attention weights. But different parts of the input might benefit from different attention patterns:\n",
    "- One head might learn syntax (subject-verb agreement)\n",
    "- Another might learn semantics (word meaning relationships)\n",
    "- Another might learn positional patterns\n",
    "\n",
    "Multi-head attention runs multiple attention operations in parallel, each with its own learned projections.\n",
    "\n",
    "### The Math\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "### Key Dimensions\n",
    "\n",
    "- `d_model`: Total embedding dimension (e.g., 512)\n",
    "- `num_heads`: Number of attention heads (e.g., 8)\n",
    "- `d_head = d_model // num_heads`: Dimension per head (e.g., 64)\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: Mask creation (causal, padding, KV cache)\n",
    "2. **Part 2**: Core multi-head mechanism (Q, K, V given) - focus on the split/concat logic\n",
    "3. **Part 3**: Multi-Head Self-Attention - Q, K, V from projections of single input x\n",
    "4. **Part 4**: MHA with KV Cache for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Attention Mask Creation\n",
    "\n",
    "The mask functions are the same as single-head attention. For multi-head, the mask broadcasts across the head dimension.\n",
    "\n",
    "Mask convention: **True = masked (cannot attend), False = can attend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len_q: int, seq_len_k: int = None, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask for autoregressive models.\n",
    "    \n",
    "    For multi-head attention, this mask broadcasts across the head dimension.\n",
    "    Shape: (seq_len_q, seq_len_k) -> broadcasts to (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length (defaults to seq_len_q)\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "              True = position should be MASKED (cannot attend)\n",
    "              False = position can be attended to\n",
    "    \n",
    "    Example for seq_len=4:\n",
    "        Q\\\\K   0      1      2      3\n",
    "        0   [False, True,  True,  True ]   # Query 0 attends only to Key 0\n",
    "        1   [False, False, True,  True ]   # Query 1 attends to Keys 0-1\n",
    "        2   [False, False, False, True ]   # Query 2 attends to Keys 0-2\n",
    "        3   [False, False, False, False]   # Query 3 attends to Keys 0-3\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a padding mask for variable-length sequences.\n",
    "    Shape: (batch, max_len) -> needs reshaping for multi-head: (batch, 1, 1, max_len)\n",
    "    \n",
    "    Args:\n",
    "        lengths: Tensor of shape (batch_size,) containing actual sequence lengths\n",
    "        max_len: Maximum sequence length (padded length)\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (batch_size, max_len)\n",
    "              True = padding position (should be MASKED)\n",
    "              False = real token (can be attended to)\n",
    "    \n",
    "    Example:\n",
    "        lengths = [3, 5, 2], max_len = 5\n",
    "        \n",
    "        Returns:\n",
    "        [[False, False, False, True,  True ],   # seq 0: tokens 0-2 real, 3-4 padding\n",
    "         [False, False, False, False, False],   # seq 1: all 5 tokens real\n",
    "         [False, False, True,  True,  True ]]   # seq 2: tokens 0-1 real, 2-4 padding\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int = None,\n",
    "    is_causal: bool = True,\n",
    "    key_padding_lengths: torch.Tensor = None,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a combined attention mask for multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length\n",
    "        is_causal: Whether to apply causal masking\n",
    "        key_padding_lengths: If provided, actual lengths of key sequences (batch_size,)\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask with shape suitable for broadcasting to (batch, num_heads, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask_with_cache(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int,\n",
    "    cache_len: int,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for attention with KV cache.\n",
    "    New queries can attend to all cached positions.\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Number of new query tokens (typically 1 during generation)\n",
    "        seq_len_k: Total key length (cache_len + seq_len_q)\n",
    "        cache_len: Number of cached tokens\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "    \n",
    "    Example: cache_len=3, seq_len_q=2, seq_len_k=5\n",
    "        Keys:    [cached0, cached1, cached2, new0, new1]\n",
    "        \n",
    "        Q\\\\K      c0     c1     c2    new0   new1\n",
    "        new0  [False, False, False, False, True ]  # new0 attends to cache + itself\n",
    "        new1  [False, False, False, False, False]  # new1 attends to cache + new0 + itself\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mask creation\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal mask (4x4):\")\n",
    "print(mask)\n",
    "print(f\"\\nThis broadcasts to (batch, num_heads, 4, 4) in multi-head attention\")\n",
    "\n",
    "# Test padding mask for multi-head (needs extra dimensions)\n",
    "lengths = torch.tensor([4, 3])\n",
    "mask_combined = create_attention_mask(4, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"\\nCombined mask shape for multi-head: {mask_combined.shape}\")\n",
    "print(\"Broadcasts to (batch, num_heads, seq_q, seq_k)\")\n",
    "\n",
    "print(\"\\n✓ Mask tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core Multi-Head Mechanism\n",
    "\n",
    "First, implement multi-head attention assuming Q, K, V are already projected.\n",
    "This isolates the split-into-heads and concatenate logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "print(f\"d_model={d_model}, num_heads={num_heads}, d_head={d_head}\")\n",
    "print(f\"\\nEach head operates on {d_head} dimensions\")\n",
    "print(f\"All {num_heads} heads run in parallel, then concatenate back to {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_core(Q, K, V, num_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Core multi-head attention computation (Q, K, V already projected).\n",
    "    \n",
    "    This function takes already-projected Q, K, V and:\n",
    "    1. Splits them into multiple heads\n",
    "    2. Computes attention for each head in parallel\n",
    "    3. Concatenates the results\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Projected tensors of shape (batch, seq_len, d_model)\n",
    "        num_heads: Number of attention heads\n",
    "        mask: Optional boolean attention mask (True = masked)\n",
    "    \n",
    "    Returns:\n",
    "        output: Multi-head attention output (batch, seq_len, d_model)\n",
    "        attn_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "    \n",
    "    Hints:\n",
    "        - Use view and transpose to split: (batch, seq, d_model) -> (batch, num_heads, seq, d_head)\n",
    "        - Compute scaled dot-product attention for all heads in parallel\n",
    "        - Transpose and view to concatenate: (batch, num_heads, seq, d_head) -> (batch, seq, d_model)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the core mechanism with random Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = multi_head_attention_core(Q, K, V, num_heads)\n",
    "\n",
    "print(f\"Input Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  -> {num_heads} heads, each with {seq_len}x{seq_len} attention matrix\")\n",
    "\n",
    "assert output.shape == Q.shape\n",
    "print(\"\\n✓ Core multi-head mechanism test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_causal, attn_causal = multi_head_attention_core(Q, K, V, num_heads, mask=causal_mask)\n",
    "\n",
    "# Verify upper triangle is zero for all heads\n",
    "for h in range(num_heads):\n",
    "    upper = attn_causal[0, h].triu(diagonal=1)\n",
    "    assert torch.allclose(upper, torch.zeros_like(upper), atol=1e-6), f\"Head {h} has non-zero upper triangle!\"\n",
    "\n",
    "print(\"✓ Causal mask works correctly across all heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Head Self-Attention\n",
    "\n",
    "Now implement the **full** multi-head self-attention where:\n",
    "- Input: Single tensor `x` (the residual stream)\n",
    "- Q, K, V are computed as projections of x\n",
    "- Each projection is split into heads\n",
    "- Output projection combines the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention where Q, K, V come from projections of the same input.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model: Total embedding dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_head: Dimension per head (d_model // num_heads)\n",
    "        W_q, W_k, W_v: Projection matrices from x to Q, K, V\n",
    "        W_o: Output projection matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        is_causal: bool = False,\n",
    "        key_padding_lengths: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            is_causal: Whether to apply causal masking\n",
    "            key_padding_lengths: If provided, actual lengths for padding mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Multi-head attention output (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Self-Attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Single input - the residual stream\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(\"This single input will be projected to create Q, K, V for all heads\")\n",
    "\n",
    "# Create multi-head self-attention\n",
    "mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass (bidirectional)\n",
    "output = mhsa(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "assert output.shape == x.shape\n",
    "\n",
    "# Forward pass (causal)\n",
    "output_causal = mhsa(x, is_causal=True)\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "\n",
    "# Forward pass (with padding)\n",
    "lengths = torch.tensor([8, 5])\n",
    "output_padded = mhsa(x, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"Padded output shape: {output_padded.shape}\")\n",
    "\n",
    "print(\"\\n✓ Multi-Head Self-Attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Against PyTorch's Implementation\n",
    "\n",
    "To verify correctness, we compare against `torch.nn.MultiheadAttention` using the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(q, k, v, num_heads, d_model, mask=None, weights=None):\n",
    "    \"\"\"\n",
    "    Multi-head attention function that can use external weights for validation.\n",
    "    \n",
    "    Note: In real transformers, this takes a single input x and projects it.\n",
    "    This version accepts separate q, k, v for compatibility with PyTorch's API.\n",
    "    \n",
    "    Args:\n",
    "        q, k, v: Query, Key, Value tensors of shape (batch, seq_len, d_model)\n",
    "        num_heads: Number of attention heads\n",
    "        d_model: Total embedding dimension\n",
    "        mask: Optional boolean attention mask (True = masked)\n",
    "        weights: Optional dict with 'q_weight', 'k_weight', 'v_weight', 'out_weight'\n",
    "    \n",
    "    Returns:\n",
    "        output: Multi-head attention output (batch, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate against PyTorch's MultiheadAttention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# For validation, we use PyTorch's API which takes q, k, v separately\n",
    "# (even though in self-attention they're all projections of the same input)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create PyTorch reference\n",
    "multihead_attn = torch.nn.MultiheadAttention(\n",
    "    embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True\n",
    ")\n",
    "\n",
    "# Extract weights\n",
    "weights = {\n",
    "    'q_weight': multihead_attn.in_proj_weight[:d_model, :],\n",
    "    'k_weight': multihead_attn.in_proj_weight[d_model:2*d_model, :],\n",
    "    'v_weight': multihead_attn.in_proj_weight[2*d_model:, :],\n",
    "    'out_weight': multihead_attn.out_proj.weight\n",
    "}\n",
    "\n",
    "# For self-attention: q=k=v=x\n",
    "output_custom = multi_head_attention(x, x, x, num_heads, d_model, weights=weights)\n",
    "output_ref, _ = multihead_attn(x, x, x)\n",
    "\n",
    "assert torch.allclose(output_custom, output_ref, atol=1e-6), \"Outputs don't match!\"\n",
    "print(\"✓ Multi-Head Attention matches PyTorch!\")\n",
    "print(f\"Max difference: {(output_custom - output_ref).abs().max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention with KV Cache\n",
    "\n",
    "### Memory Considerations\n",
    "\n",
    "KV cache memory per layer:\n",
    "```\n",
    "memory = batch_size * num_heads * seq_len * head_dim * 2 (K and V) * bytes_per_param\n",
    "```\n",
    "\n",
    "For a 70B model (80 layers, 64 heads, 128 head_dim) with 8K context:\n",
    "- Per layer: 8K * 64 * 128 * 2 * 2 bytes = ~256 MB\n",
    "- Total: 80 * 256 MB = **~20 GB just for KV cache!**\n",
    "\n",
    "This is why techniques like **Grouped Query Attention (GQA)** are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention with KV Cache support for efficient inference.\n",
    "    \n",
    "    Takes a single input x (the residual stream) and projects it to Q, K, V.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model: Total embedding dimension\n",
    "        num_heads: Number of attention heads\n",
    "        head_dim: Dimension per head (d_model // num_heads)\n",
    "        q_proj, k_proj, v_proj: Projection matrices from x to Q, K, V\n",
    "        out_proj: Output projection matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cache_k: torch.Tensor = None,\n",
    "        cache_v: torch.Tensor = None,\n",
    "        is_causal: bool = True,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            cache_k: Cached keys (batch, num_heads, cached_len, head_dim) or None\n",
    "            cache_v: Cached values (batch, num_heads, cached_len, head_dim) or None\n",
    "            is_causal: Whether to apply causal masking\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch, seq_len, d_model)\n",
    "            new_cache_k: Updated key cache\n",
    "            new_cache_v: Updated value cache\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MHA with KV Cache\n",
    "print(\"=== Testing Multi-Head Attention with KV Cache ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttentionWithCache(d_model, num_heads)\n",
    "\n",
    "# Step 1: Process prompt (3 tokens) - prefill\n",
    "prompt = torch.randn(batch_size, 3, d_model)\n",
    "print(f\"\\nInput prompt shape: {prompt.shape}\")\n",
    "print(\"Q, K, V will all be computed from this single input\")\n",
    "\n",
    "out1, cache_k, cache_v = mha(prompt, None, None)\n",
    "print(f\"\\nAfter prompt: cache shape = {cache_k.shape}\")\n",
    "print(f\"  (batch={batch_size}, num_heads={num_heads}, seq_len=3, head_dim={d_model//num_heads})\")\n",
    "\n",
    "# Step 2: Generate token 4\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out2, cache_k, cache_v = mha(new_token, cache_k, cache_v)\n",
    "print(f\"After token 4: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out3, cache_k, cache_v = mha(new_token, cache_k, cache_v)\n",
    "print(f\"After token 5: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Verify\n",
    "assert cache_k.shape == (batch_size, num_heads, 5, d_model // num_heads)\n",
    "assert out3.shape == (batch_size, 1, d_model)\n",
    "\n",
    "print(\"\\n✓ MHA with KV Cache test passed!\")\n",
    "print(f\"\\nCache memory per layer: {cache_k.numel() * 4 * 2 / 1024:.1f} KB (K+V, float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "**Q: Why use multiple heads instead of one large head?**\n",
    "A: Multiple heads allow the model to jointly attend to information from different representation subspaces. Each head can learn different patterns (syntax, semantics, positional, etc.).\n",
    "\n",
    "**Q: What's the relationship between d_model, num_heads, and d_head?**\n",
    "A: d_head = d_model / num_heads. The total computation stays the same - we're just splitting d_model dimensions across multiple parallel attention operations.\n",
    "\n",
    "**Q: How does masking work with multiple heads?**\n",
    "A: The mask broadcasts across all heads. Shape (seq_q, seq_k) broadcasts to (batch, num_heads, seq_q, seq_k). Each head uses the same mask.\n",
    "\n",
    "**Q: How does the KV cache shape differ from Q?**\n",
    "A: During generation, Q is computed only for the new token (shape: batch, num_heads, 1, head_dim), while K and V include all previous tokens from the cache.\n",
    "\n",
    "**Q: Why is the cache stored per-head rather than combined?**\n",
    "A: Each head has its own K and V projections. Storing per-head allows efficient concatenation and avoids recomputing the head split each time.\n",
    "\n",
    "**Q: What's the memory complexity of KV cache?**\n",
    "A: O(batch * layers * num_heads * seq_len * head_dim * 2). For long sequences (128K+), this can be tens of GBs, motivating techniques like GQA, MQA, and paged attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
