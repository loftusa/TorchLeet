{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Multi-Head Attention from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Multi-Head Attention (MHA) is the core mechanism of the Transformer architecture. It enables the model to **jointly attend** to information from different representation subspaces at different positions.\n",
    "\n",
    "### Background: Why Multiple Heads?\n",
    "\n",
    "Single-head attention computes one set of attention weights. But different parts of the input might benefit from different attention patterns:\n",
    "- One head might learn syntax (subject-verb agreement)\n",
    "- Another might learn semantics (word meaning relationships)\n",
    "- Another might learn positional patterns\n",
    "\n",
    "Multi-head attention runs multiple attention operations in parallel, each with its own learned projections.\n",
    "\n",
    "### The Math\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "### Key Dimensions\n",
    "\n",
    "- `d_model`: Total embedding dimension (e.g., 512)\n",
    "- `num_heads`: Number of attention heads (e.g., 8)\n",
    "- `d_head = d_model // num_heads`: Dimension per head (e.g., 64)\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: Mask creation (causal, padding, KV cache)\n",
    "2. **Part 2**: Core multi-head mechanism (Q, K, V given) - focus on the split/concat logic\n",
    "3. **Part 3**: Multi-Head Self-Attention - Q, K, V from projections of single input x\n",
    "4. **Part 4**: MHA with KV Cache for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Attention Mask Creation\n",
    "\n",
    "The mask functions are the same as single-head attention. For multi-head, the mask broadcasts across the head dimension.\n",
    "\n",
    "Mask convention: **True = masked (cannot attend), False = can attend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len_q: int, seq_len_k: int = None, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask for autoregressive models.\n",
    "    \n",
    "    For multi-head attention, this mask broadcasts across the head dimension.\n",
    "    Shape: (seq_len_q, seq_len_k) -> broadcasts to (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "    return torch.triu(torch.ones(seq_len_q, seq_len_k, dtype=torch.bool, device=device), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a padding mask for variable-length sequences.\n",
    "    Shape: (batch, max_len) -> needs reshaping for multi-head: (batch, 1, 1, max_len)\n",
    "    \"\"\"\n",
    "    positions = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n",
    "    return positions >= lengths.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int = None,\n",
    "    is_causal: bool = True,\n",
    "    key_padding_lengths: torch.Tensor = None,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a combined attention mask for multi-head attention.\n",
    "    \n",
    "    Returns:\n",
    "        mask with shape suitable for broadcasting to (batch, num_heads, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "\n",
    "    mask = torch.zeros(seq_len_q, seq_len_k, dtype=torch.bool, device=device)\n",
    "\n",
    "    if is_causal:\n",
    "        causal = create_causal_mask(seq_len_q, seq_len_k, device=device)\n",
    "        mask = mask | causal\n",
    "\n",
    "    if key_padding_lengths is not None:\n",
    "        padding = create_padding_mask(key_padding_lengths, seq_len_k)  # (batch, seq_k)\n",
    "        padding = padding.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_k) for multi-head\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0) | padding  # (batch, 1, seq_q, seq_k)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask_with_cache(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int,\n",
    "    cache_len: int,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for attention with KV cache.\n",
    "    New queries can attend to all cached positions.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(seq_len_q, seq_len_k, dtype=torch.bool, device=device)\n",
    "\n",
    "    if seq_len_q > 1:\n",
    "        new_token_mask = create_causal_mask(seq_len_q, seq_len_q, device=device)\n",
    "        mask[:, cache_len:] = new_token_mask\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mask creation\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal mask (4x4):\")\n",
    "print(mask)\n",
    "print(f\"\\nThis broadcasts to (batch, num_heads, 4, 4) in multi-head attention\")\n",
    "\n",
    "# Test padding mask for multi-head (needs extra dimensions)\n",
    "lengths = torch.tensor([4, 3])\n",
    "mask_combined = create_attention_mask(4, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"\\nCombined mask shape for multi-head: {mask_combined.shape}\")\n",
    "print(\"Broadcasts to (batch, num_heads, seq_q, seq_k)\")\n",
    "\n",
    "print(\"\\n\\u2713 Mask tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core Multi-Head Mechanism\n",
    "\n",
    "First, implement multi-head attention assuming Q, K, V are already projected.\n",
    "This isolates the split-into-heads and concatenate logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "print(f\"d_model={d_model}, num_heads={num_heads}, d_head={d_head}\")\n",
    "print(f\"\\nEach head operates on {d_head} dimensions\")\n",
    "print(f\"All {num_heads} heads run in parallel, then concatenate back to {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_core(Q, K, V, num_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Core multi-head attention computation (Q, K, V already projected).\n",
    "    \n",
    "    This function takes already-projected Q, K, V and:\n",
    "    1. Splits them into multiple heads\n",
    "    2. Computes attention for each head in parallel\n",
    "    3. Concatenates the results\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Projected tensors of shape (batch, seq_len, d_model)\n",
    "        num_heads: Number of attention heads\n",
    "        mask: Optional boolean attention mask (True = masked)\n",
    "    \n",
    "    Returns:\n",
    "        output: Multi-head attention output (batch, seq_len, d_model)\n",
    "        attn_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = Q.shape\n",
    "    d_head = d_model // num_heads\n",
    "    \n",
    "    # Split into heads: (batch, seq, d_model) -> (batch, num_heads, seq, d_head)\n",
    "    Q = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    K = K.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    V = V.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    \n",
    "    # Scaled dot-product attention per head\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "    \n",
    "    # Apply mask (broadcasts across batch and heads)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)  # (batch, num_heads, seq, d_head)\n",
    "    \n",
    "    # Concatenate heads: (batch, num_heads, seq, d_head) -> (batch, seq, d_model)\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "    \n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the core mechanism with random Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = multi_head_attention_core(Q, K, V, num_heads)\n",
    "\n",
    "print(f\"Input Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  -> {num_heads} heads, each with {seq_len}x{seq_len} attention matrix\")\n",
    "\n",
    "assert output.shape == Q.shape\n",
    "print(\"\\n\\u2713 Core multi-head mechanism test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_causal, attn_causal = multi_head_attention_core(Q, K, V, num_heads, mask=causal_mask)\n",
    "\n",
    "# Verify upper triangle is zero for all heads\n",
    "for h in range(num_heads):\n",
    "    upper = attn_causal[0, h].triu(diagonal=1)\n",
    "    assert torch.allclose(upper, torch.zeros_like(upper), atol=1e-6), f\"Head {h} has non-zero upper triangle!\"\n",
    "\n",
    "print(\"\\u2713 Causal mask works correctly across all heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Head Self-Attention\n",
    "\n",
    "Now implement the **full** multi-head self-attention where:\n",
    "- Input: Single tensor `x` (the residual stream)\n",
    "- Q, K, V are computed as projections of x\n",
    "- Each projection is split into heads\n",
    "- Output projection combines the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention where Q, K, V come from projections of the same input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "        # Projections: x -> Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        is_causal: bool = False,\n",
    "        key_padding_lengths: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            is_causal: Whether to apply causal masking\n",
    "            key_padding_lengths: If provided, actual lengths for padding mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Multi-head attention output (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project x to Q, K, V - all from the SAME input!\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split into heads: (batch, seq, d_model) -> (batch, num_heads, seq, d_head)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention per head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        \n",
    "        # Create and apply mask\n",
    "        if is_causal or key_padding_lengths is not None:\n",
    "            mask = create_attention_mask(\n",
    "                seq_len_q=seq_len,\n",
    "                seq_len_k=seq_len,\n",
    "                is_causal=is_causal,\n",
    "                key_padding_lengths=key_padding_lengths,\n",
    "                device=x.device\n",
    "            )\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads: (batch, num_heads, seq, d_head) -> (batch, seq, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        return self.W_o(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Self-Attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Single input - the residual stream\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(\"This single input will be projected to create Q, K, V for all heads\")\n",
    "\n",
    "# Create multi-head self-attention\n",
    "mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass (bidirectional)\n",
    "output = mhsa(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "assert output.shape == x.shape\n",
    "\n",
    "# Forward pass (causal)\n",
    "output_causal = mhsa(x, is_causal=True)\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "\n",
    "# Forward pass (with padding)\n",
    "lengths = torch.tensor([8, 5])\n",
    "output_padded = mhsa(x, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"Padded output shape: {output_padded.shape}\")\n",
    "\n",
    "print(\"\\n\\u2713 Multi-Head Self-Attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Against PyTorch's Implementation\n",
    "\n",
    "To verify correctness, we compare against `torch.nn.MultiheadAttention` using the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(q, k, v, num_heads, d_model, mask=None, weights=None):\n",
    "    \"\"\"\n",
    "    Multi-head attention function that can use external weights for validation.\n",
    "    \n",
    "    Note: In real transformers, this takes a single input x and projects it.\n",
    "    This version accepts separate q, k, v for compatibility with PyTorch's API.\n",
    "    \"\"\"\n",
    "    assert d_model % num_heads == 0\n",
    "    \n",
    "    d_head = d_model // num_heads\n",
    "    batch_size, seq_len, _ = q.shape\n",
    "    \n",
    "    # Create projections\n",
    "    Q_w = nn.Linear(d_model, d_model, bias=False)\n",
    "    K_w = nn.Linear(d_model, d_model, bias=False)\n",
    "    V_w = nn.Linear(d_model, d_model, bias=False)\n",
    "    W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    if weights is not None:\n",
    "        Q_w.weight.data = weights['q_weight']\n",
    "        K_w.weight.data = weights['k_weight']\n",
    "        V_w.weight.data = weights['v_weight']\n",
    "        W_out.weight.data = weights['out_weight']\n",
    "    \n",
    "    # Project\n",
    "    Q = Q_w(q)\n",
    "    K = K_w(k)\n",
    "    V = V_w(v)\n",
    "    \n",
    "    # Split into heads\n",
    "    Q = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    K = K.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    V = V.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    \n",
    "    # Attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    # Concatenate and project\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "    return W_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate against PyTorch's MultiheadAttention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# For validation, we use PyTorch's API which takes q, k, v separately\n",
    "# (even though in self-attention they're all projections of the same input)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create PyTorch reference\n",
    "multihead_attn = torch.nn.MultiheadAttention(\n",
    "    embed_dim=d_model, num_heads=num_heads, bias=False, batch_first=True\n",
    ")\n",
    "\n",
    "# Extract weights\n",
    "weights = {\n",
    "    'q_weight': multihead_attn.in_proj_weight[:d_model, :],\n",
    "    'k_weight': multihead_attn.in_proj_weight[d_model:2*d_model, :],\n",
    "    'v_weight': multihead_attn.in_proj_weight[2*d_model:, :],\n",
    "    'out_weight': multihead_attn.out_proj.weight\n",
    "}\n",
    "\n",
    "# For self-attention: q=k=v=x\n",
    "output_custom = multi_head_attention(x, x, x, num_heads, d_model, weights=weights)\n",
    "output_ref, _ = multihead_attn(x, x, x)\n",
    "\n",
    "assert torch.allclose(output_custom, output_ref, atol=1e-6), \"Outputs don't match!\"\n",
    "print(\"\\u2713 Multi-Head Attention matches PyTorch!\")\n",
    "print(f\"Max difference: {(output_custom - output_ref).abs().max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Multi-Head Attention\n",
    "\n",
    "Different heads learn to attend to different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_attention_weights(x, num_heads, d_model):\n",
    "    \"\"\"Get attention weights for visualization.\"\"\"\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "    d_head = d_model // num_heads\n",
    "    \n",
    "    # Random projections for visualization\n",
    "    W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "    W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    Q = W_q(x).view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    K = W_k(x).view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    return attn_weights\n",
    "\n",
    "# Generate attention patterns\n",
    "torch.manual_seed(123)\n",
    "vis_x = torch.randn(1, 8, 64)\n",
    "attn = get_attention_weights(vis_x, num_heads=4, d_model=64)\n",
    "\n",
    "# Plot each head's attention pattern\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "for h in range(4):\n",
    "    im = axes[h].imshow(attn[0, h].detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "    axes[h].set_title(f'Head {h}', fontsize=11)\n",
    "    axes[h].set_xlabel('Key')\n",
    "    if h == 0:\n",
    "        axes[h].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Patterns (each head learns different patterns)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mha_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different heads attend to different positions -\")\n",
    "print(\"this is how the model captures diverse relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention with KV Cache\n",
    "\n",
    "### Memory Considerations\n",
    "\n",
    "KV cache memory per layer:\n",
    "```\n",
    "memory = batch_size * num_heads * seq_len * head_dim * 2 (K and V) * bytes_per_param\n",
    "```\n",
    "\n",
    "For a 70B model (80 layers, 64 heads, 128 head_dim) with 8K context:\n",
    "- Per layer: 8K * 64 * 128 * 2 * 2 bytes = ~256 MB\n",
    "- Total: 80 * 256 MB = **~20 GB just for KV cache!**\n",
    "\n",
    "This is why techniques like **Grouped Query Attention (GQA)** are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention with KV Cache support for efficient inference.\n",
    "    \n",
    "    Takes a single input x (the residual stream) and projects it to Q, K, V.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cache_k: torch.Tensor = None,\n",
    "        cache_v: torch.Tensor = None,\n",
    "        is_causal: bool = True,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            cache_k: Cached keys (batch, num_heads, cached_len, head_dim) or None\n",
    "            cache_v: Cached values (batch, num_heads, cached_len, head_dim) or None\n",
    "            is_causal: Whether to apply causal masking\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch, seq_len, d_model)\n",
    "            new_cache_k: Updated key cache\n",
    "            new_cache_v: Updated value cache\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = x.shape\n",
    "        cache_len = cache_k.size(2) if cache_k is not None else 0\n",
    "        \n",
    "        # Project x to Q, K, V\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape to (batch, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate with cache if exists\n",
    "        if cache_k is not None and cache_v is not None:\n",
    "            K = torch.cat([cache_k, K], dim=2)\n",
    "            V = torch.cat([cache_v, V], dim=2)\n",
    "        \n",
    "        # Update cache\n",
    "        new_cache_k = K\n",
    "        new_cache_v = V\n",
    "        \n",
    "        seq_len_k = K.size(2)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask with cache offset\n",
    "        if is_causal:\n",
    "            mask = create_causal_mask_with_cache(\n",
    "                seq_len_q=seq_len_q,\n",
    "                seq_len_k=seq_len_k,\n",
    "                cache_len=cache_len,\n",
    "                device=x.device\n",
    "            )\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back: (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output, new_cache_k, new_cache_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MHA with KV Cache\n",
    "print(\"=== Testing Multi-Head Attention with KV Cache ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttentionWithCache(d_model, num_heads)\n",
    "\n",
    "# Step 1: Process prompt (3 tokens) - prefill\n",
    "prompt = torch.randn(batch_size, 3, d_model)\n",
    "print(f\"\\nInput prompt shape: {prompt.shape}\")\n",
    "print(\"Q, K, V will all be computed from this single input\")\n",
    "\n",
    "out1, cache_k, cache_v = mha(prompt, None, None)\n",
    "print(f\"\\nAfter prompt: cache shape = {cache_k.shape}\")\n",
    "print(f\"  (batch={batch_size}, num_heads={num_heads}, seq_len=3, head_dim={d_model//num_heads})\")\n",
    "\n",
    "# Step 2: Generate token 4\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out2, cache_k, cache_v = mha(new_token, cache_k, cache_v)\n",
    "print(f\"After token 4: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out3, cache_k, cache_v = mha(new_token, cache_k, cache_v)\n",
    "print(f\"After token 5: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Verify\n",
    "assert cache_k.shape == (batch_size, num_heads, 5, d_model // num_heads)\n",
    "assert out3.shape == (batch_size, 1, d_model)\n",
    "\n",
    "print(\"\\n\\u2713 MHA with KV Cache test passed!\")\n",
    "print(f\"\\nCache memory per layer: {cache_k.numel() * 4 * 2 / 1024:.1f} KB (K+V, float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "**Q: Why use multiple heads instead of one large head?**\n",
    "A: Multiple heads allow the model to jointly attend to information from different representation subspaces. Each head can learn different patterns (syntax, semantics, positional, etc.).\n",
    "\n",
    "**Q: What's the relationship between d_model, num_heads, and d_head?**\n",
    "A: d_head = d_model / num_heads. The total computation stays the same - we're just splitting d_model dimensions across multiple parallel attention operations.\n",
    "\n",
    "**Q: How does masking work with multiple heads?**\n",
    "A: The mask broadcasts across all heads. Shape (seq_q, seq_k) broadcasts to (batch, num_heads, seq_q, seq_k). Each head uses the same mask.\n",
    "\n",
    "**Q: How does the KV cache shape differ from Q?**\n",
    "A: During generation, Q is computed only for the new token (shape: batch, num_heads, 1, head_dim), while K and V include all previous tokens from the cache.\n",
    "\n",
    "**Q: Why is the cache stored per-head rather than combined?**\n",
    "A: Each head has its own K and V projections. Storing per-head allows efficient concatenation and avoids recomputing the head split each time.\n",
    "\n",
    "**Q: What's the memory complexity of KV cache?**\n",
    "A: O(batch * layers * num_heads * seq_len * head_dim * 2). For long sequences (128K+), this can be tens of GBs, motivating techniques like GQA, MQA, and paged attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
