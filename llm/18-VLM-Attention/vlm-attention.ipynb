{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM vs LLM Attention: Understanding Vision-Language Model Attention\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Compare how attention works in **Vision-Language Models (VLMs)** versus **pure Language Models (LLMs)**. This notebook focuses on the **LLaVA-style architecture**, which is the most common approach in modern VLMs.\n",
    "\n",
    "### Background: Why VLM Attention is Different\n",
    "\n",
    "In pure LLMs:\n",
    "- Input: Text tokens only\n",
    "- Attention: Causal (each token attends only to previous tokens)\n",
    "- Mask: Lower triangular\n",
    "\n",
    "In Vision-Language Models:\n",
    "- Input: Image patches + text tokens\n",
    "- Attention: Mixed (bidirectional for image, causal for text)\n",
    "- Mask: NOT simply lower triangular\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: LLM Attention Review - Quick recap of causal self-attention\n",
    "2. **Part 2**: Vision Transformer (ViT) - Patch embedding and bidirectional attention\n",
    "3. **Part 3**: LLaVA-Style VLM - How image and text tokens interact\n",
    "4. **Part 4**: Attention Visualization - Real patterns from pretrained models\n",
    "5. **Part 5**: Side-by-Side Comparison - Visual comparison\n",
    "6. **Part 6**: Interview Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# For pretrained models\n",
    "try:\n",
    "    from transformers import CLIPModel, CLIPProcessor\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"transformers not installed - pretrained visualization will be skipped\")\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    HAS_PIL = True\n",
    "except ImportError:\n",
    "    HAS_PIL = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Attention Review\n",
    "\n",
    "In pure language models (GPT, LLaMA, etc.), attention is **causal**: each token can only attend to itself and previous tokens. This is enforced with a lower-triangular mask.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "where $M$ is the causal mask with $-\\infty$ in the upper triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask.\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor (seq_len, seq_len)\n",
    "              True = masked (cannot attend), False = can attend\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    q: torch.Tensor, \n",
    "    k: torch.Tensor, \n",
    "    v: torch.Tensor, \n",
    "    mask: Optional[torch.Tensor] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        q: Query (batch, seq_q, d_k)\n",
    "        k: Key (batch, seq_k, d_k)\n",
    "        v: Value (batch, seq_k, d_v)\n",
    "        mask: Boolean mask where True = masked\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_q, d_v)\n",
    "        attn_weights: (batch, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    \n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask for LLM (True = cannot attend):\")\n",
    "print(causal_mask.int())\n",
    "print(\"\\nToken 0 can only see: itself\")\n",
    "print(\"Token 3 can see: tokens 0, 1, 2, 3\")\n",
    "print(\"Token 7 can see: all tokens 0-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate causal attention pattern\n",
    "torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "d_model = 64\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "_, attn_weights_causal = scaled_dot_product_attention(q, k, v, mask=causal_mask)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(attn_weights_causal[0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "ax.set_title('LLM: Causal Self-Attention', fontsize=12)\n",
    "ax.set_xlabel('Key Position (past → future)')\n",
    "ax.set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Upper triangle is zero (cannot attend to future tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vision Transformer (ViT) Basics\n",
    "\n",
    "In Vision Transformers, images are converted to a sequence of **patch embeddings**, then processed with **bidirectional** self-attention (no causal mask).\n",
    "\n",
    "### Key Steps:\n",
    "1. **Patch Embedding**: Split image into patches, flatten, project to embedding dim\n",
    "2. **Position Encoding**: Add 2D positional information\n",
    "3. **CLS Token**: Prepend a learnable classification token\n",
    "4. **Bidirectional Attention**: All patches attend to all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into a sequence of patch embeddings.\n",
    "    \n",
    "    Image (B, C, H, W) -> Patches (B, N_patches, embed_dim)\n",
    "    \n",
    "    For a 224x224 image with 16x16 patches:\n",
    "    - N_patches = (224/16) * (224/16) = 14 * 14 = 196\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size: int = 224, \n",
    "        patch_size: int = 16, \n",
    "        in_channels: int = 3, \n",
    "        embed_dim: int = 768\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use conv2d as an efficient way to extract and project patches\n",
    "        # kernel_size=patch_size, stride=patch_size -> non-overlapping patches\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, \n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Image tensor (batch, channels, height, width)\n",
    "        Returns:\n",
    "            patches: (batch, n_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        # (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        x = self.proj(x)\n",
    "        # (B, embed_dim, H/P, W/P) -> (B, embed_dim, N_patches)\n",
    "        x = x.flatten(2)\n",
    "        # (B, embed_dim, N_patches) -> (B, N_patches, embed_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patch embedding\n",
    "torch.manual_seed(42)\n",
    "\n",
    "patch_embed = PatchEmbedding(img_size=224, patch_size=16, embed_dim=768)\n",
    "\n",
    "# Simulate an image\n",
    "dummy_image = torch.randn(1, 3, 224, 224)\n",
    "patches = patch_embed(dummy_image)\n",
    "\n",
    "print(f\"Input image shape: {dummy_image.shape}\")\n",
    "print(f\"Output patches shape: {patches.shape}\")\n",
    "print(f\"Number of patches: {patches.shape[1]} = 14 x 14 grid\")\n",
    "print(f\"Each patch embedding dim: {patches.shape[2]}\")\n",
    "\n",
    "assert patches.shape == (1, 196, 768), \"Wrong output shape!\"\n",
    "print(\"\\n✓ Patch embedding test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention for Vision Transformer.\n",
    "    \n",
    "    Key difference from LLM attention: NO CAUSAL MASK\n",
    "    All patches can attend to all other patches (bidirectional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 768, num_heads: int = 12):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, embed_dim)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attn_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V in one projection\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention (NO MASK!)\n",
    "        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = (attn_weights @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ViT bidirectional attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use smaller dimensions for visualization\n",
    "n_patches = 16  # 4x4 grid for easy visualization\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "vit_attn = ViTAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "patch_tokens = torch.randn(1, n_patches, embed_dim)\n",
    "\n",
    "_, vit_attn_weights = vit_attn(patch_tokens)\n",
    "\n",
    "# Plot attention from head 0\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(vit_attn_weights[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.3)\n",
    "ax.set_title('ViT: Bidirectional Attention (Head 0)', fontsize=12)\n",
    "ax.set_xlabel('Key Patch')\n",
    "ax.set_ylabel('Query Patch')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: NO zeros in the matrix - all patches attend to all patches!\")\n",
    "print(\"This is bidirectional attention (no causal mask).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LLaVA-Style VLM Architecture\n",
    "\n",
    "### How LLaVA Works\n",
    "\n",
    "LLaVA (Large Language and Vision Assistant) uses a simple but effective approach:\n",
    "\n",
    "1. **Vision Encoder**: Pretrained ViT extracts visual features\n",
    "2. **Projection Layer**: Maps visual features to LLM embedding space\n",
    "3. **Concatenation**: `[Image Tokens] + [Text Tokens]`\n",
    "4. **LLM Processing**: Single decoder-only transformer processes both\n",
    "\n",
    "### The Key Insight: VLM Attention Mask\n",
    "\n",
    "The attention mask in a VLM is **NOT simply causal**:\n",
    "\n",
    "```\n",
    "                    Image Tokens    Text Tokens\n",
    "                    [I0 I1 I2 I3]   [T0 T1 T2 T3]\n",
    "Image   I0          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "Tokens  I1          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "        I2          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "        I3          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "Text    T0          [ ✓  ✓  ✓  ✓     ✓  ✗  ✗  ✗ ]\n",
    "Tokens  T1          [ ✓  ✓  ✓  ✓     ✓  ✓  ✗  ✗ ]\n",
    "        T2          [ ✓  ✓  ✓  ✓     ✓  ✓  ✓  ✗ ]\n",
    "        T3          [ ✓  ✓  ✓  ✓     ✓  ✓  ✓  ✓ ]\n",
    "```\n",
    "\n",
    "- **Image-to-Image**: Bidirectional (all can see all)\n",
    "- **Image-to-Text**: Cannot attend (image comes first, doesn't \"see\" future text)\n",
    "- **Text-to-Image**: Can attend (text can see all image tokens)\n",
    "- **Text-to-Text**: Causal (each text token sees only past text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vlm_attention_mask(n_image: int, n_text: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create attention mask for Vision-Language Model (LLaVA-style).\n",
    "    \n",
    "    The mask has a specific structure:\n",
    "    - Image tokens: bidirectional among themselves\n",
    "    - Image tokens: CANNOT attend to text tokens (they come first)\n",
    "    - Text tokens: CAN attend to all image tokens\n",
    "    - Text tokens: causal among themselves\n",
    "    \n",
    "    Args:\n",
    "        n_image: Number of image tokens (patches)\n",
    "        n_text: Number of text tokens\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor (n_image + n_text, n_image + n_text)\n",
    "              True = masked (cannot attend), False = can attend\n",
    "    \"\"\"\n",
    "    total = n_image + n_text\n",
    "    mask = torch.zeros(total, total, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Image-to-image: bidirectional (no masking needed, already False)\n",
    "    \n",
    "    # Image-to-text: MASKED (image tokens cannot see future text)\n",
    "    mask[:n_image, n_image:] = True\n",
    "    \n",
    "    # Text-to-image: NOT masked (text can see all image tokens, already False)\n",
    "    \n",
    "    # Text-to-text: causal (upper triangle masked)\n",
    "    text_causal = torch.triu(\n",
    "        torch.ones(n_text, n_text, dtype=torch.bool, device=device), \n",
    "        diagonal=1\n",
    "    )\n",
    "    mask[n_image:, n_image:] = text_causal\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VLM attention mask\n",
    "n_image = 4\n",
    "n_text = 4\n",
    "\n",
    "vlm_mask = create_vlm_attention_mask(n_image, n_text)\n",
    "\n",
    "print(\"VLM Attention Mask (True/1 = CANNOT attend):\")\n",
    "print(vlm_mask.int())\n",
    "print(f\"\\nTop-left {n_image}x{n_image}: Image-to-Image (all zeros = bidirectional)\")\n",
    "print(f\"Top-right {n_image}x{n_text}: Image-to-Text (all ones = masked)\")\n",
    "print(f\"Bottom-left {n_text}x{n_image}: Text-to-Image (all zeros = can attend)\")\n",
    "print(f\"Bottom-right {n_text}x{n_text}: Text-to-Text (causal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mask structure\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Convert to float for visualization (1 = masked/red, 0 = can attend/white)\n",
    "mask_viz = vlm_mask.float().numpy()\n",
    "\n",
    "im = ax.imshow(mask_viz, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "ax.set_title('VLM Attention Mask Structure', fontsize=12)\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "\n",
    "# Add grid lines to separate image and text regions\n",
    "ax.axhline(y=n_image - 0.5, color='black', linewidth=2)\n",
    "ax.axvline(x=n_image - 0.5, color='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "ax.text(n_image/2 - 0.5, -0.8, 'Image', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(n_image + n_text/2 - 0.5, -0.8, 'Text', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(-1.2, n_image/2 - 0.5, 'Image', va='center', fontsize=10, fontweight='bold', rotation=90)\n",
    "ax.text(-1.2, n_image + n_text/2 - 0.5, 'Text', va='center', fontsize=10, fontweight='bold', rotation=90)\n",
    "\n",
    "# Add region labels\n",
    "ax.text(n_image/2 - 0.5, n_image/2 - 0.5, 'Bidirectional', ha='center', va='center', fontsize=9, color='green')\n",
    "ax.text(n_image + n_text/2 - 0.5, n_image/2 - 0.5, 'Masked', ha='center', va='center', fontsize=9, color='red')\n",
    "ax.text(n_image/2 - 0.5, n_image + n_text/2 - 0.5, 'Cross-Attend', ha='center', va='center', fontsize=9, color='green')\n",
    "ax.text(n_image + n_text/2 - 0.5, n_image + n_text/2 - 0.5, 'Causal', ha='center', va='center', fontsize=9, color='orange')\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8, label='Masked')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vlm_mask_structure.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Project visual features from ViT to LLM embedding space.\n",
    "    \n",
    "    In LLaVA, this is typically a simple MLP:\n",
    "    vision_dim -> hidden_dim -> llm_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vision_dim: int, llm_dim: int, hidden_dim: int = None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = llm_dim\n",
    "        \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(vision_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, llm_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project vision features to LLM space.\"\"\"\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision-Language Model attention with proper masking.\n",
    "    \n",
    "    Processes concatenated [image_tokens, text_tokens] with:\n",
    "    - Bidirectional attention among image tokens\n",
    "    - Image tokens cannot attend to text tokens\n",
    "    - Text tokens can attend to all image tokens\n",
    "    - Causal attention among text tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        n_image_tokens: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Concatenated [image_tokens, text_tokens] (batch, seq_len, d_model)\n",
    "            n_image_tokens: Number of image tokens (to construct proper mask)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attn_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.shape\n",
    "        n_text_tokens = N - n_image_tokens\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply VLM mask\n",
    "        mask = create_vlm_attention_mask(n_image_tokens, n_text_tokens, device=x.device)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = (attn_weights @ V).transpose(1, 2).reshape(B, N, self.d_model)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VLM attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "n_image = 9  # 3x3 patch grid\n",
    "n_text = 6\n",
    "\n",
    "vlm_attn = VLMAttention(d_model, num_heads)\n",
    "\n",
    "# Simulate concatenated image + text tokens\n",
    "image_tokens = torch.randn(1, n_image, d_model)\n",
    "text_tokens = torch.randn(1, n_text, d_model)\n",
    "combined = torch.cat([image_tokens, text_tokens], dim=1)\n",
    "\n",
    "print(f\"Image tokens: {n_image}\")\n",
    "print(f\"Text tokens: {n_text}\")\n",
    "print(f\"Combined sequence: {combined.shape}\")\n",
    "\n",
    "output, attn_weights = vlm_attn(combined, n_image_tokens=n_image)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VLM attention pattern\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Average across heads\n",
    "attn_avg = attn_weights[0].mean(dim=0).detach().numpy()\n",
    "\n",
    "im = ax.imshow(attn_avg, cmap='Blues', vmin=0, vmax=0.3)\n",
    "ax.set_title('VLM Attention Pattern (LLaVA-style)', fontsize=12)\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "\n",
    "# Add grid lines\n",
    "ax.axhline(y=n_image - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "ax.axvline(x=n_image - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Add labels\n",
    "ax.text(n_image/2, -1, 'Image Tokens', ha='center', fontsize=10)\n",
    "ax.text(n_image + n_text/2, -1, 'Text Tokens', ha='center', fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8, label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vlm_attention_pattern.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Top-left (Image→Image): Dense attention, all patches see each other\")\n",
    "print(\"- Top-right (Image→Text): All zeros (image can't see future text)\")\n",
    "print(\"- Bottom-left (Text→Image): Dense attention, text sees all image patches\")\n",
    "print(\"- Bottom-right (Text→Text): Causal pattern (lower triangular)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attention Visualization with Pretrained Model\n",
    "\n",
    "Let's use a pretrained CLIP model to visualize real ViT attention patterns on an actual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vit_attention_on_image(image, attention_weights, patch_size=16):\n",
    "    \"\"\"\n",
    "    Overlay attention weights on an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        attention_weights: (n_patches, n_patches) attention matrix\n",
    "        patch_size: Size of each patch\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    H, W = image.shape[:2]\n",
    "    n_patches_h = H // patch_size\n",
    "    n_patches_w = W // patch_size\n",
    "    \n",
    "    # Average attention from CLS token to all patches (or use other aggregation)\n",
    "    if attention_weights.shape[0] > n_patches_h * n_patches_w:\n",
    "        # Has CLS token - use attention from CLS to patches\n",
    "        attn_map = attention_weights[0, 1:]  # CLS -> patches (skip CLS->CLS)\n",
    "    else:\n",
    "        # No CLS token - average attention to each patch\n",
    "        attn_map = attention_weights.mean(dim=0)\n",
    "    \n",
    "    # Reshape to 2D grid\n",
    "    attn_map = attn_map.reshape(n_patches_h, n_patches_w)\n",
    "    \n",
    "    # Upsample to image size\n",
    "    attn_map = F.interpolate(\n",
    "        attn_map.unsqueeze(0).unsqueeze(0).float(),\n",
    "        size=(H, W),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )[0, 0]\n",
    "    \n",
    "    return attn_map.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS and HAS_PIL:\n",
    "    print(\"Loading CLIP model for attention visualization...\")\n",
    "    \n",
    "    # Load a small CLIP model\n",
    "    model_name = \"openai/clip-vit-base-patch16\"\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Create a simple test image (gradient for easy visualization)\n",
    "    # In practice, you'd load a real image\n",
    "    size = 224\n",
    "    x = np.linspace(0, 1, size)\n",
    "    y = np.linspace(0, 1, size)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    \n",
    "    # Create RGB image with gradient patterns\n",
    "    r = (xx * 255).astype(np.uint8)\n",
    "    g = (yy * 255).astype(np.uint8)\n",
    "    b = ((xx + yy) / 2 * 255).astype(np.uint8)\n",
    "    test_image = np.stack([r, g, b], axis=-1)\n",
    "    pil_image = Image.fromarray(test_image)\n",
    "    \n",
    "    print(f\"Test image size: {test_image.shape}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "else:\n",
    "    print(\"Skipping pretrained visualization (transformers or PIL not available)\")\n",
    "    pil_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pil_image is not None:\n",
    "    # Hook to capture attention weights\n",
    "    attention_weights_list = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        # CLIP's attention returns (attn_output, attn_weights)\n",
    "        if isinstance(output, tuple) and len(output) >= 2:\n",
    "            attention_weights_list.append(output[1].detach())\n",
    "    \n",
    "    # Register hooks on vision encoder attention layers\n",
    "    hooks = []\n",
    "    for layer in model.vision_model.encoder.layers:\n",
    "        hook = layer.self_attn.register_forward_hook(attention_hook)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Force attention weights to be returned\n",
    "        outputs = model.vision_model(\n",
    "            inputs['pixel_values'], \n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Get attention from outputs\n",
    "    attentions = outputs.attentions  # tuple of (batch, heads, seq, seq)\n",
    "    \n",
    "    print(f\"Number of layers: {len(attentions)}\")\n",
    "    print(f\"Attention shape per layer: {attentions[0].shape}\")\n",
    "    print(f\"  - Batch: {attentions[0].shape[0]}\")\n",
    "    print(f\"  - Heads: {attentions[0].shape[1]}\")\n",
    "    print(f\"  - Sequence (CLS + patches): {attentions[0].shape[2]}\")\n",
    "else:\n",
    "    attentions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attentions is not None:\n",
    "    # Visualize attention from different layers\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "    \n",
    "    layers_to_show = [0, 3, 6, 9, 11, -1]  # First, middle, and last layers\n",
    "    layer_names = ['Layer 1', 'Layer 4', 'Layer 7', 'Layer 10', 'Layer 12', 'Last Layer']\n",
    "    \n",
    "    for idx, (layer_idx, name) in enumerate(zip(layers_to_show, layer_names)):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Get attention for this layer, average across heads\n",
    "        attn = attentions[layer_idx][0].mean(dim=0)  # (seq, seq)\n",
    "        \n",
    "        # Show the full attention matrix\n",
    "        im = ax.imshow(attn.numpy(), cmap='Blues', vmin=0, vmax=0.1)\n",
    "        ax.set_title(f'{name}', fontsize=11)\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "        \n",
    "        # Mark CLS token position\n",
    "        ax.axhline(y=0.5, color='red', linewidth=0.5, linestyle='--', alpha=0.5)\n",
    "        ax.axvline(x=0.5, color='red', linewidth=0.5, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.suptitle('ViT Attention Patterns Across Layers (CLIP)', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vit_attention_layers.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- Early layers: More uniform attention, local patterns\")\n",
    "    print(\"- Later layers: More specialized, global patterns\")\n",
    "    print(\"- CLS token (row/col 0): Aggregates information from all patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Side-by-Side Comparison\n",
    "\n",
    "Let's compare all three attention patterns side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "torch.manual_seed(42)\n",
    "\n",
    "seq_len = 12\n",
    "d_model = 64\n",
    "\n",
    "# Generate random Q, K for consistent comparison\n",
    "q = torch.randn(1, seq_len, d_model)\n",
    "k = torch.randn(1, seq_len, d_model)\n",
    "v = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Compute raw attention scores (before masking)\n",
    "raw_scores = (q @ k.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "\n",
    "# 1. LLM (Causal) attention\n",
    "llm_mask = create_causal_mask(seq_len)\n",
    "llm_scores = raw_scores.clone()\n",
    "llm_scores = llm_scores.masked_fill(llm_mask, float('-inf'))\n",
    "llm_attn = F.softmax(llm_scores, dim=-1)[0]\n",
    "\n",
    "# 2. ViT (Bidirectional) attention - no mask\n",
    "vit_attn = F.softmax(raw_scores, dim=-1)[0]\n",
    "\n",
    "# 3. VLM attention (6 image + 6 text tokens)\n",
    "n_img, n_txt = 6, 6\n",
    "vlm_mask = create_vlm_attention_mask(n_img, n_txt)\n",
    "vlm_scores = raw_scores.clone()\n",
    "vlm_scores = vlm_scores.masked_fill(vlm_mask, float('-inf'))\n",
    "vlm_attn = F.softmax(vlm_scores, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# LLM\n",
    "im0 = axes[0].imshow(llm_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[0].set_title('LLM: Causal Attention', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# ViT\n",
    "im1 = axes[1].imshow(vit_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[1].set_title('ViT: Bidirectional Attention', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key')\n",
    "axes[1].set_ylabel('Query')\n",
    "plt.colorbar(im1, ax=axes[1], shrink=0.8)\n",
    "\n",
    "# VLM\n",
    "im2 = axes[2].imshow(vlm_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[2].set_title('VLM: Mixed Attention (LLaVA-style)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Key')\n",
    "axes[2].set_ylabel('Query')\n",
    "axes[2].axhline(y=n_img - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "axes[2].axvline(x=n_img - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "plt.colorbar(im2, ax=axes[2], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Attention Pattern Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION PATTERN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Property':<25} {'LLM':<15} {'ViT':<15} {'VLM (LLaVA)'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mask Type':<25} {'Causal':<15} {'None':<15} {'Mixed'}\")\n",
    "print(f\"{'Upper Triangle':<25} {'Zeros':<15} {'Non-zero':<15} {'Partial zeros'}\")\n",
    "print(f\"{'Token-to-Token':<25} {'Past only':<15} {'All':<15} {'Depends on type'}\")\n",
    "print(f\"{'Cross-Modal':<25} {'N/A':<15} {'N/A':<15} {'Text→Image: Yes'}\")\n",
    "print(f\"{'Use Case':<25} {'Text gen':<15} {'Image cls':<15} {'Multimodal'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Interview Questions\n",
    "\n",
    "### Q1: How does attention differ between LLMs and VLMs?\n",
    "\n",
    "**Answer:**\n",
    "- **LLMs** use causal self-attention where each token can only attend to itself and previous tokens. This is enforced with a lower-triangular mask.\n",
    "- **VLMs** use mixed attention patterns:\n",
    "  - Image tokens use bidirectional attention (all patches see all patches)\n",
    "  - Text tokens use causal attention among themselves\n",
    "  - Text tokens can attend to all image tokens (cross-modal attention)\n",
    "  - Image tokens cannot attend to text tokens (they come first in sequence)\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: How does LLaVA handle image inputs?\n",
    "\n",
    "**Answer:**\n",
    "1. **Vision Encoder**: A pretrained ViT (e.g., CLIP ViT) encodes the image into patch embeddings\n",
    "2. **Projection Layer**: A simple MLP projects visual features from ViT dimension to LLM embedding dimension\n",
    "3. **Concatenation**: Visual tokens are prepended to text tokens: `[image_tokens] + [text_tokens]`\n",
    "4. **Processing**: The LLM processes the combined sequence with appropriate attention masking\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: What's the attention mask structure in a VLM?\n",
    "\n",
    "**Answer:**\n",
    "For a sequence with N image tokens and M text tokens:\n",
    "\n",
    "```\n",
    "                Image (N)     Text (M)\n",
    "Image (N)    [  All zeros     All ones  ]  <- Image sees image, not text\n",
    "Text (M)     [  All zeros     Causal    ]  <- Text sees image + causal text\n",
    "```\n",
    "\n",
    "Key insight: It's NOT simply a causal mask! The top-right quadrant is all ones (masked).\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: Why use a pretrained ViT instead of training from scratch?\n",
    "\n",
    "**Answer:**\n",
    "1. **Transfer Learning**: Visual representations from ImageNet pretraining generalize well\n",
    "2. **Compute Efficiency**: Training vision encoders requires massive compute (thousands of GPU hours)\n",
    "3. **Data Efficiency**: Pretrained ViT needs much less vision-language data for fine-tuning\n",
    "4. **Stability**: Pretrained weights provide stable gradients during training\n",
    "\n",
    "LLaVA specifically keeps the ViT frozen and only trains the projection layer + optionally the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: What's the difference between cross-attention (Flamingo) and concatenation (LLaVA)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Aspect | Cross-Attention (Flamingo) | Concatenation (LLaVA) |\n",
    "|--------|---------------------------|----------------------|\n",
    "| Architecture | Separate cross-attn layers | Single self-attn over concat |\n",
    "| Q, K, V | Q from text, K/V from image | All from same sequence |\n",
    "| Sequence Length | Text length only | Image + text length |\n",
    "| Complexity | More complex, extra layers | Simpler, reuses LLM |\n",
    "| Memory | Less (separate streams) | More (longer sequence) |\n",
    "\n",
    "LLaVA is simpler but has O((N+M)²) attention cost. Flamingo is more complex but keeps text attention O(M²).\n",
    "\n",
    "---\n",
    "\n",
    "### Q6: How do you extract attention weights from a pretrained VLM?\n",
    "\n",
    "**Answer:**\n",
    "```python\n",
    "# Option 1: Use output_attentions flag (HuggingFace)\n",
    "outputs = model(inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # Tuple of (batch, heads, seq, seq)\n",
    "\n",
    "# Option 2: Register forward hooks\n",
    "attention_weights = []\n",
    "def hook(module, input, output):\n",
    "    attention_weights.append(output[1])  # Capture attention weights\n",
    "\n",
    "for layer in model.encoder.layers:\n",
    "    layer.self_attn.register_forward_hook(hook)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q7: What's the computational difference between ViT and LLM attention?\n",
    "\n",
    "**Answer:**\n",
    "- **ViT**: O(N²) where N = number of patches (typically 196 for 224x224 with 16x16 patches)\n",
    "- **LLM**: O(M²) where M = sequence length (can be thousands of tokens)\n",
    "- **VLM (LLaVA)**: O((N+M)²) - quadratic in combined length\n",
    "\n",
    "This is why VLMs often use visual token reduction (pooling, resampling) to keep N small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. LLMs use CAUSAL attention (lower triangular mask)\n",
    "   - Each token sees only past tokens\n",
    "   - Used for autoregressive generation\n",
    "\n",
    "2. ViTs use BIDIRECTIONAL attention (no mask)\n",
    "   - All patches see all other patches\n",
    "   - Used for image understanding\n",
    "\n",
    "3. VLMs (LLaVA-style) use MIXED attention\n",
    "   - Image tokens: bidirectional among themselves\n",
    "   - Text tokens: causal + can see all image tokens\n",
    "   - The mask is NOT simply lower triangular!\n",
    "\n",
    "4. The key implementation insight:\n",
    "   create_vlm_attention_mask() must handle 4 quadrants:\n",
    "   - Image→Image: bidirectional (no mask)\n",
    "   - Image→Text: masked (can't see future)\n",
    "   - Text→Image: can attend (no mask)\n",
    "   - Text→Text: causal mask\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
