{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM vs LLM Attention: Understanding Vision-Language Model Attention\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Compare how attention works in **Vision-Language Models (VLMs)** versus **pure Language Models (LLMs)**. This notebook focuses on the **LLaVA-style architecture**, which is the most common approach in modern VLMs.\n",
    "\n",
    "### Background: Why VLM Attention is Different\n",
    "\n",
    "In pure LLMs:\n",
    "- Input: Text tokens only\n",
    "- Attention: Causal (each token attends only to previous tokens)\n",
    "- Mask: Lower triangular\n",
    "\n",
    "In Vision-Language Models:\n",
    "- Input: Image patches + text tokens\n",
    "- Attention: Mixed (bidirectional for image, causal for text)\n",
    "- Mask: NOT simply lower triangular\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: LLM Attention Review - Quick recap of causal self-attention\n",
    "2. **Part 2**: Vision Transformer (ViT) - Patch embedding and bidirectional attention\n",
    "3. **Part 3**: LLaVA-Style VLM - How image and text tokens interact\n",
    "4. **Part 4**: Attention Visualization - Real patterns from pretrained models\n",
    "5. **Part 5**: Side-by-Side Comparison - Visual comparison\n",
    "6. **Part 6**: Interview Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# For pretrained models\n",
    "try:\n",
    "    from transformers import CLIPModel, CLIPProcessor\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"transformers not installed - pretrained visualization will be skipped\")\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    HAS_PIL = True\n",
    "except ImportError:\n",
    "    HAS_PIL = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Attention Review\n",
    "\n",
    "In pure language models (GPT, LLaMA, etc.), attention is **causal**: each token can only attend to itself and previous tokens. This is enforced with a lower-triangular mask.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "where $M$ is the causal mask with $-\\infty$ in the upper triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask.\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor (seq_len, seq_len)\n",
    "              True = masked (cannot attend), False = can attend\n",
    "    \n",
    "    Hint: Use torch.triu() with diagonal=1 to create upper triangular mask\n",
    "    \"\"\"\n",
    "    # TODO: Implement causal mask\n",
    "    # The mask should have True in the upper triangle (positions that cannot be attended to)\n",
    "    ...\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    q: torch.Tensor, \n",
    "    k: torch.Tensor, \n",
    "    v: torch.Tensor, \n",
    "    mask: Optional[torch.Tensor] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        q: Query (batch, seq_q, d_k)\n",
    "        k: Key (batch, seq_k, d_k)\n",
    "        v: Value (batch, seq_k, d_v)\n",
    "        mask: Boolean mask where True = masked\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_q, d_v)\n",
    "        attn_weights: (batch, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    \n",
    "    # TODO: Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = ...\n",
    "    \n",
    "    # TODO: Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = ...\n",
    "    \n",
    "    # TODO: Apply softmax and compute output\n",
    "    attn_weights = ...\n",
    "    output = ...\n",
    "    \n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask for LLM (True = cannot attend):\")\n",
    "print(causal_mask.int())\n",
    "\n",
    "# Verify\n",
    "assert causal_mask.shape == (seq_len, seq_len), f\"Wrong shape: {causal_mask.shape}\"\n",
    "assert causal_mask[0, 0] == False, \"Position (0,0) should be False (can attend to self)\"\n",
    "assert causal_mask[0, 1] == True, \"Position (0,1) should be True (cannot attend to future)\"\n",
    "assert causal_mask[7, 0] == False, \"Position (7,0) should be False (can attend to past)\"\n",
    "print(\"\\n✓ Causal mask test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate causal attention pattern\n",
    "torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "d_model = 64\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "_, attn_weights_causal = scaled_dot_product_attention(q, k, v, mask=causal_mask)\n",
    "\n",
    "# Verify upper triangle is zero\n",
    "upper_triangle = attn_weights_causal[0].triu(diagonal=1)\n",
    "assert torch.allclose(upper_triangle, torch.zeros_like(upper_triangle), atol=1e-6), \\\n",
    "    \"Upper triangle should be zero!\"\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(attn_weights_causal[0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "ax.set_title('LLM: Causal Self-Attention', fontsize=12)\n",
    "ax.set_xlabel('Key Position (past → future)')\n",
    "ax.set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Upper triangle is zero (cannot attend to future tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vision Transformer (ViT) Basics\n",
    "\n",
    "In Vision Transformers, images are converted to a sequence of **patch embeddings**, then processed with **bidirectional** self-attention (no causal mask).\n",
    "\n",
    "### Key Steps:\n",
    "1. **Patch Embedding**: Split image into patches, flatten, project to embedding dim\n",
    "2. **Position Encoding**: Add 2D positional information\n",
    "3. **CLS Token**: Prepend a learnable classification token\n",
    "4. **Bidirectional Attention**: All patches attend to all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into a sequence of patch embeddings.\n",
    "    \n",
    "    Image (B, C, H, W) -> Patches (B, N_patches, embed_dim)\n",
    "    \n",
    "    For a 224x224 image with 16x16 patches:\n",
    "    - N_patches = (224/16) * (224/16) = 14 * 14 = 196\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size: int = 224, \n",
    "        patch_size: int = 16, \n",
    "        in_channels: int = 3, \n",
    "        embed_dim: int = 768\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # TODO: Create a Conv2d layer that acts as patch extraction and projection\n",
    "        # Hint: Use kernel_size=patch_size and stride=patch_size for non-overlapping patches\n",
    "        # This is more efficient than manually splitting and projecting\n",
    "        self.proj = ...\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Image tensor (batch, channels, height, width)\n",
    "        Returns:\n",
    "            patches: (batch, n_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Apply projection, flatten spatial dims, transpose\n",
    "        # 1. Apply conv: (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        # 2. Flatten: (B, embed_dim, H/P, W/P) -> (B, embed_dim, N_patches)\n",
    "        # 3. Transpose: (B, embed_dim, N_patches) -> (B, N_patches, embed_dim)\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patch embedding\n",
    "torch.manual_seed(42)\n",
    "\n",
    "patch_embed = PatchEmbedding(img_size=224, patch_size=16, embed_dim=768)\n",
    "\n",
    "# Simulate an image\n",
    "dummy_image = torch.randn(1, 3, 224, 224)\n",
    "patches = patch_embed(dummy_image)\n",
    "\n",
    "print(f\"Input image shape: {dummy_image.shape}\")\n",
    "print(f\"Output patches shape: {patches.shape}\")\n",
    "print(f\"Number of patches: {patches.shape[1]} = 14 x 14 grid\")\n",
    "print(f\"Each patch embedding dim: {patches.shape[2]}\")\n",
    "\n",
    "assert patches.shape == (1, 196, 768), f\"Wrong output shape: {patches.shape}\"\n",
    "print(\"\\n✓ Patch embedding test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention for Vision Transformer.\n",
    "    \n",
    "    Key difference from LLM attention: NO CAUSAL MASK\n",
    "    All patches can attend to all other patches (bidirectional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 768, num_heads: int = 12):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Combined Q, K, V projection for efficiency\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, embed_dim)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attn_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # TODO: Compute Q, K, V from single projection\n",
    "        # 1. Apply qkv projection: (B, N, C) -> (B, N, 3*C)\n",
    "        # 2. Reshape to (B, N, 3, num_heads, head_dim)\n",
    "        # 3. Permute to (3, B, num_heads, N, head_dim)\n",
    "        # 4. Split into q, k, v\n",
    "        qkv = ...\n",
    "        q, k, v = ...\n",
    "        \n",
    "        # TODO: Compute scaled dot-product attention (NO MASK!)\n",
    "        # This is the key difference from LLM attention - bidirectional\n",
    "        scores = ...\n",
    "        attn_weights = ...\n",
    "        \n",
    "        # TODO: Apply attention to values and reshape back\n",
    "        out = ...\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ViT bidirectional attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use smaller dimensions for visualization\n",
    "n_patches = 16  # 4x4 grid for easy visualization\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "vit_attn = ViTAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "patch_tokens = torch.randn(1, n_patches, embed_dim)\n",
    "\n",
    "_, vit_attn_weights = vit_attn(patch_tokens)\n",
    "\n",
    "# Plot attention from head 0\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(vit_attn_weights[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.3)\n",
    "ax.set_title('ViT: Bidirectional Attention (Head 0)', fontsize=12)\n",
    "ax.set_xlabel('Key Patch')\n",
    "ax.set_ylabel('Query Patch')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: NO zeros in the matrix - all patches attend to all patches!\")\n",
    "print(\"This is bidirectional attention (no causal mask).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LLaVA-Style VLM Architecture\n",
    "\n",
    "### How LLaVA Works\n",
    "\n",
    "LLaVA (Large Language and Vision Assistant) uses a simple but effective approach:\n",
    "\n",
    "1. **Vision Encoder**: Pretrained ViT extracts visual features\n",
    "2. **Projection Layer**: Maps visual features to LLM embedding space\n",
    "3. **Concatenation**: `[Image Tokens] + [Text Tokens]`\n",
    "4. **LLM Processing**: Single decoder-only transformer processes both\n",
    "\n",
    "### The Key Insight: VLM Attention Mask\n",
    "\n",
    "The attention mask in a VLM is **NOT simply causal**:\n",
    "\n",
    "```\n",
    "                    Image Tokens    Text Tokens\n",
    "                    [I0 I1 I2 I3]   [T0 T1 T2 T3]\n",
    "Image   I0          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "Tokens  I1          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "        I2          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "        I3          [ ✓  ✓  ✓  ✓     ✗  ✗  ✗  ✗ ]\n",
    "Text    T0          [ ✓  ✓  ✓  ✓     ✓  ✗  ✗  ✗ ]\n",
    "Tokens  T1          [ ✓  ✓  ✓  ✓     ✓  ✓  ✗  ✗ ]\n",
    "        T2          [ ✓  ✓  ✓  ✓     ✓  ✓  ✓  ✗ ]\n",
    "        T3          [ ✓  ✓  ✓  ✓     ✓  ✓  ✓  ✓ ]\n",
    "```\n",
    "\n",
    "- **Image-to-Image**: Bidirectional (all can see all)\n",
    "- **Image-to-Text**: Cannot attend (image comes first, doesn't \"see\" future text)\n",
    "- **Text-to-Image**: Can attend (text can see all image tokens)\n",
    "- **Text-to-Text**: Causal (each text token sees only past text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vlm_attention_mask(n_image: int, n_text: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create attention mask for Vision-Language Model (LLaVA-style).\n",
    "    \n",
    "    The mask has a specific structure:\n",
    "    - Image tokens: bidirectional among themselves (can see all image tokens)\n",
    "    - Image tokens: CANNOT attend to text tokens (they come first in sequence)\n",
    "    - Text tokens: CAN attend to all image tokens\n",
    "    - Text tokens: causal among themselves (can only see past text)\n",
    "    \n",
    "    Args:\n",
    "        n_image: Number of image tokens (patches)\n",
    "        n_text: Number of text tokens\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor (n_image + n_text, n_image + n_text)\n",
    "              True = masked (cannot attend), False = can attend\n",
    "    \n",
    "    The mask should look like:\n",
    "    \n",
    "        Image    Text\n",
    "    Image [0 0 0 | 1 1 1]   <- Image sees image (0), not text (1)\n",
    "          [0 0 0 | 1 1 1]\n",
    "          ------+------\n",
    "    Text  [0 0 0 | 0 1 1]   <- Text sees image (0), causal text\n",
    "          [0 0 0 | 0 0 1]\n",
    "          [0 0 0 | 0 0 0]\n",
    "    \"\"\"\n",
    "    total = n_image + n_text\n",
    "    \n",
    "    # TODO: Create the VLM attention mask\n",
    "    # Start with all zeros (can attend)\n",
    "    mask = torch.zeros(total, total, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # TODO: Image-to-text quadrant: MASKED (top-right)\n",
    "    # Image tokens cannot see text tokens (they come before text)\n",
    "    ...\n",
    "    \n",
    "    # TODO: Text-to-text quadrant: CAUSAL (bottom-right)\n",
    "    # Text tokens have causal attention among themselves\n",
    "    ...\n",
    "    \n",
    "    # Note: Image-to-image (top-left) stays zeros (bidirectional)\n",
    "    # Note: Text-to-image (bottom-left) stays zeros (can attend)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VLM attention mask\n",
    "n_image = 4\n",
    "n_text = 4\n",
    "\n",
    "vlm_mask = create_vlm_attention_mask(n_image, n_text)\n",
    "\n",
    "print(\"VLM Attention Mask (1 = CANNOT attend):\")\n",
    "print(vlm_mask.int())\n",
    "\n",
    "# Verify structure\n",
    "# Top-left (image-to-image): all zeros\n",
    "assert vlm_mask[:n_image, :n_image].sum() == 0, \"Image-to-image should be all zeros (bidirectional)\"\n",
    "\n",
    "# Top-right (image-to-text): all ones\n",
    "assert vlm_mask[:n_image, n_image:].sum() == n_image * n_text, \"Image-to-text should be all ones (masked)\"\n",
    "\n",
    "# Bottom-left (text-to-image): all zeros\n",
    "assert vlm_mask[n_image:, :n_image].sum() == 0, \"Text-to-image should be all zeros (can attend)\"\n",
    "\n",
    "# Bottom-right (text-to-text): causal (upper triangle)\n",
    "expected_causal = n_text * (n_text - 1) // 2  # Upper triangle count\n",
    "assert vlm_mask[n_image:, n_image:].sum() == expected_causal, \"Text-to-text should be causal\"\n",
    "\n",
    "print(\"\\n✓ VLM mask test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mask structure\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Convert to float for visualization (1 = masked/red, 0 = can attend/white)\n",
    "mask_viz = vlm_mask.float().numpy()\n",
    "\n",
    "im = ax.imshow(mask_viz, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "ax.set_title('VLM Attention Mask Structure', fontsize=12)\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "\n",
    "# Add grid lines to separate image and text regions\n",
    "ax.axhline(y=n_image - 0.5, color='black', linewidth=2)\n",
    "ax.axvline(x=n_image - 0.5, color='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "ax.text(n_image/2 - 0.5, -0.8, 'Image', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(n_image + n_text/2 - 0.5, -0.8, 'Text', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.text(-1.2, n_image/2 - 0.5, 'Image', va='center', fontsize=10, fontweight='bold', rotation=90)\n",
    "ax.text(-1.2, n_image + n_text/2 - 0.5, 'Text', va='center', fontsize=10, fontweight='bold', rotation=90)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8, label='Masked')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Project visual features from ViT to LLM embedding space.\n",
    "    \n",
    "    In LLaVA, this is typically a simple MLP:\n",
    "    vision_dim -> hidden_dim -> llm_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vision_dim: int, llm_dim: int, hidden_dim: int = None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = llm_dim\n",
    "        \n",
    "        # TODO: Create a 2-layer MLP with GELU activation\n",
    "        # vision_dim -> hidden_dim -> GELU -> llm_dim\n",
    "        self.proj = ...\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project vision features to LLM space.\"\"\"\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision-Language Model attention with proper masking.\n",
    "    \n",
    "    Processes concatenated [image_tokens, text_tokens] with:\n",
    "    - Bidirectional attention among image tokens\n",
    "    - Image tokens cannot attend to text tokens\n",
    "    - Text tokens can attend to all image tokens\n",
    "    - Causal attention among text tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        n_image_tokens: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Concatenated [image_tokens, text_tokens] (batch, seq_len, d_model)\n",
    "            n_image_tokens: Number of image tokens (to construct proper mask)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attn_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.shape\n",
    "        n_text_tokens = N - n_image_tokens\n",
    "        \n",
    "        # TODO: Project to Q, K, V and reshape for multi-head attention\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "        \n",
    "        # TODO: Compute attention scores\n",
    "        scores = ...\n",
    "        \n",
    "        # TODO: Create and apply VLM mask\n",
    "        mask = create_vlm_attention_mask(n_image_tokens, n_text_tokens, device=x.device)\n",
    "        scores = ...\n",
    "        \n",
    "        # TODO: Apply softmax and compute output\n",
    "        attn_weights = ...\n",
    "        out = ...\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VLM attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "n_image = 9  # 3x3 patch grid\n",
    "n_text = 6\n",
    "\n",
    "vlm_attn = VLMAttention(d_model, num_heads)\n",
    "\n",
    "# Simulate concatenated image + text tokens\n",
    "image_tokens = torch.randn(1, n_image, d_model)\n",
    "text_tokens = torch.randn(1, n_text, d_model)\n",
    "combined = torch.cat([image_tokens, text_tokens], dim=1)\n",
    "\n",
    "print(f\"Image tokens: {n_image}\")\n",
    "print(f\"Text tokens: {n_text}\")\n",
    "print(f\"Combined sequence: {combined.shape}\")\n",
    "\n",
    "output, attn_weights = vlm_attn(combined, n_image_tokens=n_image)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "assert output.shape == combined.shape, f\"Output shape mismatch: {output.shape}\"\n",
    "print(\"\\n✓ VLM attention test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VLM attention pattern\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Average across heads\n",
    "attn_avg = attn_weights[0].mean(dim=0).detach().numpy()\n",
    "\n",
    "im = ax.imshow(attn_avg, cmap='Blues', vmin=0, vmax=0.3)\n",
    "ax.set_title('VLM Attention Pattern (LLaVA-style)', fontsize=12)\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "\n",
    "# Add grid lines\n",
    "ax.axhline(y=n_image - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "ax.axvline(x=n_image - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Add labels\n",
    "ax.text(n_image/2, -1, 'Image Tokens', ha='center', fontsize=10)\n",
    "ax.text(n_image + n_text/2, -1, 'Text Tokens', ha='center', fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8, label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Top-left (Image→Image): Dense attention, all patches see each other\")\n",
    "print(\"- Top-right (Image→Text): All zeros (image can't see future text)\")\n",
    "print(\"- Bottom-left (Text→Image): Dense attention, text sees all image patches\")\n",
    "print(\"- Bottom-right (Text→Text): Causal pattern (lower triangular)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attention Visualization with Pretrained Model\n",
    "\n",
    "Let's use a pretrained CLIP model to visualize real ViT attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS and HAS_PIL:\n",
    "    print(\"Loading CLIP model for attention visualization...\")\n",
    "    \n",
    "    # Load a small CLIP model\n",
    "    model_name = \"openai/clip-vit-base-patch16\"\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Create a simple test image (gradient for easy visualization)\n",
    "    size = 224\n",
    "    x = np.linspace(0, 1, size)\n",
    "    y = np.linspace(0, 1, size)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    \n",
    "    # Create RGB image with gradient patterns\n",
    "    r = (xx * 255).astype(np.uint8)\n",
    "    g = (yy * 255).astype(np.uint8)\n",
    "    b = ((xx + yy) / 2 * 255).astype(np.uint8)\n",
    "    test_image = np.stack([r, g, b], axis=-1)\n",
    "    pil_image = Image.fromarray(test_image)\n",
    "    \n",
    "    print(f\"Test image size: {test_image.shape}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Process and get attention\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.vision_model(\n",
    "            inputs['pixel_values'], \n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    attentions = outputs.attentions\n",
    "    print(f\"\\nNumber of layers: {len(attentions)}\")\n",
    "    print(f\"Attention shape per layer: {attentions[0].shape}\")\n",
    "else:\n",
    "    print(\"Skipping pretrained visualization (transformers or PIL not available)\")\n",
    "    attentions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attentions is not None:\n",
    "    # Visualize attention from different layers\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "    \n",
    "    layers_to_show = [0, 3, 6, 9, 11, -1]\n",
    "    layer_names = ['Layer 1', 'Layer 4', 'Layer 7', 'Layer 10', 'Layer 12', 'Last Layer']\n",
    "    \n",
    "    for idx, (layer_idx, name) in enumerate(zip(layers_to_show, layer_names)):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Get attention for this layer, average across heads\n",
    "        attn = attentions[layer_idx][0].mean(dim=0)\n",
    "        \n",
    "        im = ax.imshow(attn.numpy(), cmap='Blues', vmin=0, vmax=0.1)\n",
    "        ax.set_title(f'{name}', fontsize=11)\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "    \n",
    "    plt.suptitle('ViT Attention Patterns Across Layers (CLIP)', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- Early layers: More uniform attention, local patterns\")\n",
    "    print(\"- Later layers: More specialized, global patterns\")\n",
    "    print(\"- CLS token (row/col 0): Aggregates information from all patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Side-by-Side Comparison\n",
    "\n",
    "Compare all three attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "torch.manual_seed(42)\n",
    "\n",
    "seq_len = 12\n",
    "d_model = 64\n",
    "\n",
    "q = torch.randn(1, seq_len, d_model)\n",
    "k = torch.randn(1, seq_len, d_model)\n",
    "v = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "raw_scores = (q @ k.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "\n",
    "# 1. LLM (Causal) attention\n",
    "llm_mask = create_causal_mask(seq_len)\n",
    "llm_scores = raw_scores.clone().masked_fill(llm_mask, float('-inf'))\n",
    "llm_attn = F.softmax(llm_scores, dim=-1)[0]\n",
    "\n",
    "# 2. ViT (Bidirectional) attention\n",
    "vit_attn = F.softmax(raw_scores, dim=-1)[0]\n",
    "\n",
    "# 3. VLM attention (6 image + 6 text tokens)\n",
    "n_img, n_txt = 6, 6\n",
    "vlm_mask = create_vlm_attention_mask(n_img, n_txt)\n",
    "vlm_scores = raw_scores.clone().masked_fill(vlm_mask, float('-inf'))\n",
    "vlm_attn = F.softmax(vlm_scores, dim=-1)[0]\n",
    "\n",
    "# Plot side-by-side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "im0 = axes[0].imshow(llm_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[0].set_title('LLM: Causal Attention', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8)\n",
    "\n",
    "im1 = axes[1].imshow(vit_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[1].set_title('ViT: Bidirectional Attention', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key')\n",
    "axes[1].set_ylabel('Query')\n",
    "plt.colorbar(im1, ax=axes[1], shrink=0.8)\n",
    "\n",
    "im2 = axes[2].imshow(vlm_attn.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
    "axes[2].set_title('VLM: Mixed Attention (LLaVA-style)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Key')\n",
    "axes[2].set_ylabel('Query')\n",
    "axes[2].axhline(y=n_img - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "axes[2].axvline(x=n_img - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "plt.colorbar(im2, ax=axes[2], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Attention Pattern Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\"*70)\n",
    "print(\"ATTENTION PATTERN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Property':<25} {'LLM':<15} {'ViT':<15} {'VLM (LLaVA)'}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mask Type':<25} {'Causal':<15} {'None':<15} {'Mixed'}\")\n",
    "print(f\"{'Upper Triangle':<25} {'Zeros':<15} {'Non-zero':<15} {'Partial zeros'}\")\n",
    "print(f\"{'Token-to-Token':<25} {'Past only':<15} {'All':<15} {'Depends on type'}\")\n",
    "print(f\"{'Cross-Modal':<25} {'N/A':<15} {'N/A':<15} {'Text→Image: Yes'}\")\n",
    "print(f\"{'Use Case':<25} {'Text gen':<15} {'Image cls':<15} {'Multimodal'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Interview Questions\n",
    "\n",
    "### Q1: How does attention differ between LLMs and VLMs?\n",
    "\n",
    "**Answer:**\n",
    "- **LLMs** use causal self-attention where each token can only attend to itself and previous tokens. This is enforced with a lower-triangular mask.\n",
    "- **VLMs** use mixed attention patterns:\n",
    "  - Image tokens use bidirectional attention (all patches see all patches)\n",
    "  - Text tokens use causal attention among themselves\n",
    "  - Text tokens can attend to all image tokens (cross-modal attention)\n",
    "  - Image tokens cannot attend to text tokens (they come first in sequence)\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: How does LLaVA handle image inputs?\n",
    "\n",
    "**Answer:**\n",
    "1. **Vision Encoder**: A pretrained ViT encodes the image into patch embeddings\n",
    "2. **Projection Layer**: A simple MLP projects visual features to LLM embedding dimension\n",
    "3. **Concatenation**: Visual tokens are prepended to text tokens\n",
    "4. **Processing**: The LLM processes the combined sequence with appropriate attention masking\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: What's the attention mask structure in a VLM?\n",
    "\n",
    "**Answer:**\n",
    "For a sequence with N image tokens and M text tokens:\n",
    "\n",
    "```\n",
    "                Image (N)     Text (M)\n",
    "Image (N)    [  All zeros     All ones  ]  <- Image sees image, not text\n",
    "Text (M)     [  All zeros     Causal    ]  <- Text sees image + causal text\n",
    "```\n",
    "\n",
    "Key insight: It's NOT simply a causal mask! The top-right quadrant is all ones (masked).\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: Why use a pretrained ViT instead of training from scratch?\n",
    "\n",
    "**Answer:**\n",
    "1. **Transfer Learning**: Visual representations generalize well\n",
    "2. **Compute Efficiency**: Training vision encoders requires massive compute\n",
    "3. **Data Efficiency**: Pretrained ViT needs less vision-language data\n",
    "4. **Stability**: Pretrained weights provide stable gradients\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: What's the difference between cross-attention (Flamingo) and concatenation (LLaVA)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Aspect | Cross-Attention (Flamingo) | Concatenation (LLaVA) |\n",
    "|--------|---------------------------|----------------------|\n",
    "| Architecture | Separate cross-attn layers | Single self-attn over concat |\n",
    "| Q, K, V | Q from text, K/V from image | All from same sequence |\n",
    "| Sequence Length | Text length only | Image + text length |\n",
    "| Complexity | More complex | Simpler |\n",
    "| Memory | Less (separate streams) | More (longer sequence) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. LLMs use CAUSAL attention (lower triangular mask)\n",
    "   - Each token sees only past tokens\n",
    "\n",
    "2. ViTs use BIDIRECTIONAL attention (no mask)\n",
    "   - All patches see all other patches\n",
    "\n",
    "3. VLMs (LLaVA-style) use MIXED attention\n",
    "   - Image tokens: bidirectional among themselves\n",
    "   - Text tokens: causal + can see all image tokens\n",
    "   - The mask is NOT simply lower triangular!\n",
    "\n",
    "4. The key implementation insight:\n",
    "   create_vlm_attention_mask() must handle 4 quadrants:\n",
    "   - Image→Image: bidirectional (no mask)\n",
    "   - Image→Text: masked (can't see future)\n",
    "   - Text→Image: can attend (no mask)\n",
    "   - Text→Text: causal mask\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
