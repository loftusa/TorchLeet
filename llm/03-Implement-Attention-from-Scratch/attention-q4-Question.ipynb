{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Attention from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Implement a **Scaled Dot-Product Attention** mechanism from scratch using PyTorch. This core component is essential in Transformer architectures and helps models focus on relevant parts of a sequence.\n",
    "\n",
    "### Background: Where Do Q, K, V Come From?\n",
    "\n",
    "In a Transformer, Q (Query), K (Key), and V (Value) are **not separate inputs**. They all come from the **same input** (the residual stream) through learned linear projections:\n",
    "\n",
    "```\n",
    "x = input tensor (batch, seq_len, d_model)  # The residual stream\n",
    "\n",
    "Q = x @ W_q  # Query projection\n",
    "K = x @ W_k  # Key projection  \n",
    "V = x @ W_v  # Value projection\n",
    "```\n",
    "\n",
    "This is called **self-attention** because the model attends to itself - the queries, keys, and values all come from the same source.\n",
    "\n",
    "### The Math\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: Core attention math (Q, K, V given) - isolate the mechanism\n",
    "2. **Part 2**: Attention mask creation (causal, padding, combined, KV cache)\n",
    "3. **Part 3**: Self-Attention class - Q, K, V from projections of single input x\n",
    "4. **Part 4**: Visualizing attention patterns\n",
    "5. **Part 5**: KV Cache for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Core Attention Math\n",
    "\n",
    "First, implement the core attention computation assuming Q, K, V are already given.\n",
    "This isolates the attention mechanism itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([2, 8, 64])\n",
      "K shape: torch.Size([2, 8, 64])\n",
      "V shape: torch.Size([2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_k = 64\n",
    "\n",
    "# For Part 1, we provide Q, K, V directly to focus on the attention math\n",
    "q = torch.randn(batch_size, seq_len, d_k)\n",
    "k = torch.randn(batch_size, seq_len, d_k)\n",
    "v = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "print(f\"Q shape: {q.shape}\")\n",
    "print(f\"K shape: {k.shape}\")\n",
    "print(f\"V shape: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "from math import inf, sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        q: Query tensor of shape (batch, seq_len_q, d_k)\n",
    "        k: Key tensor of shape (batch, seq_len_k, d_k)\n",
    "        v: Value tensor of shape (batch, seq_len_k, d_v)\n",
    "        mask: Optional boolean mask tensor where True indicates positions to mask out\n",
    "\n",
    "    Returns:\n",
    "        output: Attention output of shape (batch, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights of shape (batch, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d = q.shape[-1]\n",
    "    A = torch.einsum(\"bsd,btd -> bst\", [q, k])\n",
    "    A /= sqrt(d)\n",
    "    if mask is not None:\n",
    "        A = torch.masked_fill(A, mask=mask, value=-inf)\n",
    "    A = F.softmax(A, dim=-1)\n",
    "    Av = torch.einsum(\"bst,btd -> bsd\", [A, v])\n",
    "\n",
    "    return Av, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 8, 64])\n",
      "Attention weights shape: torch.Size([2, 8, 8])\n",
      "\n",
      "✓ Core attention test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test against PyTorch's implementation\n",
    "output_custom, attn_weights = scaled_dot_product_attention(q, k, v)\n",
    "output_ref = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(f\"Output shape: {output_custom.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "assert torch.allclose(output_custom, output_ref, atol=1e-6), \"Outputs don't match!\"\n",
    "print(\"\\n\\u2713 Core attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Attention Mask Creation\n",
    "\n",
    "Attention masks control which positions can attend to which other positions. Two common types:\n",
    "\n",
    "1. **Causal Mask**: For autoregressive models (GPT, LLaMA), prevents attending to future tokens\n",
    "2. **Padding Mask**: For batched sequences of different lengths, prevents attending to padding tokens\n",
    "\n",
    "Mask convention: **True = masked (cannot attend), False = can attend**\n",
    "\n",
    "This matches PyTorch's `masked_fill()` behavior directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Causal Mask\n",
    "\n",
    "A causal mask prevents attention to future positions - essential for autoregressive models like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(\n",
    "    seq_len_q: int, seq_len_k: int = None, device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask for autoregressive models.\n",
    "\n",
    "    In autoregressive generation, each position can only attend to previous positions\n",
    "    (including itself). This prevents \"looking into the future.\"\n",
    "\n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length (defaults to seq_len_q)\n",
    "        device: Device to create tensor on\n",
    "\n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "              True = position should be MASKED (cannot attend)\n",
    "              False = position can be attended to\n",
    "\n",
    "    Example for seq_len=4:\n",
    "        Q\\\\K   0      1      2      3\n",
    "        0   [False, True,  True,  True ]   # Query 0 attends only to Key 0\n",
    "        1   [False, False, True,  True ]   # Query 1 attends to Keys 0-1\n",
    "        2   [False, False, False, True ]   # Query 2 attends to Keys 0-2\n",
    "        3   [False, False, False, False]   # Query 3 attends to Keys 0-3\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "    mask = torch.ones((seq_len_q, seq_len_k))\n",
    "    mask = torch.triu(mask, diagonal=1).to(bool)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask (4x4):\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "True = masked (cannot attend), False = can attend\n",
      "\n",
      "Asymmetric mask (2 queries, 4 keys):\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True]])\n",
      "\n",
      "✓ Causal mask test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test causal mask creation\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal mask (4x4):\")\n",
    "print(mask)\n",
    "print(\"\\nTrue = masked (cannot attend), False = can attend\")\n",
    "\n",
    "# Verify properties\n",
    "assert mask.shape == (4, 4)\n",
    "assert mask[0, 0] == False, \"Position (0,0) should NOT be masked (can attend to self)\"\n",
    "assert mask[0, 1] == True, \"Position (0,1) should be masked (cannot attend to future)\"\n",
    "assert mask[3, 0] == False, \"Position (3,0) should NOT be masked (can attend to past)\"\n",
    "assert mask.sum() == 6, \"Upper triangle should have 6 masked elements\"\n",
    "\n",
    "# Test with different Q/K lengths\n",
    "mask_asymm = create_causal_mask(2, 4)\n",
    "print(f\"\\nAsymmetric mask (2 queries, 4 keys):\")\n",
    "print(mask_asymm)\n",
    "assert mask_asymm.shape == (2, 4)\n",
    "\n",
    "print(\"\\n\\u2713 Causal mask test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Padding Mask\n",
    "\n",
    "When batching sequences of different lengths, shorter sequences are padded. The padding tokens should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a padding mask for variable-length sequences in a batch.\n",
    "\n",
    "    When batching sequences of different lengths, shorter sequences are padded.\n",
    "    The padding tokens should not be attended to.\n",
    "\n",
    "    Args:\n",
    "        lengths: Tensor of shape (batch_size,) containing actual sequence lengths\n",
    "        max_len: Maximum sequence length (padded length)\n",
    "\n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (batch_size, max_len)\n",
    "              True = padding position (should be MASKED)\n",
    "              False = real token (can be attended to)\n",
    "\n",
    "    Example:\n",
    "        lengths = [3, 5, 2], max_len = 5\n",
    "\n",
    "        Returns:\n",
    "        [[False, False, False, True,  True ],   # seq 0: tokens 0-2 real, 3-4 padding\n",
    "         [False, False, False, False, False],   # seq 1: all 5 tokens real\n",
    "         [False, False, True,  True,  True ]]   # seq 2: tokens 0-1 real, 2-4 padding\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.arange(max_len)[None, :] >= lengths[:, None]\n",
    "    )  # (1, m) x (b, 1) -> (b, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = torch.tensor([3, 5, 2])\n",
    "max_len = 5\n",
    "torch.arange(max_len)[None, :] >= lengths[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding mask (batch=3, max_len=5):\n",
      "Sequence lengths: [3, 5, 2]\n",
      "tensor([[False, False, False,  True,  True],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False,  True,  True,  True]])\n",
      "\n",
      "✓ Padding mask test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test padding mask creation\n",
    "lengths = torch.tensor([3, 5, 2])\n",
    "mask = create_padding_mask(lengths, max_len=5)\n",
    "print(\"Padding mask (batch=3, max_len=5):\")\n",
    "print(f\"Sequence lengths: {lengths.tolist()}\")\n",
    "print(mask)\n",
    "\n",
    "# Verify\n",
    "assert mask.shape == (3, 5)\n",
    "assert mask[0].tolist() == [False, False, False, True, True]\n",
    "assert mask[1].tolist() == [False, False, False, False, False]\n",
    "assert mask[2].tolist() == [False, False, True, True, True]\n",
    "\n",
    "print(\"\\n\\u2713 Padding mask test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Combined Attention Mask\n",
    "\n",
    "In practice, you often need both causal masking AND padding masking together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros((3, 4), dtype=torch.bool)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = (\n",
    "    torch.arange(max_len)[None, :] >= key_padding_lengths[:, None]\n",
    ")  # (batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int = None,\n",
    "    is_causal: bool = True,\n",
    "    key_padding_lengths: torch.Tensor = None,\n",
    "    device=None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a combined attention mask handling both causality and padding.\n",
    "\n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length\n",
    "        is_causal: Whether to apply causal masking\n",
    "        key_padding_lengths: If provided, actual lengths of key sequences (batch_size,)\n",
    "        device: Device to create tensor on\n",
    "\n",
    "    Returns:\n",
    "        mask: Boolean tensor, True = masked position\n",
    "              Shape: (seq_len_q, seq_len_k) if no padding\n",
    "              Shape: (batch_size, seq_len_q, seq_len_k) if padding provided\n",
    "    \"\"\"\n",
    "    # causal mask\n",
    "    mask = torch.ones((seq_len_q, seq_len_k))\n",
    "    if is_causal:\n",
    "        mask = torch.triu(mask, diagonal=1)  # (seq_len_q, seq_len_k)\n",
    "    mask = mask.to(torch.bool)\n",
    "\n",
    "    # padding mask\n",
    "    max_len = seq_len_k\n",
    "    assert max_len == seq_len_k\n",
    "    if key_padding_lengths is None:\n",
    "        return mask\n",
    "    else:\n",
    "        padding_mask = (\n",
    "            torch.arange(max_len)[None, :] >= key_padding_lengths[:, None]\n",
    "        )  # (batch_size, max_len)\n",
    "\n",
    "    # make out, a (batch_size, seq_len_q, max_len) tensor.\n",
    "    # only the first key_padding_lengths[i] columns should be unmasked for out[i, ...]\n",
    "    out = padding_mask[:, None, :] & mask[None, :, :]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test combined mask: causal only\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mask_causal = \u001b[43mcreate_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCausal-only mask:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(mask_causal)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcreate_attention_mask\u001b[39m\u001b[34m(seq_len_q, seq_len_k, is_causal, key_padding_lengths, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_attention_mask\u001b[39m(\n\u001b[32m      2\u001b[39m     seq_len_q: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      3\u001b[39m     seq_len_k: \u001b[38;5;28mint\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     device=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      7\u001b[39m ) -> torch.Tensor:\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Create a combined attention mask handling both causality and padding.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m              Shape: (batch_size, seq_len_q, seq_len_k) if padding provided\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     mask = torch.triu(mask, diagonal=\u001b[32m1\u001b[39m).to(torch.bool)  \u001b[38;5;66;03m# (seq_len_q, seq_len_k)\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_lengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: ones(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\""
     ]
    }
   ],
   "source": [
    "# Test combined mask: causal only\n",
    "mask_causal = create_attention_mask(4, is_causal=True)\n",
    "print(\"Causal-only mask:\")\n",
    "print(mask_causal)\n",
    "\n",
    "# Test combined mask: causal + padding\n",
    "lengths = torch.tensor([4, 3])  # batch of 2, one full, one with padding\n",
    "mask_combined = create_attention_mask(4, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"\\nCombined causal + padding mask (batch=2):\")\n",
    "print(f\"Key lengths: {lengths.tolist()}\")\n",
    "print(f\"Batch 0 (full length):\")\n",
    "print(mask_combined[0])\n",
    "print(f\"Batch 1 (length=3, position 3 is padding):\")\n",
    "print(mask_combined[1])\n",
    "\n",
    "# Verify batch 1 has position 3 masked for all queries\n",
    "assert mask_combined[1, :, 3].all(), \"Padding position should be masked for all queries\"\n",
    "\n",
    "print(\"\\n\\u2713 Combined mask test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Causal Mask with KV Cache\n",
    "\n",
    "During autoregressive generation with KV cache, the mask needs special handling:\n",
    "- New queries can attend to ALL cached positions (they're in the past)\n",
    "- Causal masking only applies within the new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask_with_cache(\n",
    "    seq_len_q: int, seq_len_k: int, cache_len: int, device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for attention with KV cache.\n",
    "\n",
    "    During autoregressive generation with KV cache:\n",
    "    - Query has only new tokens (usually 1 during generation)\n",
    "    - Keys include cached tokens + new tokens\n",
    "    - New queries can attend to ALL cached keys (they're in the past)\n",
    "\n",
    "    Args:\n",
    "        seq_len_q: Number of new query tokens (typically 1 during generation)\n",
    "        seq_len_k: Total key length (cache_len + seq_len_q)\n",
    "        cache_len: Number of cached tokens\n",
    "        device: Device to create tensor on\n",
    "\n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "\n",
    "    Example: cache_len=3, seq_len_q=2, seq_len_k=5\n",
    "        Keys:    [cached0, cached1, cached2, new0, new1]\n",
    "\n",
    "        Q\\\\K      c0     c1     c2    new0   new1\n",
    "        new0  [False, False, False, False, True ]  # new0 attends to cache + itself\n",
    "        new1  [False, False, False, False, False]  # new1 attends to cache + new0 + itself\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test KV cache mask: single token generation (most common case)\n",
    "mask_single = create_causal_mask_with_cache(seq_len_q=1, seq_len_k=5, cache_len=4)\n",
    "print(\"KV cache mask (1 new token, 4 cached):\")\n",
    "print(f\"Shape: {mask_single.shape}\")\n",
    "print(mask_single)\n",
    "assert mask_single[0].tolist() == [False, False, False, False, False], (\n",
    "    \"Single new token can attend to all\"\n",
    ")\n",
    "\n",
    "# Test KV cache mask: multi-token (prefill or speculative decoding)\n",
    "mask_multi = create_causal_mask_with_cache(seq_len_q=3, seq_len_k=6, cache_len=3)\n",
    "print(\"\\nKV cache mask (3 new tokens, 3 cached):\")\n",
    "print(f\"Keys: [cached0, cached1, cached2, new0, new1, new2]\")\n",
    "print(mask_multi)\n",
    "# new0 can attend to cache + itself\n",
    "assert mask_multi[0].tolist() == [False, False, False, False, True, True]\n",
    "# new2 can attend to everything\n",
    "assert mask_multi[2].tolist() == [False, False, False, False, False, False]\n",
    "\n",
    "# Test prefill (no cache)\n",
    "mask_prefill = create_causal_mask_with_cache(seq_len_q=3, seq_len_k=3, cache_len=0)\n",
    "print(\"\\nPrefill mask (3 tokens, no cache):\")\n",
    "print(mask_prefill)\n",
    "assert mask_prefill[0, 1] == True, \"First query can't see second\"\n",
    "assert mask_prefill[2, 0] == False, \"Third query can see first\"\n",
    "\n",
    "print(\"\\n\\u2713 KV cache mask test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Masks with Attention\n",
    "\n",
    "Now let's verify our masks work correctly with the attention function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test causal mask with attention\n",
    "torch.manual_seed(42)\n",
    "q_test = torch.randn(batch_size, seq_len, d_k)\n",
    "k_test = torch.randn(batch_size, seq_len, d_k)\n",
    "v_test = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_masked, attn_weights_masked = scaled_dot_product_attention(\n",
    "    q_test, k_test, v_test, mask=causal_mask\n",
    ")\n",
    "\n",
    "# Verify upper triangle is zero (no attention to future tokens)\n",
    "upper_triangle = attn_weights_masked[0].triu(diagonal=1)\n",
    "assert torch.allclose(upper_triangle, torch.zeros_like(upper_triangle), atol=1e-6), (\n",
    "    \"Causal mask failed! Positions are attending to future tokens.\"\n",
    ")\n",
    "\n",
    "# Verify each row sums to 1\n",
    "row_sums = attn_weights_masked.sum(dim=-1)\n",
    "assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-6), (\n",
    "    \"Attention weights don't sum to 1!\"\n",
    ")\n",
    "\n",
    "# Compare against PyTorch's is_causal\n",
    "output_ref = F.scaled_dot_product_attention(q_test, k_test, v_test, is_causal=True)\n",
    "assert torch.allclose(output_masked, output_ref, atol=1e-6), (\n",
    "    \"Doesn't match PyTorch is_causal!\"\n",
    ")\n",
    "\n",
    "print(\"\\u2713 Causal attention test passed!\")\n",
    "print(\"\\nAttention weights (notice upper triangle is 0):\")\n",
    "print(attn_weights_masked[0].round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Self-Attention with Projections\n",
    "\n",
    "Now implement **actual self-attention** where Q, K, V come from learned projections of a single input x.\n",
    "\n",
    "This is how attention works in real Transformers:\n",
    "- Input: `x` (batch, seq_len, d_model) - the residual stream\n",
    "- Q, K, V are computed as `x @ W_q`, `x @ W_k`, `x @ W_v`\n",
    "- Output projection: `attention_output @ W_o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention layer where Q, K, V come from projections of the same input.\n",
    "\n",
    "    This is how attention actually works in Transformers - the input x\n",
    "    (the residual stream) is projected to create Q, K, and V.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        is_causal: bool = False,\n",
    "        key_padding_lengths: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            is_causal: Whether to apply causal masking\n",
    "            key_padding_lengths: If provided, actual lengths for padding mask\n",
    "\n",
    "        Returns:\n",
    "            output: Self-attention output (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Self-Attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "\n",
    "# Single input - the residual stream\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(\"This single input x will be projected to create Q, K, and V\")\n",
    "\n",
    "# Create self-attention layer\n",
    "self_attn = SelfAttention(d_model)\n",
    "\n",
    "# Forward pass (bidirectional)\n",
    "output = self_attn(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "assert output.shape == x.shape, \"Output shape should match input shape!\"\n",
    "\n",
    "# Forward pass (causal)\n",
    "output_causal = self_attn(x, is_causal=True)\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "\n",
    "# Forward pass (with padding)\n",
    "lengths = torch.tensor([8, 5])  # second sequence is shorter\n",
    "output_padded = self_attn(x, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"Padded output shape: {output_padded.shape}\")\n",
    "\n",
    "print(\"\\n\\u2713 Self-Attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get attention weights for both bidirectional and causal attention\n",
    "_, attn_bidirectional = scaled_dot_product_attention(q_test, k_test, v_test, mask=None)\n",
    "_, attn_causal = scaled_dot_product_attention(\n",
    "    q_test, k_test, v_test, mask=create_causal_mask(seq_len)\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "im1 = axes[0].imshow(\n",
    "    attn_bidirectional[0].detach().numpy(), cmap=\"Blues\", vmin=0, vmax=0.5\n",
    ")\n",
    "axes[0].set_title(\"Bidirectional Attention\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Key Position\")\n",
    "axes[0].set_ylabel(\"Query Position\")\n",
    "plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "im2 = axes[1].imshow(attn_causal[0].detach().numpy(), cmap=\"Blues\", vmin=0, vmax=0.5)\n",
    "axes[1].set_title(\"Causal Attention (Autoregressive)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Key Position\")\n",
    "axes[1].set_ylabel(\"Query Position\")\n",
    "plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"attention_patterns.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how causal attention has zeros in the upper triangle -\")\n",
    "print(\"each position can only attend to itself and earlier positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Self-Attention with KV Cache\n",
    "\n",
    "### Why KV Cache?\n",
    "\n",
    "During autoregressive generation:\n",
    "- Without cache: Recompute K, V for ALL tokens at every step → O(n²)\n",
    "- With cache: Only compute K, V for NEW token, concatenate with cache → O(n)\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "We cache the **projected** K and V values (after `W_k(x)` and `W_v(x)`).\n",
    "For each new token, we:\n",
    "1. Compute Q, K, V from the new token's x\n",
    "2. Concatenate new K, V with cached K, V\n",
    "3. Compute attention using full K, V but only new Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention with KV Cache for efficient autoregressive generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cache_k: torch.Tensor = None,\n",
    "        cache_v: torch.Tensor = None,\n",
    "        is_causal: bool = True,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            cache_k: Cached keys (batch, cached_len, d_model) or None\n",
    "            cache_v: Cached values (batch, cached_len, d_model) or None\n",
    "            is_causal: Whether to apply causal masking (default True for autoregressive)\n",
    "\n",
    "        Returns:\n",
    "            output: Attention output (batch, seq_len, d_model)\n",
    "            new_cache_k: Updated key cache\n",
    "            new_cache_v: Updated value cache\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Self-Attention with KV Cache\n",
    "print(\"=== Testing Self-Attention with KV Cache ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "\n",
    "self_attn_cached = SelfAttentionWithCache(d_model)\n",
    "\n",
    "# Step 1: Process prompt (3 tokens) - prefill\n",
    "prompt = torch.randn(batch_size, 3, d_model)\n",
    "print(f\"\\nInput prompt shape: {prompt.shape}\")\n",
    "print(\"Q, K, V will all be computed from this single input via learned projections\")\n",
    "\n",
    "out1, cache_k, cache_v = self_attn_cached(prompt, None, None)\n",
    "print(f\"\\nAfter prompt: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 2: Generate token 4 (single new token)\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out2, cache_k, cache_v = self_attn_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 4: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out3, cache_k, cache_v = self_attn_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 5: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Verify\n",
    "assert cache_k.shape == (batch_size, 5, d_model)\n",
    "assert out3.shape == (batch_size, 1, d_model)\n",
    "\n",
    "print(\"\\n\\u2713 Self-Attention with KV Cache test passed!\")\n",
    "print(\"\\nKey insight: Each generation step only computes K, V for 1 new token,\")\n",
    "print(\"but attends over ALL previous tokens via the cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "**Q: Where do Q, K, V come from in self-attention?**\n",
    "A: All three come from learned linear projections of the same input x (the residual stream): Q = W_q(x), K = W_k(x), V = W_v(x).\n",
    "\n",
    "**Q: What's the difference between self-attention and cross-attention?**\n",
    "A: In self-attention, Q, K, V come from the same input. In cross-attention (like encoder-decoder), Q comes from one source (decoder) while K, V come from another (encoder output).\n",
    "\n",
    "**Q: Why scale by sqrt(d_k)?**\n",
    "A: To prevent dot products from growing too large with high dimensions, which would push softmax into regions with tiny gradients.\n",
    "\n",
    "**Q: How do you create a causal mask?**\n",
    "A: Use `torch.triu(..., diagonal=1)` to create a boolean upper triangular matrix where True = masked. The upper triangle blocks attention to future positions.\n",
    "\n",
    "**Q: How do you handle padding in attention?**\n",
    "A: Create a padding mask where positions >= sequence length are True (masked). Combine with causal mask using OR operation.\n",
    "\n",
    "**Q: What is KV cache and why is it used?**\n",
    "A: KV cache stores computed K, V from previous tokens during autoregressive generation. Without it, generating n tokens requires O(n²) computations; with it, only O(n).\n",
    "\n",
    "**Q: How does masking work with KV cache?**\n",
    "A: New queries can attend to all cached positions (they're in the past). Causal masking only applies within the new tokens. Use `create_causal_mask_with_cache()` with the cache length offset.\n",
    "\n",
    "**Q: What happens during prefill vs decode phases?**\n",
    "A: \n",
    "- Prefill: Process entire prompt at once, populate cache (compute-bound)\n",
    "- Decode: Generate one token at a time using cache (memory-bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
