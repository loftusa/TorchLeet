{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-K and Top-P (Nucleus) Sampling\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Temperature scaling controls the sharpness of probability distributions, but it doesn't prevent sampling from low-probability tokens that could derail generation. **Top-K** and **Top-P (Nucleus) Sampling** are filtering techniques that truncate the probability distribution to improve generation quality.\n",
    "\n",
    "Your task is to implement both Top-K and Top-P sampling from scratch and understand when to use each.\n",
    "\n",
    "---\n",
    "\n",
    "### Background\n",
    "\n",
    "#### The Problem with Pure Temperature Sampling\n",
    "\n",
    "Even with temperature control, sampling from the full vocabulary can produce problematic outputs:\n",
    "- **Low-probability tokens**: With 50,000+ tokens, even 0.01% probability tokens get sampled occasionally\n",
    "- **Inconsistent quality**: Rare tokens can derail coherent generation\n",
    "- **The long tail problem**: Most of the probability mass is in a small number of tokens\n",
    "\n",
    "#### Top-K Sampling (Fan et al., 2018)\n",
    "\n",
    "**Idea**: Only consider the K most likely tokens, redistribute probability among them.\n",
    "\n",
    "**Algorithm**:\n",
    "1. Sort tokens by probability (descending)\n",
    "2. Keep only the top K tokens\n",
    "3. Set all other probabilities to 0\n",
    "4. Renormalize to sum to 1\n",
    "5. Sample from truncated distribution\n",
    "\n",
    "**Limitation**: Fixed K doesn't adapt to confidence. When the model is very confident (one token has 95% probability), K=50 still considers 49 unlikely tokens. When uncertain (flat distribution), K=50 might exclude reasonable options.\n",
    "\n",
    "#### Top-P (Nucleus) Sampling (Holtzman et al., 2020)\n",
    "\n",
    "**Idea**: Dynamically select the smallest set of tokens whose cumulative probability exceeds threshold P.\n",
    "\n",
    "**Algorithm**:\n",
    "1. Sort tokens by probability (descending)\n",
    "2. Compute cumulative probabilities\n",
    "3. Find smallest set where cumsum >= P\n",
    "4. Keep only tokens in this \"nucleus\"\n",
    "5. Renormalize and sample\n",
    "\n",
    "**Advantage**: Adapts to model confidence. Confident predictions use few tokens; uncertain predictions use more.\n",
    "\n",
    "#### Combining Techniques\n",
    "\n",
    "In practice, these techniques are often combined:\n",
    "1. Apply temperature scaling first\n",
    "2. Apply Top-K filtering\n",
    "3. Apply Top-P filtering\n",
    "4. Sample from result\n",
    "\n",
    "**Common defaults** (OpenAI, Anthropic, etc.):\n",
    "- Temperature: 0.7-1.0\n",
    "- Top-P: 0.9-0.95\n",
    "- Top-K: 40-100 (or disabled)\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Top-K\n",
    "\n",
    "Given probabilities $p_1, p_2, ..., p_V$ sorted in descending order:\n",
    "\n",
    "$$p_i^{\\text{top-k}} = \\begin{cases} \\frac{p_i}{\\sum_{j=1}^{K} p_j} & \\text{if } i \\leq K \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "#### Top-P (Nucleus)\n",
    "\n",
    "Find the smallest $k$ such that:\n",
    "\n",
    "$$\\sum_{i=1}^{k} p_i \\geq P$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$p_i^{\\text{top-p}} = \\begin{cases} \\frac{p_i}{\\sum_{j=1}^{k} p_j} & \\text{if } i \\leq k \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand why filtering is necessary beyond temperature scaling\n",
    "2. Implement Top-K sampling with proper renormalization\n",
    "3. Implement Top-P (nucleus) sampling with dynamic cutoff\n",
    "4. Know when to use each technique and common parameter values\n",
    "5. Combine multiple sampling techniques effectively\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. **Top-K Filtering**: Implement `top_k_filtering(logits, k)` that zeros out all but top-k logits\n",
    "2. **Top-P Filtering**: Implement `top_p_filtering(logits, p)` that zeros out tokens outside the nucleus\n",
    "3. **Combined Sampling**: Implement `sample(logits, temperature, top_k, top_p)` combining all techniques\n",
    "4. **Validation**: Test that filtering produces expected behavior\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Top-K Implementation</summary>\n",
    "\n",
    "Use `torch.topk()` to get indices of top-k values. Create a mask or use scatter to zero out non-top-k positions.\n",
    "\n",
    "```python\n",
    "values, indices = torch.topk(logits, k)\n",
    "# Create output with -inf for filtered positions\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Top-P Implementation</summary>\n",
    "\n",
    "1. Convert logits to probabilities with softmax\n",
    "2. Sort probabilities and compute cumsum\n",
    "3. Find where cumsum exceeds p\n",
    "4. Create mask and apply to original logits\n",
    "\n",
    "```python\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3: Handling Edge Cases</summary>\n",
    "\n",
    "- If k >= vocab_size, Top-K should return original logits\n",
    "- If p >= 1.0, Top-P should return original logits  \n",
    "- Always keep at least one token (the most probable)\n",
    "- Use -inf (not 0) to zero out logits before softmax\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 4: Combining Techniques</summary>\n",
    "\n",
    "Order matters! Standard order:\n",
    "1. Temperature scaling (divide logits by T)\n",
    "2. Top-K filtering\n",
    "3. Top-P filtering\n",
    "4. Softmax and sample\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Top-K Filtering\n",
    "\n",
    "Implement a function that keeps only the top-k logits and sets all others to negative infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filtering(logits: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Filter logits to keep only top-k values, setting others to -inf.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape (batch_size, vocab_size) or (vocab_size,)\n",
    "        k: Number of top tokens to keep\n",
    "        \n",
    "    Returns:\n",
    "        Filtered logits with same shape, non-top-k positions set to -inf\n",
    "    \"\"\"\n",
    "    # TODO: Implement top-k filtering\n",
    "    # 1. Handle edge cases (k <= 0, k >= vocab_size)\n",
    "    # 2. Find the k-th largest value as threshold\n",
    "    # 3. Create mask for values below threshold\n",
    "    # 4. Set filtered positions to -inf\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Top-K filtering\n",
    "print(\"=== Testing Top-K Filtering ===\")\n",
    "\n",
    "# Simple test case\n",
    "logits = torch.tensor([1.0, 4.0, 2.0, 5.0, 3.0])\n",
    "print(f\"Original logits: {logits}\")\n",
    "\n",
    "filtered_k2 = top_k_filtering(logits, k=2)\n",
    "print(f\"Top-2 filtered:  {filtered_k2}\")\n",
    "\n",
    "filtered_k3 = top_k_filtering(logits, k=3)\n",
    "print(f\"Top-3 filtered:  {filtered_k3}\")\n",
    "\n",
    "# Verify only k values remain finite\n",
    "assert (filtered_k2 > float('-inf')).sum() == 2, \"Should have exactly 2 finite values\"\n",
    "assert (filtered_k3 > float('-inf')).sum() == 3, \"Should have exactly 3 finite values\"\n",
    "\n",
    "# Verify the top-k values are preserved\n",
    "probs_k2 = F.softmax(filtered_k2, dim=-1)\n",
    "print(f\"Probabilities after Top-2: {probs_k2}\")\n",
    "assert probs_k2[1] > 0 and probs_k2[3] > 0, \"Tokens 1 and 3 should have probability\"\n",
    "assert probs_k2[0] == 0 and probs_k2[2] == 0 and probs_k2[4] == 0, \"Other tokens should have 0 probability\"\n",
    "\n",
    "print(\"\\n Top-K filtering tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Top-P (Nucleus) Filtering\n",
    "\n",
    "Implement a function that keeps the smallest set of tokens whose cumulative probability exceeds p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_filtering(logits: torch.Tensor, p: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Filter logits using nucleus (top-p) sampling.\n",
    "    Keep smallest set of tokens with cumulative probability >= p.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape (batch_size, vocab_size) or (vocab_size,)\n",
    "        p: Cumulative probability threshold (0 < p <= 1)\n",
    "        \n",
    "    Returns:\n",
    "        Filtered logits with same shape, tokens outside nucleus set to -inf\n",
    "    \"\"\"\n",
    "    # TODO: Implement top-p (nucleus) filtering\n",
    "    # 1. Convert logits to probabilities\n",
    "    # 2. Sort probabilities descending\n",
    "    # 3. Compute cumulative sum\n",
    "    # 4. Find tokens to remove (cumsum > p)\n",
    "    # 5. Create mask and apply to original logits\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Top-P filtering\n",
    "print(\"=== Testing Top-P (Nucleus) Filtering ===\")\n",
    "\n",
    "# Create logits where we know the probabilities\n",
    "# logits = [0, 1, 2, 3] -> softmax ~ [0.032, 0.087, 0.236, 0.643]\n",
    "logits = torch.tensor([0.0, 1.0, 2.0, 3.0])\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"Original logits: {logits}\")\n",
    "print(f\"Original probs:  {probs}\")\n",
    "print(f\"Cumulative:      {torch.cumsum(probs.sort(descending=True)[0], dim=-1)}\")\n",
    "\n",
    "# Top-p with p=0.9 should keep tokens with cumsum <= 0.9\n",
    "# Sorted: 0.643 (cumsum=0.643), 0.236 (cumsum=0.879), 0.087 (cumsum=0.966)\n",
    "# With p=0.9, we keep until cumsum >= 0.9, so we keep top 3\n",
    "filtered_p09 = top_p_filtering(logits, p=0.9)\n",
    "probs_p09 = F.softmax(filtered_p09, dim=-1)\n",
    "print(f\"\\nTop-p=0.9 filtered: {filtered_p09}\")\n",
    "print(f\"Probs after p=0.9:  {probs_p09}\")\n",
    "\n",
    "# Top-p with p=0.7 should keep fewer tokens\n",
    "# We need cumsum >= 0.7, so just top 2 (0.643 + 0.236 = 0.879 >= 0.7)\n",
    "filtered_p07 = top_p_filtering(logits, p=0.7)\n",
    "probs_p07 = F.softmax(filtered_p07, dim=-1)\n",
    "print(f\"\\nTop-p=0.7 filtered: {filtered_p07}\")\n",
    "print(f\"Probs after p=0.7:  {probs_p07}\")\n",
    "\n",
    "# Verify the nucleus property\n",
    "print(f\"\\n--- Verification ---\")\n",
    "kept_p09 = (probs_p09 > 0).sum().item()\n",
    "kept_p07 = (probs_p07 > 0).sum().item()\n",
    "print(f\"Tokens kept with p=0.9: {kept_p09}\")\n",
    "print(f\"Tokens kept with p=0.7: {kept_p07}\")\n",
    "assert kept_p07 <= kept_p09, \"Lower p should keep fewer or equal tokens\"\n",
    "\n",
    "# Test that probabilities still sum to 1\n",
    "assert torch.allclose(probs_p09.sum(), torch.tensor(1.0)), \"Probs should sum to 1\"\n",
    "assert torch.allclose(probs_p07.sum(), torch.tensor(1.0)), \"Probs should sum to 1\"\n",
    "\n",
    "print(\"\\n Top-P filtering tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Combined Sampling Function\n",
    "\n",
    "Implement a complete sampling function that combines temperature, top-k, and top-p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample from logits with temperature scaling and optional top-k/top-p filtering.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape (batch_size, vocab_size) or (vocab_size,)\n",
    "        temperature: Temperature for scaling (default 1.0)\n",
    "        top_k: If > 0, only sample from top-k tokens (default 0 = disabled)\n",
    "        top_p: If < 1.0, use nucleus sampling (default 1.0 = disabled)\n",
    "        \n",
    "    Returns:\n",
    "        Sampled token indices\n",
    "    \"\"\"\n",
    "    # TODO: Implement combined sampling\n",
    "    # 1. Apply temperature scaling\n",
    "    # 2. Apply top-k filtering (if top_k > 0)\n",
    "    # 3. Apply top-p filtering (if top_p < 1.0)\n",
    "    # 4. Convert to probabilities and sample\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test combined sampling\n",
    "print(\"=== Testing Combined Sampling ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create test logits\n",
    "vocab_size = 100\n",
    "logits = torch.randn(vocab_size)\n",
    "\n",
    "# Test 1: Pure sampling (no filtering)\n",
    "samples_pure = [sample(logits, temperature=1.0).item() for _ in range(100)]\n",
    "unique_pure = len(set(samples_pure))\n",
    "print(f\"Pure sampling (T=1.0): {unique_pure} unique tokens from 100 samples\")\n",
    "\n",
    "# Test 2: Low temperature (more deterministic)\n",
    "samples_low_t = [sample(logits, temperature=0.1).item() for _ in range(100)]\n",
    "unique_low_t = len(set(samples_low_t))\n",
    "print(f\"Low temp (T=0.1): {unique_low_t} unique tokens from 100 samples\")\n",
    "\n",
    "# Test 3: Top-K sampling\n",
    "samples_topk = [sample(logits, top_k=5).item() for _ in range(100)]\n",
    "unique_topk = len(set(samples_topk))\n",
    "print(f\"Top-K (k=5): {unique_topk} unique tokens from 100 samples\")\n",
    "assert unique_topk <= 5, \"Top-K should limit to at most K unique tokens\"\n",
    "\n",
    "# Test 4: Top-P sampling\n",
    "samples_topp = [sample(logits, top_p=0.5).item() for _ in range(100)]\n",
    "unique_topp = len(set(samples_topp))\n",
    "print(f\"Top-P (p=0.5): {unique_topp} unique tokens from 100 samples\")\n",
    "\n",
    "# Test 5: Combined (typical production settings)\n",
    "samples_combined = [sample(logits, temperature=0.7, top_k=40, top_p=0.9).item() for _ in range(100)]\n",
    "unique_combined = len(set(samples_combined))\n",
    "print(f\"Combined (T=0.7, k=40, p=0.9): {unique_combined} unique tokens from 100 samples\")\n",
    "\n",
    "# Test 6: Batch processing\n",
    "batch_logits = torch.randn(4, vocab_size)\n",
    "batch_samples = sample(batch_logits, temperature=0.8, top_k=10)\n",
    "assert batch_samples.shape == (4, 1), f\"Expected shape (4, 1), got {batch_samples.shape}\"\n",
    "print(f\"Batch sampling works: shape {batch_samples.shape}\")\n",
    "\n",
    "print(\"\\n Combined sampling tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization\n",
    "\n",
    "Let's visualize how Top-K and Top-P affect the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filtering():\n",
    "    \"\"\"Visualize the effects of Top-K and Top-P filtering.\"\"\"\n",
    "    # Create a realistic distribution (Zipf-like, common in language models)\n",
    "    vocab_size = 50\n",
    "    # Higher logits for fewer tokens (realistic LM distribution)\n",
    "    logits = torch.randn(vocab_size) * 2\n",
    "    logits[0] = 5.0   # One very likely token\n",
    "    logits[1] = 3.5\n",
    "    logits[2] = 2.5\n",
    "    logits[3] = 2.0\n",
    "    logits[4] = 1.5\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Original distribution\n",
    "    original_probs = F.softmax(logits, dim=-1).numpy()\n",
    "    sorted_probs = np.sort(original_probs)[::-1]\n",
    "    \n",
    "    # Plot 1: Original distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.bar(range(vocab_size), sorted_probs, color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Token Rank')\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.set_title('Original Distribution')\n",
    "    ax1.axhline(y=0.01, color='red', linestyle='--', label='1% threshold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Top-K filtered (k=10)\n",
    "    filtered_k10 = top_k_filtering(logits, k=10)\n",
    "    probs_k10 = F.softmax(filtered_k10, dim=-1).numpy()\n",
    "    sorted_probs_k10 = np.sort(probs_k10)[::-1]\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    colors = ['coral' if p > 0 else 'lightgray' for p in sorted_probs_k10]\n",
    "    ax2.bar(range(vocab_size), sorted_probs_k10, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Token Rank')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.set_title('Top-K Filtered (k=10)')\n",
    "    ax2.annotate(f'Kept: 10 tokens', xy=(0.7, 0.9), xycoords='axes fraction')\n",
    "    \n",
    "    # Plot 3: Top-P filtered (p=0.9)\n",
    "    filtered_p09 = top_p_filtering(logits, p=0.9)\n",
    "    probs_p09 = F.softmax(filtered_p09, dim=-1).numpy()\n",
    "    sorted_probs_p09 = np.sort(probs_p09)[::-1]\n",
    "    n_kept_p09 = (probs_p09 > 0).sum()\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    colors = ['seagreen' if p > 0 else 'lightgray' for p in sorted_probs_p09]\n",
    "    ax3.bar(range(vocab_size), sorted_probs_p09, color=colors, alpha=0.7)\n",
    "    ax3.set_xlabel('Token Rank')\n",
    "    ax3.set_ylabel('Probability')\n",
    "    ax3.set_title('Top-P Filtered (p=0.9)')\n",
    "    ax3.annotate(f'Kept: {n_kept_p09} tokens', xy=(0.7, 0.9), xycoords='axes fraction')\n",
    "    \n",
    "    # Plot 4: Combined (k=20, p=0.9)\n",
    "    filtered_combined = top_p_filtering(top_k_filtering(logits, k=20), p=0.9)\n",
    "    probs_combined = F.softmax(filtered_combined, dim=-1).numpy()\n",
    "    sorted_probs_combined = np.sort(probs_combined)[::-1]\n",
    "    n_kept_combined = (probs_combined > 0).sum()\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    colors = ['purple' if p > 0 else 'lightgray' for p in sorted_probs_combined]\n",
    "    ax4.bar(range(vocab_size), sorted_probs_combined, color=colors, alpha=0.7)\n",
    "    ax4.set_xlabel('Token Rank')\n",
    "    ax4.set_ylabel('Probability')\n",
    "    ax4.set_title('Combined: Top-K (k=20) + Top-P (p=0.9)')\n",
    "    ax4.annotate(f'Kept: {n_kept_combined} tokens', xy=(0.7, 0.9), xycoords='axes fraction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('filtering_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved visualization to filtering_visualization.png\")\n",
    "\n",
    "# Uncomment to run after implementing the functions:\n",
    "# visualize_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Adaptive Behavior Demonstration\n",
    "\n",
    "Show how Top-P adapts to model confidence while Top-K does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_adaptive_behavior():\n",
    "    \"\"\"Show how Top-P adapts to confidence while Top-K doesn't.\"\"\"\n",
    "    vocab_size = 100\n",
    "    \n",
    "    print(\"=== Adaptive Behavior: Top-K vs Top-P ===\")\n",
    "    print()\n",
    "    \n",
    "    # Scenario 1: High confidence (one dominant token)\n",
    "    print(\"Scenario 1: HIGH CONFIDENCE (one token dominates)\")\n",
    "    logits_confident = torch.zeros(vocab_size)\n",
    "    logits_confident[0] = 10.0  # One very confident prediction\n",
    "    probs_confident = F.softmax(logits_confident, dim=-1)\n",
    "    print(f\"  Top token probability: {probs_confident[0]:.4f}\")\n",
    "    print(f\"  Top 5 tokens sum: {probs_confident[:5].sum():.4f}\")\n",
    "    \n",
    "    # Top-K still keeps K tokens even though only 1 matters\n",
    "    filtered_k10 = top_k_filtering(logits_confident, k=10)\n",
    "    n_kept_k = (F.softmax(filtered_k10, dim=-1) > 1e-6).sum().item()\n",
    "    print(f\"  Top-K (k=10) keeps: {n_kept_k} tokens (wasteful!)\")\n",
    "    \n",
    "    # Top-P adapts and keeps fewer\n",
    "    filtered_p09 = top_p_filtering(logits_confident, p=0.9)\n",
    "    n_kept_p = (F.softmax(filtered_p09, dim=-1) > 1e-6).sum().item()\n",
    "    print(f\"  Top-P (p=0.9) keeps: {n_kept_p} tokens (adaptive!)\")\n",
    "    print()\n",
    "    \n",
    "    # Scenario 2: Low confidence (flat distribution)\n",
    "    print(\"Scenario 2: LOW CONFIDENCE (flat distribution)\")\n",
    "    logits_uncertain = torch.randn(vocab_size) * 0.1  # Very flat\n",
    "    probs_uncertain = F.softmax(logits_uncertain, dim=-1)\n",
    "    print(f\"  Max token probability: {probs_uncertain.max():.4f}\")\n",
    "    print(f\"  Top 5 tokens sum: {probs_uncertain.topk(5)[0].sum():.4f}\")\n",
    "    \n",
    "    # Top-K still only keeps K tokens\n",
    "    filtered_k10_unc = top_k_filtering(logits_uncertain, k=10)\n",
    "    n_kept_k_unc = (F.softmax(filtered_k10_unc, dim=-1) > 1e-6).sum().item()\n",
    "    print(f\"  Top-K (k=10) keeps: {n_kept_k_unc} tokens (might miss good options!)\")\n",
    "    \n",
    "    # Top-P adapts and keeps more\n",
    "    filtered_p09_unc = top_p_filtering(logits_uncertain, p=0.9)\n",
    "    n_kept_p_unc = (F.softmax(filtered_p09_unc, dim=-1) > 1e-6).sum().item()\n",
    "    print(f\"  Top-P (p=0.9) keeps: {n_kept_p_unc} tokens (explores more options!)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Key Insight:\")\n",
    "    print(\"  - Top-K: Fixed number of tokens regardless of confidence\")\n",
    "    print(\"  - Top-P: Adapts to model confidence (fewer when confident, more when uncertain)\")\n",
    "\n",
    "# Uncomment to run after implementing the functions:\n",
    "# demonstrate_adaptive_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Top-K Sampling**: Keep only K most likely tokens\n",
    "   - Simple and predictable\n",
    "   - Doesn't adapt to model confidence\n",
    "   - Good for limiting search space\n",
    "\n",
    "2. **Top-P (Nucleus) Sampling**: Keep smallest set with cumulative prob >= P\n",
    "   - Adapts to model confidence\n",
    "   - More flexible than Top-K\n",
    "   - Industry standard (p=0.9 common default)\n",
    "\n",
    "3. **Combining Techniques**: Temperature + Top-K + Top-P\n",
    "   - Order: Temperature -> Top-K -> Top-P -> Sample\n",
    "   - Production defaults: T=0.7, top_p=0.9, top_k=40 (or disabled)\n",
    "\n",
    "4. **Implementation Details**:\n",
    "   - Use -inf (not 0) to filter logits before softmax\n",
    "   - Always keep at least one token\n",
    "   - Handle batch dimension properly\n",
    "\n",
    "---\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "**Q: What's the difference between Top-K and Top-P sampling?**\n",
    "\n",
    "A: Top-K keeps exactly K tokens regardless of their probabilities. Top-P (nucleus) dynamically selects tokens until cumulative probability reaches threshold P. Top-P adapts to model confidence - when confident, it uses fewer tokens; when uncertain, it uses more.\n",
    "\n",
    "**Q: Why not just use temperature alone?**\n",
    "\n",
    "A: Temperature affects distribution sharpness but still allows sampling from the entire vocabulary. With 50K+ tokens, even 0.001% probability tokens occasionally get sampled, potentially derailing generation. Filtering removes low-probability tokens entirely.\n",
    "\n",
    "**Q: What are typical production values for these parameters?**\n",
    "\n",
    "A: Common defaults: Temperature 0.7-1.0, Top-P 0.9-0.95. Top-K is often disabled or set to 40-100. ChatGPT, Claude, and other systems use similar ranges.\n",
    "\n",
    "**Q: In what order should you apply these techniques?**\n",
    "\n",
    "A: Temperature first (scales logits), then Top-K (fixed filtering), then Top-P (adaptive filtering), then softmax and sample. Temperature must come before filtering because it changes relative probabilities.\n",
    "\n",
    "**Q: When would you use Top-K over Top-P?**\n",
    "\n",
    "A: Top-K when you want predictable compute/memory (always exactly K candidates) or when the model's confidence estimates are unreliable. Top-P is generally preferred for text generation due to its adaptive behavior.\n",
    "\n",
    "**Q: How do you handle the edge case where filtering removes all tokens?**\n",
    "\n",
    "A: Always keep at least the most probable token. In Top-P, even with p=0.01, the top token is always included. This prevents sampling from an empty distribution.\n",
    "\n",
    "**Q: What's the computational complexity of Top-P?**\n",
    "\n",
    "A: O(V log V) for sorting the vocabulary, then O(V) for cumsum and masking. In practice, can be optimized with partial sorting since we often only need a small nucleus.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Fan, A., Lewis, M., & Dauphin, Y. (2018). \"Hierarchical Neural Story Generation.\" *ACL 2018*. (Introduced Top-K sampling)\n",
    "\n",
    "2. Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2020). \"The Curious Case of Neural Text Degeneration.\" *ICLR 2020*. (Introduced nucleus/Top-P sampling)\n",
    "\n",
    "3. [HuggingFace Transformers - Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "\n",
    "4. [OpenAI API Documentation - Temperature and Top-P](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
