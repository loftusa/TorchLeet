{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings out of an LLM\n",
    "\n",
    "### Problem Statement\n",
    "Your mission, should you choose to accept it, is to extract **meaningful sentence-level embeddings** using a pre-trained **causal language model (SmolLM2-135M)** on Amazon Reviews.\n",
    "\n",
    "You're working with a **generative language model**, but you‚Äôre not here to generate Shakespeare. Instead, you‚Äôll tap into its **hidden states** to get semantic embeddings that capture the essence of a review ‚Äî the good, the bad, and the brutally honest.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. **Load and Tokenize Text**\n",
    "   - Use the `McAuley-Lab/Amazon-Reviews-2023` dataset (subset: `raw_review_All_Beauty`).\n",
    "   - Load ~10 sample reviews for testing.\n",
    "   - Tokenize them using `\"HuggingFaceTB/SmolLM2-135M\"` tokenizer.\n",
    "\n",
    "2. **Extract Embeddings**\n",
    "   - Run the tokenized batch through the model with `output_hidden_states=True`.\n",
    "   - Access the **last hidden layer** from `outputs.hidden_states[-1]`.\n",
    "\n",
    "3. **Compute Sentence Embeddings**\n",
    "   - Options:\n",
    "     - If the model uses a classification token (e.g., `[CLS]`), extract its embedding.\n",
    "     - For causal models (which typically don‚Äôt), **average the token embeddings** from the final layer, **excluding padding tokens**.\n",
    "\n",
    "4. **Find the cosine similarity for a given keyword** \n",
    "   - Compute the cosine similarity between the average embeddings of the reviews and a keyword.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- ‚ùå Do **not** use sentence-transformers or pre-built embedding tools like `bert-as-service`.\n",
    "- ‚ùå Do **not** generate text (no `.generate()`).\n",
    "- ‚úÖ Use only Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer`.\n",
    "- ‚úÖ Exclude padding tokens when computing average embeddings.\n",
    "- ‚úÖ Ensure everything runs on `cuda` if available.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint</summary>\n",
    "\n",
    "```python\n",
    "# Run model with hidden states\n",
    "outputs = model(**tokenized_inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "# Get the last hidden layer (batch_size, seq_len, hidden_dim)\n",
    "last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "# Use the attention mask to avoid averaging over padding\n",
    "attention_mask = tokenized_inputs['attention_mask']  # (batch_size, seq_len)\n",
    "\n",
    "# Compute masked average: zero out padding tokens\n",
    "masked_embeddings = last_hidden * attention_mask.unsqueeze(-1)  # broadcast mask\n",
    "summed = masked_embeddings.sum(dim=1)  # sum across tokens\n",
    "count = attention_mask.sum(dim=1, keepdim=True)  # count of non-padding tokens\n",
    "\n",
    "# Final sentence-level embeddings\n",
    "sentence_embeddings = summed / count  # (batch_size, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample reviews for testing\n# Using synthetic reviews instead of loading from dataset due to compatibility issues\nreviews = [\n    \"This product has amazing quality and works perfectly!\",\n    \"Terrible quality, broke after one use. Very disappointed.\",\n    \"Great value for money. The quality exceeded my expectations.\",\n    \"The quality is okay but nothing special. Average product.\",\n    \"Absolutely love the quality! Best purchase I've made.\",\n    \"Poor quality materials. Would not recommend to anyone.\",\n    \"Decent quality for the price point. Does what it needs to do.\",\n    \"Outstanding quality and craftsmanship. Worth every penny!\",\n    \"The quality is questionable. Mine arrived damaged.\",\n    \"Exceptional quality! This product will last for years.\"\n]\n\nprint(f\"Loaded {len(reviews)} sample reviews\")\nprint(f\"\\nFirst review: {reviews[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample reviews loaded successfully\n# Ready to tokenize and extract embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load SmolLM2-135M model and tokenizer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n\n# Set padding token (required for batch processing)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(isinstance(model, torch.nn.Module))  # Should print: True\nprint(f\"Model loaded: {model.__class__.__name__}\")\nprint(f\"Padding token set to: {tokenizer.pad_token}\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\n\n# Tokenize the reviews with padding for batch processing\nencodings = tokenizer(reviews, return_tensors=\"pt\", padding=True, truncation=True)\ninput_ids = encodings['input_ids'].to(device)\nattention_mask = encodings['attention_mask'].to(device)\n\nprint(f\"Tokenized {len(reviews)} reviews\")\nprint(f\"Input shape: {input_ids.shape}\")\n\n# Forward pass with output_hidden_states=True to get all hidden states\nwith torch.no_grad():\n    outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n\n# Extract last hidden states (batch_size, seq_len, hidden_dim)\nlast_hidden_states = outputs.hidden_states[-1]\nprint(f\"Last hidden states shape: {last_hidden_states.shape}\")\n\n# Compute sentence embeddings by averaging token embeddings excluding padding tokens\n# attention_mask has 1 for real tokens, 0 for padding\nexpanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\nsum_embeddings = torch.sum(last_hidden_states * expanded_mask, dim=1)\nsum_mask = torch.clamp(expanded_mask.sum(dim=1), min=1e-9)  # avoid division by zero\nsentence_embeddings = sum_embeddings / sum_mask  # (batch_size, hidden_dim)\n\nprint(f\"Sentence embeddings shape: {sentence_embeddings.shape}\")\n\n# --- Cosine similarity for a given keyword ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"Computing cosine similarity with keyword: 'quality'\")\nprint(\"=\"*60)\n\nkeyword = \"quality\"\n\n# Tokenize and embed the keyword the same way\nkeyword_enc = tokenizer(keyword, return_tensors=\"pt\")\nkeyword_input_ids = keyword_enc['input_ids'].to(device)\nkeyword_attention_mask = keyword_enc['attention_mask'].to(device)\n\nwith torch.no_grad():\n    keyword_outputs = model(keyword_input_ids, attention_mask=keyword_attention_mask, output_hidden_states=True)\n\nkeyword_last_hidden = keyword_outputs.hidden_states[-1]\nkeyword_mask = keyword_attention_mask.unsqueeze(-1).expand(keyword_last_hidden.size()).float()\nkeyword_embedding = (keyword_last_hidden * keyword_mask).sum(dim=1) / torch.clamp(keyword_mask.sum(dim=1), min=1e-9)\n\n# Compute cosine similarity between keyword embedding and each review embedding\ncosine_similarities = F.cosine_similarity(sentence_embeddings, keyword_embedding)\n\nprint(f\"\\nCosine similarities (higher = more similar to '{keyword}'):\\n\")\nfor i, (review, sim) in enumerate(zip(reviews, cosine_similarities)):\n    print(f\"Review #{i+1} [similarity: {sim.item():.4f}]: {review[:60]}...\")\n\nprint(\"\\n‚úÖ Success! Extracted embeddings and computed similarities\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}