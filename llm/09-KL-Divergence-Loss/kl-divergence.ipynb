{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Implement KL Divergence Loss\n",
    "\n",
    "### Background\n",
    "\n",
    "**Kullback-Leibler (KL) Divergence** is a fundamental concept in information theory and machine learning. It measures how one probability distribution differs from another reference distribution.\n",
    "\n",
    "### Mathematical Definitions\n",
    "\n",
    "#### Discrete KL Divergence\n",
    "\n",
    "For discrete probability distributions $P$ and $Q$:\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Always non-negative: $D_{KL}(P || Q) \\geq 0$\n",
    "- Zero if and only if $P = Q$\n",
    "- Not symmetric: $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$\n",
    "\n",
    "#### Continuous KL Divergence (Gaussian)\n",
    "\n",
    "For two Gaussian distributions $\\mathcal{N}(\\mu_p, \\sigma_p^2)$ and $\\mathcal{N}(\\mu_q, \\sigma_q^2)$:\n",
    "\n",
    "$$\n",
    "D_{KL}(\\mathcal{N}_p || \\mathcal{N}_q) = \\log \\frac{\\sigma_q}{\\sigma_p} + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "### Knowledge Distillation\n",
    "\n",
    "**Knowledge distillation** uses KL divergence to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model.\n",
    "\n",
    "The distillation loss combines two terms:\n",
    "\n",
    "$$\n",
    "L_{distill} = \\alpha \\cdot L_{soft} + (1-\\alpha) \\cdot L_{hard}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L_{soft} = T^2 \\cdot D_{KL}(\\text{teacher}_T || \\text{student}_T)$ - Soft target loss using temperature $T$\n",
    "- $L_{hard} = CE(\\text{student}, \\text{labels})$ - Hard target loss (standard cross-entropy)\n",
    "- $\\alpha$ - Weighting between soft and hard targets (typically 0.5-0.9)\n",
    "- $T$ - Temperature for softening distributions (typically 1-5)\n",
    "\n",
    "**Temperature scaling**: Higher temperatures produce softer probability distributions, exposing the relative confidences of the teacher model.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this problem, you will:\n",
    "1. Implement discrete KL divergence for classification tasks\n",
    "2. Implement continuous KL divergence for Gaussian distributions\n",
    "3. Create a distillation loss function combining soft and hard targets\n",
    "4. Train a teacher model and compress it into a student model using distillation\n",
    "5. Compare distilled vs baseline student performance\n",
    "\n",
    "### References\n",
    "- [Hinton et al. - Distilling the Knowledge in a Neural Network (2015)](https://arxiv.org/abs/1503.02531)\n",
    "- [KL Divergence on Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic MNIST-like dataset\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 500 samples, 28x28 flattened = 784 features, 10 classes\n",
    "num_samples = 500\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Generate random features\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "\n",
    "# Generate labels with class-dependent means for better separability\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# Add class-specific signal to features\n",
    "for i in range(num_classes):\n",
    "    mask = y == i\n",
    "    class_signal = torch.randn(input_dim) * 0.5\n",
    "    X[mask] += class_signal\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Dataset: {num_samples} samples, {input_dim} features, {num_classes} classes\")\n",
    "print(f\"Batch size: 32, Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_discrete(p_logits, q_logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between two discrete distributions from logits.\n",
    "    \n",
    "    Args:\n",
    "        p_logits: Target distribution logits [batch_size, num_classes]\n",
    "        q_logits: Predicted distribution logits [batch_size, num_classes]\n",
    "        temperature: Temperature for softening distributions (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence loss (scalar)\n",
    "    \n",
    "    Note:\n",
    "        - Higher temperature produces softer probability distributions\n",
    "        - Temperature T=1.0 is standard softmax\n",
    "        - Uses log-space for numerical stability\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling to both distributions\n",
    "    p_scaled = p_logits / temperature\n",
    "    q_scaled = q_logits / temperature\n",
    "    \n",
    "    # Convert to log probabilities (log_softmax is numerically stable)\n",
    "    log_p = F.log_softmax(p_scaled, dim=-1)\n",
    "    log_q = F.log_softmax(q_scaled, dim=-1)\n",
    "    \n",
    "    # KL(P || Q) = sum(P * log(P/Q)) = sum(P * (log(P) - log(Q)))\n",
    "    # In PyTorch's F.kl_div, we need to pass log_q and p (not log_p)\n",
    "    # But for manual calculation: KL = exp(log_p) * (log_p - log_q)\n",
    "    p = torch.exp(log_p)\n",
    "    kl = torch.sum(p * (log_p - log_q), dim=-1)\n",
    "    \n",
    "    return kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test discrete KL divergence\n",
    "print(\"Testing Discrete KL Divergence\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Temperature = 1.0\n",
    "p_logits = torch.tensor([[2.0, 1.0, 0.1], [1.0, 2.0, 0.5]])\n",
    "q_logits = torch.tensor([[1.8, 1.1, 0.2], [0.9, 2.1, 0.6]])\n",
    "\n",
    "kl_custom = kl_divergence_discrete(p_logits, q_logits, temperature=1.0)\n",
    "\n",
    "# PyTorch reference: F.kl_div expects log_q and p (not log_p)\n",
    "log_p = F.log_softmax(p_logits, dim=-1)\n",
    "log_q = F.log_softmax(q_logits, dim=-1)\n",
    "kl_pytorch = F.kl_div(log_q, log_p, reduction='batchmean', log_target=True)\n",
    "\n",
    "print(f\"Custom KL (T=1.0): {kl_custom.item():.6f}\")\n",
    "print(f\"PyTorch KL (T=1.0): {kl_pytorch.item():.6f}\")\n",
    "assert torch.allclose(kl_custom, kl_pytorch, atol=1e-6, rtol=1e-5), \"KL divergence mismatch at T=1.0!\"\n",
    "print(\"✓ Test passed for T=1.0\\n\")\n",
    "\n",
    "# Test 2: Temperature = 3.0 (softer distributions)\n",
    "kl_custom_t3 = kl_divergence_discrete(p_logits, q_logits, temperature=3.0)\n",
    "\n",
    "log_p_t3 = F.log_softmax(p_logits / 3.0, dim=-1)\n",
    "log_q_t3 = F.log_softmax(q_logits / 3.0, dim=-1)\n",
    "kl_pytorch_t3 = F.kl_div(log_q_t3, log_p_t3, reduction='batchmean', log_target=True)\n",
    "\n",
    "print(f\"Custom KL (T=3.0): {kl_custom_t3.item():.6f}\")\n",
    "print(f\"PyTorch KL (T=3.0): {kl_pytorch_t3.item():.6f}\")\n",
    "assert torch.allclose(kl_custom_t3, kl_pytorch_t3, atol=1e-6, rtol=1e-5), \"KL divergence mismatch at T=3.0!\"\n",
    "print(\"✓ Test passed for T=3.0\")\n",
    "\n",
    "print(f\"\\nNote: Higher temperature reduces KL divergence ({kl_custom.item():.6f} → {kl_custom_t3.item():.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between two Gaussian distributions.\n",
    "    \n",
    "    Args:\n",
    "        mu_p: Mean of distribution P (scalar or tensor)\n",
    "        sigma_p: Standard deviation of distribution P (must be positive)\n",
    "        mu_q: Mean of distribution Q (scalar or tensor)\n",
    "        sigma_q: Standard deviation of distribution Q (must be positive)\n",
    "    \n",
    "    Returns:\n",
    "        KL(P || Q) for Gaussian distributions\n",
    "    \n",
    "    Formula:\n",
    "        KL(N(μ_p, σ_p²) || N(μ_q, σ_q²)) = \n",
    "            log(σ_q/σ_p) + (σ_p² + (μ_p - μ_q)²) / (2σ_q²) - 1/2\n",
    "    \"\"\"\n",
    "    # Ensure positive standard deviations\n",
    "    assert (sigma_p > 0).all() and (sigma_q > 0).all(), \"Standard deviations must be positive\"\n",
    "    \n",
    "    # KL divergence formula for Gaussians\n",
    "    var_p = sigma_p ** 2\n",
    "    var_q = sigma_q ** 2\n",
    "    \n",
    "    kl = torch.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n",
    "    \n",
    "    return kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Gaussian KL divergence\n",
    "print(\"Testing Gaussian KL Divergence\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Simple Gaussian distributions\n",
    "mu_p = torch.tensor([0.0, 1.0, -0.5])\n",
    "sigma_p = torch.tensor([1.0, 1.5, 0.8])\n",
    "mu_q = torch.tensor([0.1, 1.2, -0.4])\n",
    "sigma_q = torch.tensor([1.1, 1.4, 0.9])\n",
    "\n",
    "kl_custom = kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q)\n",
    "\n",
    "# PyTorch reference using torch.distributions\n",
    "from torch.distributions import Normal, kl_divergence as torch_kl\n",
    "\n",
    "p_dist = Normal(mu_p, sigma_p)\n",
    "q_dist = Normal(mu_q, sigma_q)\n",
    "kl_pytorch = torch_kl(p_dist, q_dist).mean()\n",
    "\n",
    "print(f\"Custom Gaussian KL: {kl_custom.item():.6f}\")\n",
    "print(f\"PyTorch Gaussian KL: {kl_pytorch.item():.6f}\")\n",
    "assert torch.allclose(kl_custom, kl_pytorch, atol=1e-6, rtol=1e-5), \"Gaussian KL divergence mismatch!\"\n",
    "print(\"✓ Test passed for Gaussian KL divergence\\n\")\n",
    "\n",
    "# Test 2: Identical distributions (KL should be ~0)\n",
    "kl_identical = kl_divergence_gaussian(mu_p, sigma_p, mu_p, sigma_p)\n",
    "print(f\"KL for identical distributions: {kl_identical.item():.8f} (should be ~0)\")\n",
    "assert torch.allclose(kl_identical, torch.tensor(0.0), atol=1e-6), \"KL should be 0 for identical distributions!\"\n",
    "print(\"✓ Test passed for identical distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, alpha=0.7, temperature=3.0):\n",
    "    \"\"\"\n",
    "    Combined distillation loss for knowledge transfer.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Student model outputs [batch_size, num_classes]\n",
    "        teacher_logits: Teacher model outputs [batch_size, num_classes] (detached)\n",
    "        labels: Ground truth labels [batch_size]\n",
    "        alpha: Weight for soft targets (default: 0.7 = 70% distillation)\n",
    "        temperature: Temperature for soft targets (default: 3.0)\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss: alpha * L_soft + (1-alpha) * L_hard\n",
    "    \n",
    "    Notes:\n",
    "        - L_soft uses temperature-scaled KL divergence (scaled by T²)\n",
    "        - L_hard is standard cross-entropy with true labels\n",
    "        - Higher alpha prioritizes learning from teacher\n",
    "    \"\"\"\n",
    "    # Soft target loss: KL divergence between teacher and student (with temperature)\n",
    "    # Scale by T² to balance gradient magnitudes (see Hinton et al. 2015)\n",
    "    soft_loss = kl_divergence_discrete(teacher_logits, student_logits, temperature) * (temperature ** 2)\n",
    "    \n",
    "    # Hard target loss: Standard cross-entropy with ground truth\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Combine losses with alpha weighting\n",
    "    total_loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Large teacher model with 4 hidden layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize teacher and print parameter count\n",
    "teacher = TeacherModel()\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "print(f\"Teacher Model: {teacher_params:,} parameters\")\n",
    "print(f\"Architecture: 784 → 256 → 256 → 256 → 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Smaller student model with 2 hidden layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize student and print parameter count\n",
    "student = StudentModel()\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Student Model: {student_params:,} parameters\")\n",
    "print(f\"Architecture: 784 → 64 → 10\")\n",
    "print(f\"Compression ratio: {teacher_params / student_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train teacher model\n",
    "print(\"Training Teacher Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "teacher = TeacherModel()\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "teacher.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = teacher(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/20], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Teacher training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train student with distillation\n",
    "print(\"Training Student Model with Distillation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "student_distilled = StudentModel()\n",
    "optimizer = torch.optim.Adam(student_distilled.parameters(), lr=0.001)\n",
    "\n",
    "teacher.eval()  # Teacher in eval mode\n",
    "student_distilled.train()\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Get teacher outputs (no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(batch_x)\n",
    "        \n",
    "        # Student forward pass\n",
    "        student_logits = student_distilled(batch_x)\n",
    "        \n",
    "        # Distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, batch_y, alpha=0.7, temperature=3.0)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(student_logits, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/20], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Student distillation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline student (without distillation)\n",
    "print(\"Training Baseline Student Model (No Distillation)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "student_baseline = StudentModel()\n",
    "optimizer = torch.optim.Adam(student_baseline.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "student_baseline.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = student_baseline(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/20], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Baseline student training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Calculate accuracy on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Evaluate all models\n",
    "teacher_acc = evaluate_model(teacher, dataloader)\n",
    "student_distilled_acc = evaluate_model(student_distilled, dataloader)\n",
    "student_baseline_acc = evaluate_model(student_baseline, dataloader)\n",
    "\n",
    "print(\"Final Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Teacher Model: {teacher_acc:.2%} accuracy (~{teacher_params:,} params)\")\n",
    "print(f\"Student (Distilled): {student_distilled_acc:.2%} accuracy (~{student_params:,} params)\")\n",
    "print(f\"Student (Baseline): {student_baseline_acc:.2%} accuracy (~{student_params:,} params)\")\n",
    "print(f\"\\nDistillation Improvement: {(student_distilled_acc - student_baseline_acc)*100:.1f}%\")\n",
    "print(f\"Gap from Teacher: {(teacher_acc - student_distilled_acc)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effect\n",
    "print(\"\\nTemperature Effect on Soft Targets\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "temperatures = [1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "sample_logits = torch.tensor([2.0, 1.0, 0.1, -1.0, -2.0])\n",
    "\n",
    "fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n",
    "\n",
    "for ax, T in zip(axes, temperatures):\n",
    "    probs = F.softmax(sample_logits / T, dim=0)\n",
    "    ax.bar(range(len(probs)), probs.numpy())\n",
    "    ax.set_title(f'T={T}')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Temperature on Soft Target Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Higher temperatures produce softer (more uniform) distributions.\")\n",
    "print(\"This helps the student learn from the teacher's relative confidences, not just the top class.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
