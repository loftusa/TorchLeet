{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Grouped Query Attention from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Standard Multi-Head Attention (MHA) assigns a separate key and value projection to each attention head. **Grouped Query Attention (GQA)** is a more efficient variant where multiple query heads share the same key-value heads.\n",
    "\n",
    "### Background: Why GQA?\n",
    "\n",
    "During autoregressive generation, the KV cache can become a memory bottleneck:\n",
    "- For a 70B model with 64 heads and 8K context, MHA needs ~20GB just for KV cache\n",
    "- GQA reduces this by sharing K/V across multiple query heads\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "| Attention Type | Query Heads | KV Heads | KV Cache Size |\n",
    "|---------------|-------------|----------|---------------|\n",
    "| MHA | 32 | 32 | 32 * seq * d_head * 2 |\n",
    "| GQA (8 groups) | 32 | 8 | **8 * seq * d_head * 2** |\n",
    "| MQA (1 group) | 32 | 1 | **1 * seq * d_head * 2** |\n",
    "\n",
    "**Real-world examples:**\n",
    "- LLaMA-2 70B: 64 query heads, 8 KV heads = **8x memory reduction**\n",
    "- Mistral 7B: 32 query heads, 8 KV heads = **4x memory reduction**\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: Mask creation (causal, padding, KV cache)\n",
    "2. **Part 2**: Core GQA mechanism - understand the repeat_interleave for grouped K/V\n",
    "3. **Part 3**: GQA Self-Attention - Q, K, V from projections of single input x\n",
    "4. **Part 4**: GQA with KV Cache - see the actual memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Attention Mask Creation\n",
    "\n",
    "The mask functions are the same as standard attention. For GQA, the mask broadcasts across all heads.\n",
    "\n",
    "Mask convention: **True = masked (cannot attend), False = can attend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len_q: int, seq_len_k: int = None, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask for autoregressive models.\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length (defaults to seq_len_q)\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "              True = position should be MASKED (cannot attend)\n",
    "              False = position can be attended to\n",
    "    \n",
    "    Example for seq_len=4:\n",
    "        Q\\\\K   0      1      2      3\n",
    "        0   [False, True,  True,  True ]   # Query 0 attends only to Key 0\n",
    "        1   [False, False, True,  True ]   # Query 1 attends to Keys 0-1\n",
    "        2   [False, False, False, True ]   # Query 2 attends to Keys 0-2\n",
    "        3   [False, False, False, False]   # Query 3 attends to Keys 0-3\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a padding mask for variable-length sequences.\n",
    "    \n",
    "    Args:\n",
    "        lengths: Tensor of shape (batch_size,) containing actual sequence lengths\n",
    "        max_len: Maximum sequence length (padded length)\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (batch_size, max_len)\n",
    "              True = padding position (should be MASKED)\n",
    "              False = real token (can be attended to)\n",
    "    \n",
    "    Example:\n",
    "        lengths = [3, 5, 2], max_len = 5\n",
    "        \n",
    "        Returns:\n",
    "        [[False, False, False, True,  True ],   # seq 0: tokens 0-2 real, 3-4 padding\n",
    "         [False, False, False, False, False],   # seq 1: all 5 tokens real\n",
    "         [False, False, True,  True,  True ]]   # seq 2: tokens 0-1 real, 2-4 padding\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int = None,\n",
    "    is_causal: bool = True,\n",
    "    key_padding_lengths: torch.Tensor = None,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a combined attention mask for GQA.\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Query sequence length\n",
    "        seq_len_k: Key sequence length\n",
    "        is_causal: Whether to apply causal masking\n",
    "        key_padding_lengths: If provided, actual lengths of key sequences (batch_size,)\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask with shape suitable for broadcasting to (batch, num_heads, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask_with_cache(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int,\n",
    "    cache_len: int,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for attention with KV cache.\n",
    "    New queries can attend to all cached positions.\n",
    "    \n",
    "    Args:\n",
    "        seq_len_q: Number of new query tokens (typically 1 during generation)\n",
    "        seq_len_k: Total key length (cache_len + seq_len_q)\n",
    "        cache_len: Number of cached tokens\n",
    "        device: Device to create tensor on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor of shape (seq_len_q, seq_len_k)\n",
    "    \n",
    "    Example: cache_len=3, seq_len_q=2, seq_len_k=5\n",
    "        Keys:    [cached0, cached1, cached2, new0, new1]\n",
    "        \n",
    "        Q\\\\K      c0     c1     c2    new0   new1\n",
    "        new0  [False, False, False, False, True ]  # new0 attends to cache + itself\n",
    "        new1  [False, False, False, False, False]  # new1 attends to cache + new0 + itself\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mask creation\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal mask (4x4):\")\n",
    "print(mask)\n",
    "\n",
    "# Test with padding\n",
    "lengths = torch.tensor([4, 3])\n",
    "mask_combined = create_attention_mask(4, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"\\nCombined mask shape: {mask_combined.shape}\")\n",
    "print(\"\\n\\u2713 Mask tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core GQA Mechanism\n",
    "\n",
    "The key difference from standard MHA:\n",
    "- Q has `num_query_heads` heads\n",
    "- K and V have `num_kv_heads` heads (fewer!)\n",
    "- We expand K, V using `repeat_interleave` to match Q's head count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_query_heads = 8\n",
    "num_kv_heads = 2  # 4x fewer KV heads than query heads\n",
    "d_head = d_model // num_query_heads\n",
    "\n",
    "print(f\"d_model={d_model}\")\n",
    "print(f\"num_query_heads={num_query_heads}, num_kv_heads={num_kv_heads}\")\n",
    "print(f\"d_head={d_head}\")\n",
    "print(f\"Query heads per KV head: {num_query_heads // num_kv_heads}\")\n",
    "print(f\"\\nKV projection size: {num_kv_heads * d_head} (vs {d_model} for full MHA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Core GQA computation (Q, K, V already projected and reshaped to heads).\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, num_query_heads, seq_len, d_head)\n",
    "        K: Key tensor (batch, num_kv_heads, seq_len, d_head) - fewer heads!\n",
    "        V: Value tensor (batch, num_kv_heads, seq_len, d_head) - fewer heads!\n",
    "        num_query_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads\n",
    "        mask: Optional boolean attention mask (True = masked)\n",
    "    \n",
    "    Returns:\n",
    "        output: GQA output (batch, num_query_heads, seq_len, d_head)\n",
    "    \n",
    "    Hints:\n",
    "        - Use repeat_interleave to expand K, V to match query heads\n",
    "        - The repeat factor is num_query_heads // num_kv_heads\n",
    "        - After expansion, use standard scaled dot-product attention\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core GQA mechanism\n",
    "Q = torch.randn(batch_size, num_query_heads, seq_len, d_head)\n",
    "K = torch.randn(batch_size, num_kv_heads, seq_len, d_head)  # Fewer heads!\n",
    "V = torch.randn(batch_size, num_kv_heads, seq_len, d_head)  # Fewer heads!\n",
    "\n",
    "print(f\"Q shape: {Q.shape} ({num_query_heads} heads)\")\n",
    "print(f\"K shape: {K.shape} ({num_kv_heads} heads)\")\n",
    "print(f\"V shape: {V.shape} ({num_kv_heads} heads)\")\n",
    "\n",
    "output = grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "assert output.shape == Q.shape\n",
    "print(\"\\n\\u2713 Core GQA mechanism test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_causal = grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads, mask=causal_mask)\n",
    "\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "print(\"\\u2713 GQA with causal mask works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GQA Self-Attention\n",
    "\n",
    "Full GQA where Q, K, V come from projections of a single input x.\n",
    "\n",
    "Key difference from MHA:\n",
    "- Q projection: `d_model -> d_model` (all query heads)\n",
    "- K/V projection: `d_model -> kv_dim` where `kv_dim = num_kv_heads * d_head` (smaller!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQuerySelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Self-Attention where Q, K, V come from projections of the same input,\n",
    "    but K and V have fewer heads than Q.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model: Total embedding dimension\n",
    "        num_query_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads (fewer than query heads)\n",
    "        d_head: Dimension per head (d_model // num_query_heads)\n",
    "        kv_dim: K/V projection dimension (num_kv_heads * d_head) - smaller than d_model!\n",
    "        W_q: Query projection (d_model -> d_model)\n",
    "        W_k, W_v: Key/Value projections (d_model -> kv_dim) - smaller!\n",
    "        W_o: Output projection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, num_kv_heads: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        is_causal: bool = False,\n",
    "        key_padding_lengths: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            is_causal: Whether to apply causal masking\n",
    "            key_padding_lengths: If provided, actual lengths for padding mask\n",
    "        \n",
    "        Returns:\n",
    "            output: GQA output (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GQA Self-Attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Single input - the residual stream\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(\"This single input will be projected to create Q, K, V\")\n",
    "print(f\"Q: {d_model} dims ({num_query_heads} heads)\")\n",
    "print(f\"K/V: {num_kv_heads * d_head} dims ({num_kv_heads} heads) - smaller!\")\n",
    "\n",
    "# Create GQA layer\n",
    "gqa = GroupedQuerySelfAttention(d_model, num_query_heads, num_kv_heads)\n",
    "\n",
    "# Forward pass (bidirectional)\n",
    "output = gqa(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "# Forward pass (causal)\n",
    "output_causal = gqa(x, is_causal=True)\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "\n",
    "# Forward pass (with padding)\n",
    "lengths = torch.tensor([8, 5])\n",
    "output_padded = gqa(x, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"Padded output shape: {output_padded.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "mha_params = d_model * d_model * 4  # Q, K, V, O each d_model x d_model\n",
    "gqa_params = d_model * d_model + 2 * d_model * (num_kv_heads * d_head) + d_model * d_model\n",
    "print(f\"\\nMHA projection params: {mha_params:,}\")\n",
    "print(f\"GQA projection params: {gqa_params:,}\")\n",
    "print(f\"Parameter reduction: {(1 - gqa_params/mha_params)*100:.1f}%\")\n",
    "\n",
    "assert output.shape == x.shape\n",
    "print(\"\\n\\u2713 GQA Self-Attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: GQA with KV Cache\n",
    "\n",
    "### Why GQA Dramatically Reduces KV Cache Memory\n",
    "\n",
    "The cache only needs to store K and V for `num_kv_heads`, not `num_query_heads`!\n",
    "\n",
    "**Cache shape comparison:**\n",
    "- MHA: `(batch, num_query_heads, seq_len, head_dim)`\n",
    "- GQA: `(batch, num_kv_heads, seq_len, head_dim)` - smaller!\n",
    "\n",
    "For LLaMA-2 70B with 8K context:\n",
    "- MHA cache: ~20 GB\n",
    "- GQA cache (8 KV heads): ~2.5 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQAWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Self-Attention with KV Cache for efficient inference.\n",
    "    \n",
    "    Takes a single input x (the residual stream) and projects it to Q, K, V.\n",
    "    The cache stores K and V with fewer heads than Q.\n",
    "    \n",
    "    Attributes:\n",
    "        d_model: Total embedding dimension\n",
    "        num_query_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads (fewer than query heads)\n",
    "        head_dim: Dimension per head (d_model // num_query_heads)\n",
    "        kv_dim: K/V projection dimension (num_kv_heads * head_dim) - smaller than d_model!\n",
    "        q_proj, k_proj, v_proj: Projections\n",
    "        out_proj: Output projection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, num_kv_heads: int):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cache_k: torch.Tensor = None,\n",
    "        cache_v: torch.Tensor = None,\n",
    "        is_causal: bool = True,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            cache_k: Cached keys (batch, num_kv_heads, cached_len, head_dim)\n",
    "            cache_v: Cached values (batch, num_kv_heads, cached_len, head_dim)\n",
    "            is_causal: Whether to apply causal masking\n",
    "        \n",
    "        Returns:\n",
    "            output: GQA output (batch, seq_len, d_model)\n",
    "            new_cache_k: Updated key cache\n",
    "            new_cache_v: Updated value cache\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GQA with KV Cache\n",
    "print(\"=== Testing GQA with KV Cache ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "num_query_heads = 8\n",
    "num_kv_heads = 2  # 4x fewer KV heads!\n",
    "\n",
    "gqa_cached = GQAWithCache(d_model, num_query_heads, num_kv_heads)\n",
    "\n",
    "# Step 1: Process prompt (3 tokens)\n",
    "# x is the residual stream - Q, K, V all come from projections of x\n",
    "prompt = torch.randn(batch_size, 3, d_model)\n",
    "print(f\"\\nInput prompt shape: {prompt.shape}\")\n",
    "print(f\"Q will have {num_query_heads} heads, K/V will have {num_kv_heads} heads\")\n",
    "\n",
    "out1, cache_k, cache_v = gqa_cached(prompt, None, None)\n",
    "print(f\"\\nAfter prompt: cache shape = {cache_k.shape}\")\n",
    "print(f\"  Note: only {num_kv_heads} KV heads cached, not {num_query_heads}!\")\n",
    "\n",
    "# Step 2: Generate token 4\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out2, cache_k, cache_v = gqa_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 4: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out3, cache_k, cache_v = gqa_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 5: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Verify\n",
    "head_dim = d_model // num_query_heads\n",
    "assert cache_k.shape == (batch_size, num_kv_heads, 5, head_dim)\n",
    "assert out3.shape == (batch_size, 1, d_model)\n",
    "\n",
    "print(\"\\n\\u2713 GQA with KV Cache test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison: GQA vs MHA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Memory Comparison: GQA vs MHA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mha_cache = batch_size * num_query_heads * 5 * head_dim * 2  # K + V\n",
    "gqa_cache = batch_size * num_kv_heads * 5 * head_dim * 2     # K + V\n",
    "\n",
    "print(f\"MHA cache elements: {mha_cache:,} ({num_query_heads} heads)\")\n",
    "print(f\"GQA cache elements: {gqa_cache:,} ({num_kv_heads} heads)\")\n",
    "print(f\"Memory reduction: {mha_cache / gqa_cache:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Real-World Scaling (LLaMA-2 70B, 8K context)\")\n",
    "print(\"=\"*60)\n",
    "print(\"MHA: 64 heads x 8K x 128 dim x 2 (K+V) x 80 layers x 2 bytes = ~20 GB\")\n",
    "print(\"GQA:  8 heads x 8K x 128 dim x 2 (K+V) x 80 layers x 2 bytes = ~2.5 GB\")\n",
    "print(\"Savings: 8x memory reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "**Q: What's the difference between MHA, GQA, and MQA?**\n",
    "A:\n",
    "- MHA: Each query head has its own K and V heads (1:1 ratio)\n",
    "- GQA: Multiple query heads share K/V heads (e.g., 8:1 ratio)\n",
    "- MQA: All query heads share a single K/V head (all:1 ratio)\n",
    "\n",
    "**Q: Why does GQA reduce memory but not compute?**\n",
    "A: The K/V projections are smaller, saving some compute. But during attention, we expand K/V using repeat_interleave, so the actual attention computation is similar to MHA. The main savings are in KV cache memory.\n",
    "\n",
    "**Q: When should you use GQA vs MHA?**\n",
    "A: GQA is preferred for large models where KV cache memory is a bottleneck (inference with long contexts). For training or small models, MHA is fine.\n",
    "\n",
    "**Q: How do you choose the number of KV heads?**\n",
    "A: Common ratios are 4:1 to 8:1 (query:KV heads). LLaMA-2 70B uses 64:8, Mistral uses 32:8. The ratio depends on the tradeoff between quality and memory.\n",
    "\n",
    "**Q: Does GQA hurt model quality?**\n",
    "A: Slightly, but the tradeoff is usually worthwhile. GQA models achieve ~95-99% of MHA quality while using 4-8x less KV cache memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
