{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Grouped Query Attention from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Standard Multi-Head Attention (MHA) assigns a separate key and value projection to each attention head. **Grouped Query Attention (GQA)** is a more efficient variant where multiple query heads share the same key-value heads.\n",
    "\n",
    "### Background: Why GQA?\n",
    "\n",
    "During autoregressive generation, the KV cache can become a memory bottleneck:\n",
    "- For a 70B model with 64 heads and 8K context, MHA needs ~20GB just for KV cache\n",
    "- GQA reduces this by sharing K/V across multiple query heads\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "| Attention Type | Query Heads | KV Heads | KV Cache Size |\n",
    "|---------------|-------------|----------|---------------|\n",
    "| MHA | 32 | 32 | 32 * seq * d_head * 2 |\n",
    "| GQA (8 groups) | 32 | 8 | **8 * seq * d_head * 2** |\n",
    "| MQA (1 group) | 32 | 1 | **1 * seq * d_head * 2** |\n",
    "\n",
    "**Real-world examples:**\n",
    "- LLaMA-2 70B: 64 query heads, 8 KV heads = **8x memory reduction**\n",
    "- Mistral 7B: 32 query heads, 8 KV heads = **4x memory reduction**\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. **Part 1**: Mask creation (causal, padding, KV cache)\n",
    "2. **Part 2**: Core GQA mechanism - understand the repeat_interleave for grouped K/V\n",
    "3. **Part 3**: GQA Self-Attention - Q, K, V from projections of single input x\n",
    "4. **Part 4**: GQA with KV Cache - see the actual memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Attention Mask Creation\n",
    "\n",
    "The mask functions are the same as standard attention. For GQA, the mask broadcasts across all heads.\n",
    "\n",
    "Mask convention: **True = masked (cannot attend), False = can attend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len_q: int, seq_len_k: int = None, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask for autoregressive models.\n",
    "    Shape: (seq_len_q, seq_len_k) -> broadcasts to (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "    return torch.triu(torch.ones(seq_len_q, seq_len_k, dtype=torch.bool, device=device), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a padding mask for variable-length sequences.\n",
    "    Shape: (batch, max_len)\n",
    "    \"\"\"\n",
    "    positions = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n",
    "    return positions >= lengths.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int = None,\n",
    "    is_causal: bool = True,\n",
    "    key_padding_lengths: torch.Tensor = None,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a combined attention mask for GQA.\n",
    "    Returns mask with shape suitable for broadcasting to (batch, num_heads, seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "\n",
    "    mask = torch.zeros(seq_len_q, seq_len_k, dtype=torch.bool, device=device)\n",
    "\n",
    "    if is_causal:\n",
    "        causal = create_causal_mask(seq_len_q, seq_len_k, device=device)\n",
    "        mask = mask | causal\n",
    "\n",
    "    if key_padding_lengths is not None:\n",
    "        padding = create_padding_mask(key_padding_lengths, seq_len_k)\n",
    "        padding = padding.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_k)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0) | padding  # (batch, 1, seq_q, seq_k)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask_with_cache(\n",
    "    seq_len_q: int,\n",
    "    seq_len_k: int,\n",
    "    cache_len: int,\n",
    "    device=None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for attention with KV cache.\n",
    "    New queries can attend to all cached positions.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(seq_len_q, seq_len_k, dtype=torch.bool, device=device)\n",
    "\n",
    "    if seq_len_q > 1:\n",
    "        new_token_mask = create_causal_mask(seq_len_q, seq_len_q, device=device)\n",
    "        mask[:, cache_len:] = new_token_mask\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mask creation\n",
    "mask = create_causal_mask(4)\n",
    "print(\"Causal mask (4x4):\")\n",
    "print(mask)\n",
    "\n",
    "# Test with padding\n",
    "lengths = torch.tensor([4, 3])\n",
    "mask_combined = create_attention_mask(4, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"\\nCombined mask shape: {mask_combined.shape}\")\n",
    "print(\"\\n\\u2713 Mask tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core GQA Mechanism\n",
    "\n",
    "The key difference from standard MHA:\n",
    "- Q has `num_query_heads` heads\n",
    "- K and V have `num_kv_heads` heads (fewer!)\n",
    "- We expand K, V using `repeat_interleave` to match Q's head count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_query_heads = 8\n",
    "num_kv_heads = 2  # 4x fewer KV heads than query heads\n",
    "d_head = d_model // num_query_heads\n",
    "\n",
    "print(f\"d_model={d_model}\")\n",
    "print(f\"num_query_heads={num_query_heads}, num_kv_heads={num_kv_heads}\")\n",
    "print(f\"d_head={d_head}\")\n",
    "print(f\"Query heads per KV head: {num_query_heads // num_kv_heads}\")\n",
    "print(f\"\\nKV projection size: {num_kv_heads * d_head} (vs {d_model} for full MHA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Core GQA computation (Q, K, V already projected and reshaped to heads).\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, num_query_heads, seq_len, d_head)\n",
    "        K: Key tensor (batch, num_kv_heads, seq_len, d_head) - fewer heads!\n",
    "        V: Value tensor (batch, num_kv_heads, seq_len, d_head) - fewer heads!\n",
    "        num_query_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads\n",
    "        mask: Optional boolean attention mask (True = masked)\n",
    "    \n",
    "    Returns:\n",
    "        output: GQA output (batch, num_query_heads, seq_len, d_head)\n",
    "    \"\"\"\n",
    "    batch_size, _, seq_len, d_head = Q.shape\n",
    "    \n",
    "    # Expand K, V to match query heads\n",
    "    # Each KV head is shared by (num_query_heads // num_kv_heads) query heads\n",
    "    repeat_factor = num_query_heads // num_kv_heads\n",
    "    K = K.repeat_interleave(repeat_factor, dim=1)  # (batch, num_query_heads, seq, d_head)\n",
    "    V = V.repeat_interleave(repeat_factor, dim=1)\n",
    "    \n",
    "    # Standard attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core GQA mechanism\n",
    "Q = torch.randn(batch_size, num_query_heads, seq_len, d_head)\n",
    "K = torch.randn(batch_size, num_kv_heads, seq_len, d_head)  # Fewer heads!\n",
    "V = torch.randn(batch_size, num_kv_heads, seq_len, d_head)  # Fewer heads!\n",
    "\n",
    "print(f\"Q shape: {Q.shape} ({num_query_heads} heads)\")\n",
    "print(f\"K shape: {K.shape} ({num_kv_heads} heads)\")\n",
    "print(f\"V shape: {V.shape} ({num_kv_heads} heads)\")\n",
    "\n",
    "output = grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "assert output.shape == Q.shape\n",
    "print(\"\\n\\u2713 Core GQA mechanism test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_causal = grouped_query_attention_core(Q, K, V, num_query_heads, num_kv_heads, mask=causal_mask)\n",
    "\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "print(\"\\u2713 GQA with causal mask works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GQA Self-Attention\n",
    "\n",
    "Full GQA where Q, K, V come from projections of a single input x.\n",
    "\n",
    "Key difference from MHA:\n",
    "- Q projection: `d_model -> d_model` (all query heads)\n",
    "- K/V projection: `d_model -> kv_dim` where `kv_dim = num_kv_heads * d_head` (smaller!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQuerySelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Self-Attention where Q, K, V come from projections of the same input,\n",
    "    but K and V have fewer heads than Q.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, num_kv_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_query_heads == 0\n",
    "        assert num_query_heads % num_kv_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.d_head = d_model // num_query_heads\n",
    "        self.kv_dim = num_kv_heads * self.d_head  # Smaller than d_model!\n",
    "        \n",
    "        # Q projection: full d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # K/V projections: smaller!\n",
    "        self.W_k = nn.Linear(d_model, self.kv_dim, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, self.kv_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        is_causal: bool = False,\n",
    "        key_padding_lengths: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            is_causal: Whether to apply causal masking\n",
    "            key_padding_lengths: If provided, actual lengths for padding mask\n",
    "        \n",
    "        Returns:\n",
    "            output: GQA output (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project x to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_k(x)  # (batch, seq, kv_dim) - smaller!\n",
    "        V = self.W_v(x)  # (batch, seq, kv_dim) - smaller!\n",
    "        \n",
    "        # Reshape to heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Expand K, V to match query heads\n",
    "        repeat_factor = self.num_query_heads // self.num_kv_heads\n",
    "        K = K.repeat_interleave(repeat_factor, dim=1)\n",
    "        V = V.repeat_interleave(repeat_factor, dim=1)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        \n",
    "        # Create and apply mask\n",
    "        if is_causal or key_padding_lengths is not None:\n",
    "            mask = create_attention_mask(\n",
    "                seq_len_q=seq_len,\n",
    "                seq_len_k=seq_len,\n",
    "                is_causal=is_causal,\n",
    "                key_padding_lengths=key_padding_lengths,\n",
    "                device=x.device\n",
    "            )\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GQA Self-Attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Single input - the residual stream\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(\"This single input will be projected to create Q, K, V\")\n",
    "print(f\"Q: {d_model} dims ({num_query_heads} heads)\")\n",
    "print(f\"K/V: {num_kv_heads * d_head} dims ({num_kv_heads} heads) - smaller!\")\n",
    "\n",
    "# Create GQA layer\n",
    "gqa = GroupedQuerySelfAttention(d_model, num_query_heads, num_kv_heads)\n",
    "\n",
    "# Forward pass (bidirectional)\n",
    "output = gqa(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "# Forward pass (causal)\n",
    "output_causal = gqa(x, is_causal=True)\n",
    "print(f\"Causal output shape: {output_causal.shape}\")\n",
    "\n",
    "# Forward pass (with padding)\n",
    "lengths = torch.tensor([8, 5])\n",
    "output_padded = gqa(x, is_causal=True, key_padding_lengths=lengths)\n",
    "print(f\"Padded output shape: {output_padded.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "mha_params = d_model * d_model * 4  # Q, K, V, O each d_model x d_model\n",
    "gqa_params = d_model * d_model + 2 * d_model * (num_kv_heads * d_head) + d_model * d_model\n",
    "print(f\"\\nMHA projection params: {mha_params:,}\")\n",
    "print(f\"GQA projection params: {gqa_params:,}\")\n",
    "print(f\"Parameter reduction: {(1 - gqa_params/mha_params)*100:.1f}%\")\n",
    "\n",
    "assert output.shape == x.shape\n",
    "print(\"\\n\\u2713 GQA Self-Attention test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GQA degenerates to MHA\n",
    "\n",
    "When `num_query_heads == num_kv_heads`, GQA should behave identically to standard MHA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_query_attention(q, k, v, num_query_heads, num_kv_heads, d_model, mask=None, weights=None):\n",
    "    \"\"\"\n",
    "    GQA function that can use external weights for validation against PyTorch MHA.\n",
    "    \n",
    "    When num_query_heads == num_kv_heads, this degenerates to standard MHA.\n",
    "    \"\"\"\n",
    "    assert d_model % num_query_heads == 0\n",
    "    assert num_query_heads % num_kv_heads == 0\n",
    "    \n",
    "    d_head = d_model // num_query_heads\n",
    "    kv_dim = num_kv_heads * d_head\n",
    "    batch_size, seq_len, _ = q.shape\n",
    "    \n",
    "    # Create projections\n",
    "    Q_w = nn.Linear(d_model, d_model, bias=False)\n",
    "    K_w = nn.Linear(d_model, kv_dim, bias=False)\n",
    "    V_w = nn.Linear(d_model, kv_dim, bias=False)\n",
    "    W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    if weights is not None:\n",
    "        Q_w.weight.data = weights['q_weight']\n",
    "        K_w.weight.data = weights['k_weight']\n",
    "        V_w.weight.data = weights['v_weight']\n",
    "        W_out.weight.data = weights['out_weight']\n",
    "    \n",
    "    # Project\n",
    "    Q = Q_w(q)\n",
    "    K = K_w(k)\n",
    "    V = V_w(v)\n",
    "    \n",
    "    # Reshape to heads\n",
    "    Q = Q.view(batch_size, seq_len, num_query_heads, d_head).transpose(1, 2)\n",
    "    K = K.view(batch_size, seq_len, num_kv_heads, d_head).transpose(1, 2)\n",
    "    V = V.view(batch_size, seq_len, num_kv_heads, d_head).transpose(1, 2)\n",
    "    \n",
    "    # Expand K, V\n",
    "    repeat_factor = num_query_heads // num_kv_heads\n",
    "    K = K.repeat_interleave(repeat_factor, dim=1)\n",
    "    V = V.repeat_interleave(repeat_factor, dim=1)\n",
    "    \n",
    "    # Attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    # Concatenate and project\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "    return W_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: GQA degenerating to MHA\n",
    "torch.manual_seed(42)\n",
    "\n",
    "test_d_model = 64\n",
    "test_num_heads = 4\n",
    "x = torch.randn(batch_size, seq_len, test_d_model)\n",
    "\n",
    "# Create PyTorch MHA reference\n",
    "mha = torch.nn.MultiheadAttention(\n",
    "    embed_dim=test_d_model, num_heads=test_num_heads, bias=False, batch_first=True\n",
    ")\n",
    "\n",
    "# Extract weights\n",
    "weights = {\n",
    "    'q_weight': mha.in_proj_weight[:test_d_model, :],\n",
    "    'k_weight': mha.in_proj_weight[test_d_model:2*test_d_model, :],\n",
    "    'v_weight': mha.in_proj_weight[2*test_d_model:, :],\n",
    "    'out_weight': mha.out_proj.weight\n",
    "}\n",
    "\n",
    "# GQA with equal query and KV heads = MHA\n",
    "output_gqa = grouped_query_attention(\n",
    "    x, x, x,\n",
    "    num_query_heads=test_num_heads,\n",
    "    num_kv_heads=test_num_heads,  # Same as query heads = MHA!\n",
    "    d_model=test_d_model,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "output_mha, _ = mha(x, x, x)\n",
    "\n",
    "assert torch.allclose(output_gqa, output_mha, atol=1e-6), \"GQA doesn't match MHA!\"\n",
    "print(\"\\u2713 GQA matches MHA when num_query_heads == num_kv_heads\")\n",
    "print(f\"Max difference: {(output_gqa - output_mha).abs().max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: GQA with KV Cache\n",
    "\n",
    "### Why GQA Dramatically Reduces KV Cache Memory\n",
    "\n",
    "The cache only needs to store K and V for `num_kv_heads`, not `num_query_heads`!\n",
    "\n",
    "**Cache shape comparison:**\n",
    "- MHA: `(batch, num_query_heads, seq_len, head_dim)`\n",
    "- GQA: `(batch, num_kv_heads, seq_len, head_dim)` - smaller!\n",
    "\n",
    "For LLaMA-2 70B with 8K context:\n",
    "- MHA cache: ~20 GB\n",
    "- GQA cache (8 KV heads): ~2.5 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQAWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Self-Attention with KV Cache for efficient inference.\n",
    "    \n",
    "    Takes a single input x (the residual stream) and projects it to Q, K, V.\n",
    "    The cache stores K and V with fewer heads than Q.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_query_heads: int, num_kv_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_query_heads == 0\n",
    "        assert num_query_heads % num_kv_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_model // num_query_heads\n",
    "        self.kv_dim = num_kv_heads * self.head_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, self.kv_dim, bias=False)  # Smaller!\n",
    "        self.v_proj = nn.Linear(d_model, self.kv_dim, bias=False)  # Smaller!\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cache_k: torch.Tensor = None,\n",
    "        cache_v: torch.Tensor = None,\n",
    "        is_causal: bool = True,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model) - the residual stream\n",
    "            cache_k: Cached keys (batch, num_kv_heads, cached_len, head_dim)\n",
    "            cache_v: Cached values (batch, num_kv_heads, cached_len, head_dim)\n",
    "            is_causal: Whether to apply causal masking\n",
    "        \n",
    "        Returns:\n",
    "            output: GQA output (batch, seq_len, d_model)\n",
    "            new_cache_k: Updated key cache\n",
    "            new_cache_v: Updated value cache\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = x.shape\n",
    "        cache_len = cache_k.size(2) if cache_k is not None else 0\n",
    "        \n",
    "        # Project x to Q, K, V\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)  # Smaller: (batch, seq, kv_dim)\n",
    "        V = self.v_proj(x)  # Smaller: (batch, seq, kv_dim)\n",
    "        \n",
    "        # Reshape to heads\n",
    "        Q = Q.view(batch_size, seq_len_q, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len_q, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len_q, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate with cache (smaller cache!)\n",
    "        if cache_k is not None and cache_v is not None:\n",
    "            K = torch.cat([cache_k, K], dim=2)\n",
    "            V = torch.cat([cache_v, V], dim=2)\n",
    "        \n",
    "        # Update cache (store the smaller K, V)\n",
    "        new_cache_k = K\n",
    "        new_cache_v = V\n",
    "        \n",
    "        seq_len_k = K.size(2)\n",
    "        \n",
    "        # Expand K, V to match query heads for attention\n",
    "        repeat_factor = self.num_query_heads // self.num_kv_heads\n",
    "        K_expanded = K.repeat_interleave(repeat_factor, dim=1)\n",
    "        V_expanded = V.repeat_interleave(repeat_factor, dim=1)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask with cache offset\n",
    "        if is_causal:\n",
    "            mask = create_causal_mask_with_cache(\n",
    "                seq_len_q=seq_len_q,\n",
    "                seq_len_k=seq_len_k,\n",
    "                cache_len=cache_len,\n",
    "                device=x.device\n",
    "            )\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V_expanded)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output, new_cache_k, new_cache_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GQA with KV Cache\n",
    "print(\"=== Testing GQA with KV Cache ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "num_query_heads = 8\n",
    "num_kv_heads = 2  # 4x fewer KV heads!\n",
    "\n",
    "gqa_cached = GQAWithCache(d_model, num_query_heads, num_kv_heads)\n",
    "\n",
    "# Step 1: Process prompt (3 tokens)\n",
    "# x is the residual stream - Q, K, V all come from projections of x\n",
    "prompt = torch.randn(batch_size, 3, d_model)\n",
    "print(f\"\\nInput prompt shape: {prompt.shape}\")\n",
    "print(f\"Q will have {num_query_heads} heads, K/V will have {num_kv_heads} heads\")\n",
    "\n",
    "out1, cache_k, cache_v = gqa_cached(prompt, None, None)\n",
    "print(f\"\\nAfter prompt: cache shape = {cache_k.shape}\")\n",
    "print(f\"  Note: only {num_kv_heads} KV heads cached, not {num_query_heads}!\")\n",
    "\n",
    "# Step 2: Generate token 4\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out2, cache_k, cache_v = gqa_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 4: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Step 3: Generate token 5\n",
    "new_token = torch.randn(batch_size, 1, d_model)\n",
    "out3, cache_k, cache_v = gqa_cached(new_token, cache_k, cache_v)\n",
    "print(f\"After token 5: cache shape = {cache_k.shape}\")\n",
    "\n",
    "# Verify\n",
    "head_dim = d_model // num_query_heads\n",
    "assert cache_k.shape == (batch_size, num_kv_heads, 5, head_dim)\n",
    "assert out3.shape == (batch_size, 1, d_model)\n",
    "\n",
    "print(\"\\n\\u2713 GQA with KV Cache test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison: GQA vs MHA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Memory Comparison: GQA vs MHA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mha_cache = batch_size * num_query_heads * 5 * head_dim * 2  # K + V\n",
    "gqa_cache = batch_size * num_kv_heads * 5 * head_dim * 2     # K + V\n",
    "\n",
    "print(f\"MHA cache elements: {mha_cache:,} ({num_query_heads} heads)\")\n",
    "print(f\"GQA cache elements: {gqa_cache:,} ({num_kv_heads} heads)\")\n",
    "print(f\"Memory reduction: {mha_cache / gqa_cache:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Real-World Scaling (LLaMA-2 70B, 8K context)\")\n",
    "print(\"=\"*60)\n",
    "print(\"MHA: 64 heads x 8K x 128 dim x 2 (K+V) x 80 layers x 2 bytes = ~20 GB\")\n",
    "print(\"GQA:  8 heads x 8K x 128 dim x 2 (K+V) x 80 layers x 2 bytes = ~2.5 GB\")\n",
    "print(\"Savings: 8x memory reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "**Q: What's the difference between MHA, GQA, and MQA?**\n",
    "A:\n",
    "- MHA: Each query head has its own K and V heads (1:1 ratio)\n",
    "- GQA: Multiple query heads share K/V heads (e.g., 8:1 ratio)\n",
    "- MQA: All query heads share a single K/V head (all:1 ratio)\n",
    "\n",
    "**Q: Why does GQA reduce memory but not compute?**\n",
    "A: The K/V projections are smaller, saving some compute. But during attention, we expand K/V using repeat_interleave, so the actual attention computation is similar to MHA. The main savings are in KV cache memory.\n",
    "\n",
    "**Q: When should you use GQA vs MHA?**\n",
    "A: GQA is preferred for large models where KV cache memory is a bottleneck (inference with long contexts). For training or small models, MHA is fine.\n",
    "\n",
    "**Q: How do you choose the number of KV heads?**\n",
    "A: Common ratios are 4:1 to 8:1 (query:KV heads). LLaMA-2 70B uses 64:8, Mistral uses 32:8. The ratio depends on the tradeoff between quality and memory.\n",
    "\n",
    "**Q: Does GQA hurt model quality?**\n",
    "A: Slightly, but the tradeoff is usually worthwhile. GQA models achieve ~95-99% of MHA quality while using 4-8x less KV cache memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
