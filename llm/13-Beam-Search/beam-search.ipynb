{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Decoding\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement beam search, a breadth-first search algorithm that maintains the top-k most promising partial sequences at each decoding step.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Why Not Greedy Decoding?\n",
    "\n",
    "**Greedy decoding** selects the highest-probability token at each step:\n",
    "- Fast (O(V) per step)\n",
    "- But can miss globally optimal sequences\n",
    "- \"The cat sat\" might be better than \"The dog...\" even if P(dog|The) > P(cat|The)\n",
    "\n",
    "**Exhaustive search** considers all possible sequences:\n",
    "- Guarantees optimal solution\n",
    "- But exponential: O(V^T) for vocabulary V and length T\n",
    "- Completely infeasible\n",
    "\n",
    "### Beam Search: A Middle Ground\n",
    "\n",
    "**Beam search** maintains B (beam width) best partial sequences:\n",
    "- At each step, expand all B sequences by all V tokens = B*V candidates\n",
    "- Keep only top B by cumulative log probability\n",
    "- Complexity: O(B*V*T) - linear in sequence length!\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Beam Width (B)**: Number of candidates to keep (typically 4-10)\n",
    "2. **Log Probabilities**: Use log-sum to avoid numerical underflow\n",
    "3. **Length Normalization**: Longer sequences have lower log-probs; normalize to compare fairly\n",
    "4. **Early Stopping**: Stop beams that generate EOS token\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "At step $t$, we have $B$ partial sequences $\\{y^{(1)}_{1:t}, ..., y^{(B)}_{1:t}\\}$.\n",
    "\n",
    "For each sequence $y^{(b)}$:\n",
    "1. Compute next-token distribution: $P(y_{t+1} | y^{(b)}_{1:t})$\n",
    "2. Score all extensions: $\\log P(y^{(b)}_{1:t}) + \\log P(y_{t+1} | y^{(b)}_{1:t})$\n",
    "\n",
    "Select top $B$ from all $B \\times V$ candidates based on cumulative score.\n",
    "\n",
    "**Length Normalization**:\n",
    "$$\\text{score}(y) = \\frac{\\log P(y)}{|y|^\\alpha}$$\n",
    "\n",
    "where $\\alpha \\in [0, 1]$ controls normalization strength (typically 0.6-0.7).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why beam search outperforms greedy decoding\n",
    "2. Implement basic beam search with log probabilities\n",
    "3. Add length normalization for fair comparison\n",
    "4. Handle EOS tokens and early stopping\n",
    "5. Know the tradeoffs of different beam widths\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. `beam_search_step()` - One step of beam search expansion\n",
    "2. `beam_search()` - Full beam search decoding\n",
    "3. `beam_search_with_length_norm()` - With length normalization\n",
    "\n",
    "## Hints\n",
    "\n",
    "1. Use `torch.topk()` to get top-B candidates efficiently\n",
    "2. Track both token sequences and cumulative log-probs\n",
    "3. Use `-float('inf')` for terminated beams to exclude them\n",
    "4. Consider returning top-N final sequences, not just top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BeamHypothesis:\n",
    "    \"\"\"A single beam hypothesis (partial or complete sequence).\"\"\"\n",
    "    tokens: torch.Tensor  # Token IDs\n",
    "    log_prob: float       # Cumulative log probability\n",
    "    is_finished: bool     # Whether EOS was generated\n",
    "    \n",
    "    def score(self, length_penalty: float = 1.0) -> float:\n",
    "        \"\"\"Compute length-normalized score.\"\"\"\n",
    "        length = len(self.tokens)\n",
    "        if length_penalty == 0:\n",
    "            return self.log_prob\n",
    "        return self.log_prob / (length ** length_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_step(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    beams: List[BeamHypothesis],\n",
    "    beam_width: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    ") -> List[BeamHypothesis]:\n",
    "    \"\"\"\n",
    "    Perform one step of beam search expansion.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that takes token sequences and returns logits\n",
    "                  Input: (batch_size, seq_len) -> Output: (batch_size, seq_len, vocab_size)\n",
    "        beams: Current list of beam hypotheses\n",
    "        beam_width: Number of beams to keep\n",
    "        vocab_size: Size of vocabulary\n",
    "        eos_token_id: End-of-sequence token ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        List of new beam hypotheses after expansion\n",
    "    \"\"\"\n",
    "    # Separate finished and active beams\n",
    "    finished_beams = [b for b in beams if b.is_finished]\n",
    "    active_beams = [b for b in beams if not b.is_finished]\n",
    "    \n",
    "    if not active_beams:\n",
    "        return finished_beams\n",
    "    \n",
    "    # Stack all active beam tokens for batched inference\n",
    "    # Pad to same length for batching\n",
    "    max_len = max(len(b.tokens) for b in active_beams)\n",
    "    batch_tokens = torch.zeros(len(active_beams), max_len, dtype=torch.long)\n",
    "    for i, beam in enumerate(active_beams):\n",
    "        batch_tokens[i, :len(beam.tokens)] = beam.tokens\n",
    "    \n",
    "    # Get logits from model\n",
    "    with torch.no_grad():\n",
    "        logits = model_fn(batch_tokens)  # (batch, seq_len, vocab)\n",
    "    \n",
    "    # Get last token logits and convert to log probs\n",
    "    # Use actual sequence lengths\n",
    "    next_token_logits = torch.stack([\n",
    "        logits[i, len(active_beams[i].tokens) - 1] \n",
    "        for i in range(len(active_beams))\n",
    "    ])  # (num_active, vocab)\n",
    "    log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # For each beam, compute scores for all possible next tokens\n",
    "    all_candidates = []\n",
    "    \n",
    "    for beam_idx, beam in enumerate(active_beams):\n",
    "        for token_id in range(vocab_size):\n",
    "            new_log_prob = beam.log_prob + log_probs[beam_idx, token_id].item()\n",
    "            new_tokens = torch.cat([beam.tokens, torch.tensor([token_id])])\n",
    "            is_finished = (eos_token_id is not None and token_id == eos_token_id)\n",
    "            \n",
    "            all_candidates.append(BeamHypothesis(\n",
    "                tokens=new_tokens,\n",
    "                log_prob=new_log_prob,\n",
    "                is_finished=is_finished,\n",
    "            ))\n",
    "    \n",
    "    # Add finished beams to candidates (they don't expand)\n",
    "    all_candidates.extend(finished_beams)\n",
    "    \n",
    "    # Sort by log probability and keep top beam_width\n",
    "    all_candidates.sort(key=lambda x: x.log_prob, reverse=True)\n",
    "    \n",
    "    return all_candidates[:beam_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    prompt_tokens: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    max_length: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> List[BeamHypothesis]:\n",
    "    \"\"\"\n",
    "    Perform beam search decoding.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Model function for next-token prediction\n",
    "        prompt_tokens: Initial token sequence (1D tensor)\n",
    "        beam_width: Number of beams to maintain\n",
    "        max_length: Maximum generation length\n",
    "        vocab_size: Vocabulary size\n",
    "        eos_token_id: EOS token ID (optional)\n",
    "        length_penalty: Length normalization factor (0 = no normalization)\n",
    "        \n",
    "    Returns:\n",
    "        List of final beam hypotheses, sorted by score\n",
    "    \"\"\"\n",
    "    # Initialize with single beam containing prompt\n",
    "    beams = [BeamHypothesis(\n",
    "        tokens=prompt_tokens.clone(),\n",
    "        log_prob=0.0,\n",
    "        is_finished=False,\n",
    "    )]\n",
    "    \n",
    "    # Generate tokens\n",
    "    for step in range(max_length):\n",
    "        beams = beam_search_step(\n",
    "            model_fn=model_fn,\n",
    "            beams=beams,\n",
    "            beam_width=beam_width,\n",
    "            vocab_size=vocab_size,\n",
    "            eos_token_id=eos_token_id,\n",
    "        )\n",
    "        \n",
    "        # Early stop if all beams are finished\n",
    "        if all(b.is_finished for b in beams):\n",
    "            break\n",
    "    \n",
    "    # Sort by score (with length penalty)\n",
    "    beams.sort(key=lambda x: x.score(length_penalty), reverse=True)\n",
    "    \n",
    "    return beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_efficient(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    prompt_tokens: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    max_length: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> List[Tuple[torch.Tensor, float]]:\n",
    "    \"\"\"\n",
    "    Efficient beam search using tensor operations.\n",
    "    Maintains beams as tensors rather than lists.\n",
    "    \n",
    "    Returns:\n",
    "        List of (tokens, score) tuples\n",
    "    \"\"\"\n",
    "    device = prompt_tokens.device\n",
    "    batch_size = 1  # Single sequence beam search\n",
    "    \n",
    "    # Initialize beams: (beam_width, seq_len)\n",
    "    # Start with single beam containing prompt, replicate for beam_width\n",
    "    beam_tokens = prompt_tokens.unsqueeze(0).expand(beam_width, -1).clone()\n",
    "    beam_log_probs = torch.zeros(beam_width, device=device)\n",
    "    beam_log_probs[1:] = float('-inf')  # Only first beam is valid initially\n",
    "    \n",
    "    finished_beams = []  # Store (tokens, score) for finished sequences\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Get logits for all beams\n",
    "        with torch.no_grad():\n",
    "            logits = model_fn(beam_tokens)  # (beam_width, seq_len, vocab)\n",
    "        \n",
    "        # Get next-token log probs\n",
    "        next_log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # (beam_width, vocab)\n",
    "        \n",
    "        # Compute scores for all beam x vocab combinations\n",
    "        # (beam_width, 1) + (beam_width, vocab) = (beam_width, vocab)\n",
    "        candidate_scores = beam_log_probs.unsqueeze(-1) + next_log_probs\n",
    "        \n",
    "        # Flatten and get top-k\n",
    "        flat_scores = candidate_scores.view(-1)  # (beam_width * vocab)\n",
    "        top_scores, top_indices = torch.topk(flat_scores, beam_width)\n",
    "        \n",
    "        # Decode indices back to (beam_idx, token_id)\n",
    "        beam_indices = top_indices // vocab_size\n",
    "        token_indices = top_indices % vocab_size\n",
    "        \n",
    "        # Create new beams\n",
    "        new_beam_tokens = torch.cat([\n",
    "            beam_tokens[beam_indices],\n",
    "            token_indices.unsqueeze(-1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Check for EOS\n",
    "        if eos_token_id is not None:\n",
    "            eos_mask = token_indices == eos_token_id\n",
    "            for i in range(beam_width):\n",
    "                if eos_mask[i]:\n",
    "                    length = new_beam_tokens[i].shape[0]\n",
    "                    score = top_scores[i].item() / (length ** length_penalty) if length_penalty > 0 else top_scores[i].item()\n",
    "                    finished_beams.append((new_beam_tokens[i].clone(), score))\n",
    "                    top_scores[i] = float('-inf')  # Mark as done\n",
    "        \n",
    "        beam_tokens = new_beam_tokens\n",
    "        beam_log_probs = top_scores\n",
    "        \n",
    "        # Early stop if all beams finished\n",
    "        if (beam_log_probs == float('-inf')).all():\n",
    "            break\n",
    "    \n",
    "    # Add remaining active beams to finished\n",
    "    for i in range(beam_width):\n",
    "        if beam_log_probs[i] > float('-inf'):\n",
    "            length = beam_tokens[i].shape[0]\n",
    "            score = beam_log_probs[i].item() / (length ** length_penalty) if length_penalty > 0 else beam_log_probs[i].item()\n",
    "            finished_beams.append((beam_tokens[i].clone(), score))\n",
    "    \n",
    "    # Sort by score\n",
    "    finished_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return finished_beams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple mock language model for testing\n",
    "class MockLanguageModel:\n",
    "    \"\"\"\n",
    "    A simple mock LM that has deterministic preferences.\n",
    "    Vocabulary: 0='<pad>', 1='<eos>', 2='the', 3='cat', 4='dog', 5='sat', 6='ran'\n",
    "    \n",
    "    Preferences:\n",
    "    - After 'the': prefers 'cat' > 'dog'\n",
    "    - After 'cat': prefers 'sat' > 'ran'\n",
    "    - After 'dog': prefers 'ran' > 'sat'\n",
    "    - After 'sat'/'ran': prefers '<eos>'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 7\n",
    "        self.eos_token_id = 1\n",
    "    \n",
    "    def __call__(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        logits = torch.zeros(batch_size, seq_len, self.vocab_size)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                last_token = tokens[b, t].item()\n",
    "                \n",
    "                # Set logits based on previous token\n",
    "                if last_token == 2:  # 'the'\n",
    "                    logits[b, t, 3] = 2.0  # 'cat'\n",
    "                    logits[b, t, 4] = 1.5  # 'dog'\n",
    "                elif last_token == 3:  # 'cat'\n",
    "                    logits[b, t, 5] = 2.0  # 'sat'\n",
    "                    logits[b, t, 6] = 1.0  # 'ran'\n",
    "                elif last_token == 4:  # 'dog'\n",
    "                    logits[b, t, 6] = 2.0  # 'ran'\n",
    "                    logits[b, t, 5] = 1.0  # 'sat'\n",
    "                elif last_token in [5, 6]:  # 'sat' or 'ran'\n",
    "                    logits[b, t, 1] = 3.0  # '<eos>'\n",
    "                else:\n",
    "                    logits[b, t, 2] = 1.0  # 'the'\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = MockLanguageModel()\n",
    "print(f\"Vocabulary size: {model.vocab_size}\")\n",
    "print(f\"Token mapping: 0='<pad>', 1='<eos>', 2='the', 3='cat', 4='dog', 5='sat', 6='ran'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic beam search\n",
    "print(\"=== Testing Basic Beam Search ===\")\n",
    "\n",
    "prompt = torch.tensor([2])  # Start with 'the'\n",
    "\n",
    "# Run beam search with beam_width=2\n",
    "results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=2,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "token_names = {0: '<pad>', 1: '<eos>', 2: 'the', 3: 'cat', 4: 'dog', 5: 'sat', 6: 'ran'}\n",
    "\n",
    "print(f\"\\nTop {len(results)} beams:\")\n",
    "for i, beam in enumerate(results):\n",
    "    tokens = [token_names[t.item()] for t in beam.tokens]\n",
    "    print(f\"  {i+1}. {' '.join(tokens)} (log_prob={beam.log_prob:.3f}, finished={beam.is_finished})\")\n",
    "\n",
    "# The best sequence should be \"the cat sat <eos>\" \n",
    "# because cat is preferred after 'the', and sat is preferred after 'cat'\n",
    "best_tokens = [token_names[t.item()] for t in results[0].tokens]\n",
    "print(f\"\\nBest sequence: {' '.join(best_tokens)}\")\n",
    "assert 'cat' in best_tokens, \"Best sequence should contain 'cat'\"\n",
    "print(\"Basic beam search test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test greedy vs beam search\n",
    "print(\"\\n=== Greedy vs Beam Search ===\")\n",
    "\n",
    "# Greedy is equivalent to beam_width=1\n",
    "greedy_results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=1,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "beam_results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=4,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "greedy_tokens = [token_names[t.item()] for t in greedy_results[0].tokens]\n",
    "beam_tokens = [token_names[t.item()] for t in beam_results[0].tokens]\n",
    "\n",
    "print(f\"Greedy (beam=1): {' '.join(greedy_tokens)} (score={greedy_results[0].log_prob:.3f})\")\n",
    "print(f\"Beam (beam=4):   {' '.join(beam_tokens)} (score={beam_results[0].log_prob:.3f})\")\n",
    "\n",
    "# In this case they should be the same since 'cat' > 'dog' at every step\n",
    "print(\"Greedy vs Beam comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test efficient beam search\n",
    "print(\"\\n=== Testing Efficient Beam Search ===\")\n",
    "\n",
    "efficient_results = beam_search_efficient(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=4,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    "    length_penalty=0.0,\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {min(4, len(efficient_results))} sequences:\")\n",
    "for i, (tokens, score) in enumerate(efficient_results[:4]):\n",
    "    token_str = [token_names[t.item()] for t in tokens]\n",
    "    print(f\"  {i+1}. {' '.join(token_str)} (score={score:.3f})\")\n",
    "\n",
    "print(\"Efficient beam search test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test length penalty effect\n",
    "print(\"\\n=== Testing Length Penalty ===\")\n",
    "\n",
    "# Without length penalty, longer sequences are penalized\n",
    "results_no_penalty = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=4,\n",
    "    max_length=5,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=None,  # No early stopping\n",
    "    length_penalty=0.0,\n",
    ")\n",
    "\n",
    "# With length penalty, we normalize by length\n",
    "results_with_penalty = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=4,\n",
    "    max_length=5,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=None,\n",
    "    length_penalty=0.6,\n",
    ")\n",
    "\n",
    "print(\"Without length penalty (prefers shorter):\")\n",
    "for beam in results_no_penalty[:2]:\n",
    "    tokens = [token_names[t.item()] for t in beam.tokens]\n",
    "    print(f\"  {' '.join(tokens)} (raw={beam.log_prob:.3f})\")\n",
    "\n",
    "print(\"\\nWith length penalty (fairer comparison):\")\n",
    "for beam in results_with_penalty[:2]:\n",
    "    tokens = [token_names[t.item()] for t in beam.tokens]\n",
    "    print(f\"  {' '.join(tokens)} (normalized={beam.score(0.6):.3f})\")\n",
    "\n",
    "print(\"Length penalty test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate beam width effect\n",
    "print(\"\\n=== Effect of Beam Width ===\")\n",
    "\n",
    "for beam_width in [1, 2, 4, 8]:\n",
    "    results = beam_search(\n",
    "        model_fn=model,\n",
    "        prompt_tokens=prompt,\n",
    "        beam_width=beam_width,\n",
    "        max_length=3,\n",
    "        vocab_size=model.vocab_size,\n",
    "        eos_token_id=model.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    best_tokens = [token_names[t.item()] for t in results[0].tokens]\n",
    "    unique_results = len(set(tuple(b.tokens.tolist()) for b in results))\n",
    "    print(f\"Beam width={beam_width}: Best='{' '.join(best_tokens)}', Unique sequences={unique_results}\")\n",
    "\n",
    "print(\"\\nLarger beam width explores more alternatives but takes more compute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All Beam Search tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Beam Search** maintains top-B candidates at each step\n",
    "   - Trade-off between quality and computation\n",
    "   - Complexity: O(B * V * T) vs O(V^T) for exhaustive\n",
    "\n",
    "2. **Log Probabilities** prevent numerical underflow\n",
    "   - Sum log-probs instead of multiplying probs\n",
    "   - P(seq) = P(t1) * P(t2|t1) * ... becomes log P(t1) + log P(t2|t1) + ...\n",
    "\n",
    "3. **Length Normalization** enables fair comparison\n",
    "   - Without it, shorter sequences always score higher\n",
    "   - Typical alpha: 0.6-0.7\n",
    "\n",
    "4. **Early Stopping** on EOS improves efficiency\n",
    "   - Finished beams don't expand further\n",
    "   - Stop when all beams reach EOS\n",
    "\n",
    "### Beam Width Selection\n",
    "\n",
    "| Beam Width | Use Case | Notes |\n",
    "|------------|----------|-------|\n",
    "| 1 (greedy) | Fast inference, low diversity | May miss better sequences |\n",
    "| 4-5 | Standard translation/generation | Good quality/speed balance |\n",
    "| 10-20 | High-quality translation | Diminishing returns beyond 10 |\n",
    "\n",
    "### Common Variations\n",
    "\n",
    "- **Diverse Beam Search**: Penalize similar beams\n",
    "- **Constrained Beam Search**: Force certain tokens\n",
    "- **Top-k/Top-p + Beam**: Combine sampling with search\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "1. **Why log probabilities?** Numerical stability - probabilities become tiny for long sequences\n",
    "\n",
    "2. **Greedy vs Beam?** Greedy is O(V*T), beam is O(B*V*T) but finds better sequences\n",
    "\n",
    "3. **Optimal beam width?** Diminishing returns past 5-10; depends on task\n",
    "\n",
    "4. **Length normalization?** Without it, beam search prefers short sequences\n",
    "\n",
    "5. **When NOT to use beam search?** Creative tasks where diversity matters (use sampling instead)\n",
    "\n",
    "6. **Time complexity?** O(B * V * T) where B=beam width, V=vocab, T=length\n",
    "\n",
    "7. **Space complexity?** O(B * T) for storing beam tokens\n",
    "\n",
    "## References\n",
    "\n",
    "1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al., 2014\n",
    "2. [Google's Neural Machine Translation System](https://arxiv.org/abs/1609.08144) - Wu et al., 2016 (length penalty)\n",
    "3. [A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/abs/1611.08562) - Diverse beam search\n",
    "4. [HuggingFace - Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
