{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Decoding\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement beam search, a breadth-first search algorithm that maintains the top-k most promising partial sequences at each decoding step.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Why Not Greedy Decoding?\n",
    "\n",
    "**Greedy decoding** selects the highest-probability token at each step:\n",
    "- Fast (O(V) per step)\n",
    "- But can miss globally optimal sequences\n",
    "- \"The cat sat\" might be better than \"The dog...\" even if P(dog|The) > P(cat|The)\n",
    "\n",
    "**Exhaustive search** considers all possible sequences:\n",
    "- Guarantees optimal solution\n",
    "- But exponential: O(V^T) for vocabulary V and length T\n",
    "- Completely infeasible\n",
    "\n",
    "### Beam Search: A Middle Ground\n",
    "\n",
    "**Beam search** maintains B (beam width) best partial sequences:\n",
    "- At each step, expand all B sequences by all V tokens = B*V candidates\n",
    "- Keep only top B by cumulative log probability\n",
    "- Complexity: O(B*V*T) - linear in sequence length!\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Beam Width (B)**: Number of candidates to keep (typically 4-10)\n",
    "2. **Log Probabilities**: Use log-sum to avoid numerical underflow\n",
    "3. **Length Normalization**: Longer sequences have lower log-probs; normalize to compare fairly\n",
    "4. **Early Stopping**: Stop beams that generate EOS token\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "At step $t$, we have $B$ partial sequences $\\{y^{(1)}_{1:t}, ..., y^{(B)}_{1:t}\\}$.\n",
    "\n",
    "For each sequence $y^{(b)}$:\n",
    "1. Compute next-token distribution: $P(y_{t+1} | y^{(b)}_{1:t})$\n",
    "2. Score all extensions: $\\log P(y^{(b)}_{1:t}) + \\log P(y_{t+1} | y^{(b)}_{1:t})$\n",
    "\n",
    "Select top $B$ from all $B \\times V$ candidates based on cumulative score.\n",
    "\n",
    "**Length Normalization**:\n",
    "$$\\text{score}(y) = \\frac{\\log P(y)}{|y|^\\alpha}$$\n",
    "\n",
    "where $\\alpha \\in [0, 1]$ controls normalization strength (typically 0.6-0.7).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why beam search outperforms greedy decoding\n",
    "2. Implement basic beam search with log probabilities\n",
    "3. Add length normalization for fair comparison\n",
    "4. Handle EOS tokens and early stopping\n",
    "5. Know the tradeoffs of different beam widths\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. `beam_search_step()` - One step of beam search expansion\n",
    "2. `beam_search()` - Full beam search decoding\n",
    "3. `beam_search_with_length_norm()` - With length normalization\n",
    "\n",
    "## Hints\n",
    "\n",
    "1. Use `torch.topk()` to get top-B candidates efficiently\n",
    "2. Track both token sequences and cumulative log-probs\n",
    "3. Use `-float('inf')` for terminated beams to exclude them\n",
    "4. Consider returning top-N final sequences, not just top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BeamHypothesis:\n",
    "    \"\"\"A single beam hypothesis (partial or complete sequence).\"\"\"\n",
    "    tokens: torch.Tensor  # Token IDs\n",
    "    log_prob: float       # Cumulative log probability\n",
    "    is_finished: bool     # Whether EOS was generated\n",
    "    \n",
    "    def score(self, length_penalty: float = 1.0) -> float:\n",
    "        \"\"\"Compute length-normalized score.\"\"\"\n",
    "        # TODO: Implement length-normalized scoring\n",
    "        # Return log_prob / (length ** length_penalty)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_step(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    beams: List[BeamHypothesis],\n",
    "    beam_width: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    ") -> List[BeamHypothesis]:\n",
    "    \"\"\"\n",
    "    Perform one step of beam search expansion.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that takes token sequences and returns logits\n",
    "                  Input: (batch_size, seq_len) -> Output: (batch_size, seq_len, vocab_size)\n",
    "        beams: Current list of beam hypotheses\n",
    "        beam_width: Number of beams to keep\n",
    "        vocab_size: Size of vocabulary\n",
    "        eos_token_id: End-of-sequence token ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        List of new beam hypotheses after expansion\n",
    "    \"\"\"\n",
    "    # TODO: Implement beam search step\n",
    "    # 1. Separate finished and active beams\n",
    "    # 2. Stack active beam tokens for batched inference\n",
    "    # 3. Get logits from model and convert to log probs\n",
    "    # 4. For each beam, compute scores for all next tokens\n",
    "    # 5. Sort all candidates and keep top beam_width\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    prompt_tokens: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    max_length: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> List[BeamHypothesis]:\n",
    "    \"\"\"\n",
    "    Perform beam search decoding.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Model function for next-token prediction\n",
    "        prompt_tokens: Initial token sequence (1D tensor)\n",
    "        beam_width: Number of beams to maintain\n",
    "        max_length: Maximum generation length\n",
    "        vocab_size: Vocabulary size\n",
    "        eos_token_id: EOS token ID (optional)\n",
    "        length_penalty: Length normalization factor (0 = no normalization)\n",
    "        \n",
    "    Returns:\n",
    "        List of final beam hypotheses, sorted by score\n",
    "    \"\"\"\n",
    "    # TODO: Implement beam search\n",
    "    # 1. Initialize with single beam containing prompt\n",
    "    # 2. Loop for max_length steps, calling beam_search_step\n",
    "    # 3. Early stop if all beams are finished\n",
    "    # 4. Sort by score (with length penalty) and return\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_efficient(\n",
    "    model_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    prompt_tokens: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    max_length: int,\n",
    "    vocab_size: int,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> List[Tuple[torch.Tensor, float]]:\n",
    "    \"\"\"\n",
    "    Efficient beam search using tensor operations.\n",
    "    Maintains beams as tensors rather than lists.\n",
    "    \n",
    "    Returns:\n",
    "        List of (tokens, score) tuples\n",
    "    \"\"\"\n",
    "    # TODO (BONUS): Implement efficient beam search with tensors\n",
    "    # Key optimizations:\n",
    "    # 1. Use tensor operations instead of Python loops\n",
    "    # 2. Use torch.topk for efficient top-B selection\n",
    "    # 3. Decode indices: beam_idx = idx // vocab_size, token_idx = idx % vocab_size\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple mock language model for testing\n",
    "class MockLanguageModel:\n",
    "    \"\"\"\n",
    "    A simple mock LM that has deterministic preferences.\n",
    "    Vocabulary: 0='<pad>', 1='<eos>', 2='the', 3='cat', 4='dog', 5='sat', 6='ran'\n",
    "    \n",
    "    Preferences:\n",
    "    - After 'the': prefers 'cat' > 'dog'\n",
    "    - After 'cat': prefers 'sat' > 'ran'\n",
    "    - After 'dog': prefers 'ran' > 'sat'\n",
    "    - After 'sat'/'ran': prefers '<eos>'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 7\n",
    "        self.eos_token_id = 1\n",
    "    \n",
    "    def __call__(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        logits = torch.zeros(batch_size, seq_len, self.vocab_size)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                last_token = tokens[b, t].item()\n",
    "                \n",
    "                # Set logits based on previous token\n",
    "                if last_token == 2:  # 'the'\n",
    "                    logits[b, t, 3] = 2.0  # 'cat'\n",
    "                    logits[b, t, 4] = 1.5  # 'dog'\n",
    "                elif last_token == 3:  # 'cat'\n",
    "                    logits[b, t, 5] = 2.0  # 'sat'\n",
    "                    logits[b, t, 6] = 1.0  # 'ran'\n",
    "                elif last_token == 4:  # 'dog'\n",
    "                    logits[b, t, 6] = 2.0  # 'ran'\n",
    "                    logits[b, t, 5] = 1.0  # 'sat'\n",
    "                elif last_token in [5, 6]:  # 'sat' or 'ran'\n",
    "                    logits[b, t, 1] = 3.0  # '<eos>'\n",
    "                else:\n",
    "                    logits[b, t, 2] = 1.0  # 'the'\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = MockLanguageModel()\n",
    "print(f\"Vocabulary size: {model.vocab_size}\")\n",
    "print(f\"Token mapping: 0='<pad>', 1='<eos>', 2='the', 3='cat', 4='dog', 5='sat', 6='ran'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic beam search\n",
    "print(\"=== Testing Basic Beam Search ===\")\n",
    "\n",
    "prompt = torch.tensor([2])  # Start with 'the'\n",
    "\n",
    "# Run beam search with beam_width=2\n",
    "results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=2,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "token_names = {0: '<pad>', 1: '<eos>', 2: 'the', 3: 'cat', 4: 'dog', 5: 'sat', 6: 'ran'}\n",
    "\n",
    "print(f\"\\nTop {len(results)} beams:\")\n",
    "for i, beam in enumerate(results):\n",
    "    tokens = [token_names[t.item()] for t in beam.tokens]\n",
    "    print(f\"  {i+1}. {' '.join(tokens)} (log_prob={beam.log_prob:.3f}, finished={beam.is_finished})\")\n",
    "\n",
    "# The best sequence should be \"the cat sat <eos>\" \n",
    "# because cat is preferred after 'the', and sat is preferred after 'cat'\n",
    "best_tokens = [token_names[t.item()] for t in results[0].tokens]\n",
    "print(f\"\\nBest sequence: {' '.join(best_tokens)}\")\n",
    "assert 'cat' in best_tokens, \"Best sequence should contain 'cat'\"\n",
    "print(\"Basic beam search test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test greedy vs beam search\n",
    "print(\"\\n=== Greedy vs Beam Search ===\")\n",
    "\n",
    "# Greedy is equivalent to beam_width=1\n",
    "greedy_results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=1,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "beam_results = beam_search(\n",
    "    model_fn=model,\n",
    "    prompt_tokens=prompt,\n",
    "    beam_width=4,\n",
    "    max_length=3,\n",
    "    vocab_size=model.vocab_size,\n",
    "    eos_token_id=model.eos_token_id,\n",
    ")\n",
    "\n",
    "greedy_tokens = [token_names[t.item()] for t in greedy_results[0].tokens]\n",
    "beam_tokens = [token_names[t.item()] for t in beam_results[0].tokens]\n",
    "\n",
    "print(f\"Greedy (beam=1): {' '.join(greedy_tokens)} (score={greedy_results[0].log_prob:.3f})\")\n",
    "print(f\"Beam (beam=4):   {' '.join(beam_tokens)} (score={beam_results[0].log_prob:.3f})\")\n",
    "\n",
    "# In this case they should be the same since 'cat' > 'dog' at every step\n",
    "print(\"Greedy vs Beam comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate beam width effect\n",
    "print(\"\\n=== Effect of Beam Width ===\")\n",
    "\n",
    "for beam_width in [1, 2, 4, 8]:\n",
    "    results = beam_search(\n",
    "        model_fn=model,\n",
    "        prompt_tokens=prompt,\n",
    "        beam_width=beam_width,\n",
    "        max_length=3,\n",
    "        vocab_size=model.vocab_size,\n",
    "        eos_token_id=model.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    best_tokens = [token_names[t.item()] for t in results[0].tokens]\n",
    "    unique_results = len(set(tuple(b.tokens.tolist()) for b in results))\n",
    "    print(f\"Beam width={beam_width}: Best='{' '.join(best_tokens)}', Unique sequences={unique_results}\")\n",
    "\n",
    "print(\"\\nLarger beam width explores more alternatives but takes more compute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All Beam Search tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Beam Search** maintains top-B candidates at each step\n",
    "   - Trade-off between quality and computation\n",
    "   - Complexity: O(B * V * T) vs O(V^T) for exhaustive\n",
    "\n",
    "2. **Log Probabilities** prevent numerical underflow\n",
    "   - Sum log-probs instead of multiplying probs\n",
    "   - P(seq) = P(t1) * P(t2|t1) * ... becomes log P(t1) + log P(t2|t1) + ...\n",
    "\n",
    "3. **Length Normalization** enables fair comparison\n",
    "   - Without it, shorter sequences always score higher\n",
    "   - Typical alpha: 0.6-0.7\n",
    "\n",
    "4. **Early Stopping** on EOS improves efficiency\n",
    "   - Finished beams don't expand further\n",
    "   - Stop when all beams reach EOS\n",
    "\n",
    "### Beam Width Selection\n",
    "\n",
    "| Beam Width | Use Case | Notes |\n",
    "|------------|----------|-------|\n",
    "| 1 (greedy) | Fast inference, low diversity | May miss better sequences |\n",
    "| 4-5 | Standard translation/generation | Good quality/speed balance |\n",
    "| 10-20 | High-quality translation | Diminishing returns beyond 10 |\n",
    "\n",
    "### Common Variations\n",
    "\n",
    "- **Diverse Beam Search**: Penalize similar beams\n",
    "- **Constrained Beam Search**: Force certain tokens\n",
    "- **Top-k/Top-p + Beam**: Combine sampling with search\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "1. **Why log probabilities?** Numerical stability - probabilities become tiny for long sequences\n",
    "\n",
    "2. **Greedy vs Beam?** Greedy is O(V*T), beam is O(B*V*T) but finds better sequences\n",
    "\n",
    "3. **Optimal beam width?** Diminishing returns past 5-10; depends on task\n",
    "\n",
    "4. **Length normalization?** Without it, beam search prefers short sequences\n",
    "\n",
    "5. **When NOT to use beam search?** Creative tasks where diversity matters (use sampling instead)\n",
    "\n",
    "6. **Time complexity?** O(B * V * T) where B=beam width, V=vocab, T=length\n",
    "\n",
    "7. **Space complexity?** O(B * T) for storing beam tokens\n",
    "\n",
    "## References\n",
    "\n",
    "1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) - Sutskever et al., 2014\n",
    "2. [Google's Neural Machine Translation System](https://arxiv.org/abs/1609.08144) - Wu et al., 2016 (length penalty)\n",
    "3. [A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/abs/1611.08562) - Diverse beam search\n",
    "4. [HuggingFace - Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
