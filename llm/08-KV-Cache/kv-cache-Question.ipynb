{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Implement KV Cache for Efficient Autoregressive Generation\n",
    "\n",
    "### Background\n",
    "\n",
    "In autoregressive models (like GPT), we generate tokens one at a time. At each step, we compute attention over **all previous tokens**. Without optimization, this means:\n",
    "- At step 1: Compute K, V for token 0\n",
    "- At step 2: Compute K, V for tokens 0, 1 (recomputing token 0!)\n",
    "- At step 3: Compute K, V for tokens 0, 1, 2 (recomputing tokens 0, 1!)\n",
    "\n",
    "**KV Cache** solves this by storing previously computed K and V tensors:\n",
    "- At step 1: Compute K‚ÇÄ, V‚ÇÄ, cache them\n",
    "- At step 2: Compute K‚ÇÅ, V‚ÇÅ, concatenate with cached [K‚ÇÄ, V‚ÇÄ]\n",
    "- At step 3: Compute K‚ÇÇ, V‚ÇÇ, concatenate with cached [K‚ÇÄ, K‚ÇÅ], [V‚ÇÄ, V‚ÇÅ]\n",
    "\n",
    "This reduces computation from O(n¬≤) to O(n) for generating n tokens!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Without cache:\n",
    "```\n",
    "Q_t = W_q @ X[0:t+1]  # Query for all tokens up to t\n",
    "K_t = W_k @ X[0:t+1]  # Recompute keys for all tokens\n",
    "V_t = W_v @ X[0:t+1]  # Recompute values for all tokens\n",
    "```\n",
    "\n",
    "With cache:\n",
    "```\n",
    "Q_t = W_q @ X[t]           # Query only for new token\n",
    "K_new = W_k @ X[t]         # Key only for new token\n",
    "V_new = W_v @ X[t]         # Value only for new token\n",
    "K_cached = concat(K_cache, K_new)  # Append to cache\n",
    "V_cached = concat(V_cache, V_new)  # Append to cache\n",
    "```\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Implement attention with KV caching\n",
    "2. Understand cache management (initialization, updates)\n",
    "3. Measure performance improvements\n",
    "4. Handle edge cases (first token, cache limits)\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 1: Cache Initialization</summary>\n",
    "  For the first token, initialize the cache with the computed K and V. For subsequent tokens, concatenate new K,V with cached values along the sequence dimension.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 2: Concatenation</summary>\n",
    "  Use `torch.cat([cached, new], dim=1)` where dim=1 is the sequence length dimension.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 3: Multi-Head</summary>\n",
    "  For multi-head attention, cache shape should be [batch, num_heads, seq_len, d_k]. Concatenate along dim=2 (sequence dimension).\n",
    "</details>\n",
    "\n",
    "### References\n",
    "- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- [FlashAttention paper](https://arxiv.org/abs/2205.14135) - discusses memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Baseline Attention (No Cache)\n",
    "\n",
    "First, let's implement standard scaled dot-product attention without any caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_no_cache(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention without caching.\n",
    "    \n",
    "    Args:\n",
    "        q: Query [batch, seq_len_q, d_k]\n",
    "        k: Key [batch, seq_len_k, d_k]\n",
    "        v: Value [batch, seq_len_k, d_v]\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch, seq_len_q, d_v]\n",
    "    \"\"\"\n",
    "    # TODO: Compute scaled dot-product attention\n",
    "    # Hint: scores = Q @ K^T / sqrt(d_k)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Attention with KV Cache\n",
    "\n",
    "Now implement attention that maintains a cache of Key and Value tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Cache for storing Key and Value tensors during autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.k_cache = None  # [batch, seq_len, d_k]\n",
    "        self.v_cache = None  # [batch, seq_len, d_v]\n",
    "    \n",
    "    def update(self, k_new, v_new):\n",
    "        \"\"\"\n",
    "        Add new key and value tensors to the cache.\n",
    "        \n",
    "        Args:\n",
    "            k_new: New keys [batch, 1, d_k]\n",
    "            v_new: New values [batch, 1, d_v]\n",
    "        \n",
    "        Returns:\n",
    "            k_cached: All keys including new [batch, seq_len, d_k]\n",
    "            v_cached: All values including new [batch, seq_len, d_v]\n",
    "        \"\"\"\n",
    "        # TODO: Implement cache update logic\n",
    "        # If cache is None (first token): initialize with k_new, v_new\n",
    "        # Otherwise: concatenate k_new, v_new with cached values\n",
    "        # Hint: torch.cat([self.k_cache, k_new], dim=1)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Reset the cache.\"\"\"\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "\n",
    "def attention_with_cache(\n",
    "    q, k_new, v_new, \n",
    "    cache: Optional[KVCache] = None,\n",
    "    mask=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with KV caching.\n",
    "    \n",
    "    Args:\n",
    "        q: Query for current token [batch, 1, d_k]\n",
    "        k_new: Key for current token [batch, 1, d_k]\n",
    "        v_new: Value for current token [batch, 1, d_v]\n",
    "        cache: KVCache object to store/retrieve cached K,V\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output [batch, 1, d_v]\n",
    "    \"\"\"\n",
    "    # TODO: Implement attention with KV cache\n",
    "    # 1. Update cache with new K, V\n",
    "    # 2. Compute attention using query and ALL cached keys/values\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test Correctness\n",
    "\n",
    "Verify that cached attention produces the same results as non-cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test parameters\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "seq_len = 5\n",
    "\n",
    "# Simulate autoregressive generation\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Projection matrices (shared for both methods)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"Testing correctness of KV cache implementation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Without cache (recompute everything each step)\n",
    "outputs_no_cache = []\n",
    "for t in range(seq_len):\n",
    "    x_current = X[:, :t+1, :]\n",
    "    q = W_q(x_current[:, -1:, :])\n",
    "    k = W_k(x_current)\n",
    "    v = W_v(x_current)\n",
    "    output = attention_no_cache(q, k, v)\n",
    "    outputs_no_cache.append(output)\n",
    "\n",
    "# Method 2: With cache (only compute new K, V each step)\n",
    "cache = KVCache()\n",
    "outputs_with_cache = []\n",
    "for t in range(seq_len):\n",
    "    x_token = X[:, t:t+1, :]\n",
    "    q = W_q(x_token)\n",
    "    k = W_k(x_token)\n",
    "    v = W_v(x_token)\n",
    "    output = attention_with_cache(q, k, v, cache=cache)\n",
    "    outputs_with_cache.append(output)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"\\nComparing outputs at each timestep:\\n\")\n",
    "all_match = True\n",
    "for t in range(seq_len):\n",
    "    match = torch.allclose(outputs_no_cache[t], outputs_with_cache[t], atol=1e-6, rtol=1e-5)\n",
    "    status = \"‚úì\" if match else \"‚úó\"\n",
    "    print(f\"  Step {t}: {status} {'Match' if match else 'Mismatch'}\")\n",
    "    if not match:\n",
    "        all_match = False\n",
    "\n",
    "print()\n",
    "if all_match:\n",
    "    print(\"‚úì All outputs match! KV cache is working correctly.\")\n",
    "else:\n",
    "    print(\"‚úó Outputs don't match. Check your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Measure Performance Improvement\n",
    "\n",
    "Compare the computational cost of cached vs non-cached attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with longer sequences\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "d_model = 512\n",
    "seq_len = 100\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sequence length: {seq_len} tokens\\n\")\n",
    "\n",
    "# Benchmark without cache\n",
    "start = time.time()\n",
    "for t in range(seq_len):\n",
    "    x_current = X[:, :t+1, :]\n",
    "    q = W_q(x_current[:, -1:, :])\n",
    "    k = W_k(x_current)\n",
    "    v = W_v(x_current)\n",
    "    _ = attention_no_cache(q, k, v)\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "# Benchmark with cache\n",
    "cache = KVCache()\n",
    "start = time.time()\n",
    "for t in range(seq_len):\n",
    "    x_token = X[:, t:t+1, :]\n",
    "    q = W_q(x_token)\n",
    "    k = W_k(x_token)\n",
    "    v = W_v(x_token)\n",
    "    _ = attention_with_cache(q, k, v, cache=cache)\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "speedup = time_no_cache / time_with_cache\n",
    "\n",
    "print(f\"Time without cache: {time_no_cache:.4f}s\")\n",
    "print(f\"Time with cache:    {time_with_cache:.4f}s\")\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x faster\")\n",
    "print(f\"Time saved: {(time_no_cache - time_with_cache):.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Multi-Head Attention with KV Cache (Bonus)\n",
    "\n",
    "Extend KV cache to work with multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadKVCache:\n",
    "    \"\"\"\n",
    "    KV Cache for multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.k_cache = None  # [batch, num_heads, seq_len, d_k]\n",
    "        self.v_cache = None  # [batch, num_heads, seq_len, d_v]\n",
    "    \n",
    "    def update(self, k_new, v_new):\n",
    "        \"\"\"\n",
    "        Update cache with new multi-head K, V tensors.\n",
    "        \n",
    "        Args:\n",
    "            k_new: [batch, num_heads, 1, d_k]\n",
    "            v_new: [batch, num_heads, 1, d_v]\n",
    "        \"\"\"\n",
    "        # TODO: Implement multi-head cache update\n",
    "        # Hint: Concatenate along dim=2 (sequence length dimension)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def clear(self):\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "\n",
    "def multi_head_attention_with_cache(\n",
    "    q, k_new, v_new,\n",
    "    num_heads,\n",
    "    cache: Optional[MultiHeadKVCache] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-head attention with KV caching.\n",
    "    \"\"\"\n",
    "    # TODO: Implement multi-head attention with cache\n",
    "    # 1. Reshape Q, K, V to [batch, num_heads, seq_len, d_k]\n",
    "    # 2. Update cache\n",
    "    # 3. Compute attention\n",
    "    # 4. Concatenate heads back\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **KV Cache**: Store K,V from previous tokens to avoid recomputation\n",
    "- **Performance**: O(n¬≤) ‚Üí O(n) complexity reduction\n",
    "- **Trade-off**: Uses O(n) memory but saves massive computation\n",
    "- **Production Critical**: All modern LLMs use KV caching for inference\n",
    "\n",
    "### Interview Tips\n",
    "\n",
    "Be ready to answer:\n",
    "- Why is KV cache needed? (Avoid recomputing K,V)\n",
    "- What's the complexity improvement? (O(n¬≤) ‚Üí O(n))\n",
    "- What's the memory cost? (O(batch √ó seq_len √ó d_model))\n",
    "- How does it work with beam search? (Separate cache per beam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
