{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Implement KV Cache for Efficient Autoregressive Generation\n",
    "\n",
    "### Background\n",
    "\n",
    "In autoregressive models (like GPT), we generate tokens one at a time. At each step, we compute attention over **all previous tokens**. Without optimization, this means:\n",
    "- At step 1: Compute K, V for token 0\n",
    "- At step 2: Compute K, V for tokens 0, 1 (recomputing token 0!)\n",
    "- At step 3: Compute K, V for tokens 0, 1, 2 (recomputing tokens 0, 1!)\n",
    "\n",
    "**KV Cache** solves this by storing previously computed K and V tensors:\n",
    "- At step 1: Compute K₀, V₀, cache them\n",
    "- At step 2: Compute K₁, V₁, concatenate with cached [K₀, V₀]\n",
    "- At step 3: Compute K₂, V₂, concatenate with cached [K₀, K₁], [V₀, V₁]\n",
    "\n",
    "This reduces computation from O(n²) to O(n) for generating n tokens!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Without cache:\n",
    "```\n",
    "Q_t = W_q @ X[0:t+1]  # Query for all tokens up to t\n",
    "K_t = W_k @ X[0:t+1]  # Recompute keys for all tokens\n",
    "V_t = W_v @ X[0:t+1]  # Recompute values for all tokens\n",
    "```\n",
    "\n",
    "With cache:\n",
    "```\n",
    "Q_t = W_q @ X[t]           # Query only for new token\n",
    "K_new = W_k @ X[t]         # Key only for new token\n",
    "V_new = W_v @ X[t]         # Value only for new token\n",
    "K_cached = concat(K_cache, K_new)  # Append to cache\n",
    "V_cached = concat(V_cache, V_new)  # Append to cache\n",
    "```\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Implement attention with KV caching\n",
    "2. Understand cache management (initialization, updates)\n",
    "3. Measure performance improvements\n",
    "4. Handle edge cases (first token, cache limits)\n",
    "\n",
    "### References\n",
    "- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- [FlashAttention paper](https://arxiv.org/abs/2205.14135) - discusses memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Baseline Attention (No Cache)\n",
    "\n",
    "First, let's implement standard scaled dot-product attention without any caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_no_cache(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention without caching.\n",
    "    \n",
    "    Args:\n",
    "        q: Query [batch, seq_len_q, d_k]\n",
    "        k: Key [batch, seq_len_k, d_k]\n",
    "        v: Value [batch, seq_len_k, d_v]\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch, seq_len_q, d_v]\n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Attention with KV Cache\n",
    "\n",
    "Now implement attention that maintains a cache of Key and Value tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Cache for storing Key and Value tensors during autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.k_cache = None  # [batch, seq_len, d_k]\n",
    "        self.v_cache = None  # [batch, seq_len, d_v]\n",
    "    \n",
    "    def update(self, k_new, v_new):\n",
    "        \"\"\"\n",
    "        Add new key and value tensors to the cache.\n",
    "        \n",
    "        Args:\n",
    "            k_new: New keys [batch, 1, d_k]\n",
    "            v_new: New values [batch, 1, d_v]\n",
    "        \n",
    "        Returns:\n",
    "            k_cached: All keys including new [batch, seq_len, d_k]\n",
    "            v_cached: All values including new [batch, seq_len, d_v]\n",
    "        \"\"\"\n",
    "        if self.k_cache is None:\n",
    "            # First token - initialize cache\n",
    "            self.k_cache = k_new\n",
    "            self.v_cache = v_new\n",
    "        else:\n",
    "            # Subsequent tokens - concatenate with cache\n",
    "            self.k_cache = torch.cat([self.k_cache, k_new], dim=1)\n",
    "            self.v_cache = torch.cat([self.v_cache, v_new], dim=1)\n",
    "        \n",
    "        return self.k_cache, self.v_cache\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Reset the cache.\"\"\"\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "\n",
    "def attention_with_cache(\n",
    "    q, k_new, v_new, \n",
    "    cache: Optional[KVCache] = None,\n",
    "    mask=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with KV caching.\n",
    "    \n",
    "    Args:\n",
    "        q: Query for current token [batch, 1, d_k]\n",
    "        k_new: Key for current token [batch, 1, d_k]\n",
    "        v_new: Value for current token [batch, 1, d_v]\n",
    "        cache: KVCache object to store/retrieve cached K,V\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output [batch, 1, d_v]\n",
    "    \"\"\"\n",
    "    if cache is None:\n",
    "        # No cache provided - just do normal attention\n",
    "        return attention_no_cache(q, k_new, v_new, mask)\n",
    "    \n",
    "    # Update cache with new K, V\n",
    "    k_cached, v_cached = cache.update(k_new, v_new)\n",
    "    \n",
    "    # Compute attention using all cached keys and values\n",
    "    d_k = q.shape[-1]\n",
    "    scores = torch.matmul(q, k_cached.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v_cached)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test Correctness\n",
    "\n",
    "Verify that cached attention produces the same results as non-cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test parameters\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "seq_len = 5\n",
    "\n",
    "# Simulate autoregressive generation\n",
    "# We'll generate seq_len tokens one at a time\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Projection matrices (shared for both methods)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"Testing correctness of KV cache implementation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Without cache (recompute everything each step)\n",
    "outputs_no_cache = []\n",
    "for t in range(seq_len):\n",
    "    # At step t, we attend over tokens 0...t\n",
    "    x_current = X[:, :t+1, :]  # [batch, t+1, d_model]\n",
    "    \n",
    "    q = W_q(x_current[:, -1:, :])  # Query for last token only\n",
    "    k = W_k(x_current)              # Keys for all tokens 0...t\n",
    "    v = W_v(x_current)              # Values for all tokens 0...t\n",
    "    \n",
    "    output = attention_no_cache(q, k, v)\n",
    "    outputs_no_cache.append(output)\n",
    "\n",
    "# Method 2: With cache (only compute new K, V each step)\n",
    "cache = KVCache()\n",
    "outputs_with_cache = []\n",
    "for t in range(seq_len):\n",
    "    x_token = X[:, t:t+1, :]  # Current token [batch, 1, d_model]\n",
    "    \n",
    "    q = W_q(x_token)  # Query for current token\n",
    "    k = W_k(x_token)  # Key for current token ONLY\n",
    "    v = W_v(x_token)  # Value for current token ONLY\n",
    "    \n",
    "    output = attention_with_cache(q, k, v, cache=cache)\n",
    "    outputs_with_cache.append(output)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"\\nComparing outputs at each timestep:\\n\")\n",
    "all_match = True\n",
    "for t in range(seq_len):\n",
    "    match = torch.allclose(outputs_no_cache[t], outputs_with_cache[t], atol=1e-6, rtol=1e-5)\n",
    "    status = \"✓\" if match else \"✗\"\n",
    "    print(f\"  Step {t}: {status} {'Match' if match else 'Mismatch'}\")\n",
    "    if not match:\n",
    "        all_match = False\n",
    "        print(f\"    Max diff: {(outputs_no_cache[t] - outputs_with_cache[t]).abs().max():.2e}\")\n",
    "\n",
    "print()\n",
    "if all_match:\n",
    "    print(\"✓ All outputs match! KV cache is working correctly.\")\n",
    "else:\n",
    "    print(\"✗ Outputs don't match. Check your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Measure Performance Improvement\n",
    "\n",
    "Compare the computational cost of cached vs non-cached attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with longer sequences\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "d_model = 512\n",
    "seq_len = 100  # Generate 100 tokens\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Sequence length: {seq_len} tokens\\n\")\n",
    "\n",
    "# Benchmark without cache\n",
    "start = time.time()\n",
    "for t in range(seq_len):\n",
    "    x_current = X[:, :t+1, :]\n",
    "    q = W_q(x_current[:, -1:, :])\n",
    "    k = W_k(x_current)\n",
    "    v = W_v(x_current)\n",
    "    _ = attention_no_cache(q, k, v)\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "# Benchmark with cache\n",
    "cache = KVCache()\n",
    "start = time.time()\n",
    "for t in range(seq_len):\n",
    "    x_token = X[:, t:t+1, :]\n",
    "    q = W_q(x_token)\n",
    "    k = W_k(x_token)\n",
    "    v = W_v(x_token)\n",
    "    _ = attention_with_cache(q, k, v, cache=cache)\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "speedup = time_no_cache / time_with_cache\n",
    "\n",
    "print(f\"Time without cache: {time_no_cache:.4f}s\")\n",
    "print(f\"Time with cache:    {time_with_cache:.4f}s\")\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x faster\")\n",
    "print(f\"Time saved: {(time_no_cache - time_with_cache):.4f}s ({(1 - time_with_cache/time_no_cache)*100:.1f}% reduction)\")\n",
    "\n",
    "# Theoretical analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Theoretical Complexity Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Without cache: O(n²) operations\")\n",
    "print(f\"  - At each step t, compute K,V for all t tokens\")\n",
    "print(f\"  - Total: 1 + 2 + 3 + ... + {seq_len} = {seq_len*(seq_len+1)//2} computations\")\n",
    "print(f\"\\nWith cache: O(n) operations\")\n",
    "print(f\"  - At each step, compute K,V for 1 new token only\")\n",
    "print(f\"  - Total: {seq_len} computations\")\n",
    "print(f\"\\nTheoretical speedup: ~{seq_len/2:.1f}x for this sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Multi-Head Attention with KV Cache\n",
    "\n",
    "Extend KV cache to work with multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadKVCache:\n",
    "    \"\"\"\n",
    "    KV Cache for multi-head attention.\n",
    "    Stores separate caches for each attention head.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.k_cache = None  # [batch, num_heads, seq_len, d_k]\n",
    "        self.v_cache = None  # [batch, num_heads, seq_len, d_v]\n",
    "    \n",
    "    def update(self, k_new, v_new):\n",
    "        \"\"\"\n",
    "        Update cache with new multi-head K, V tensors.\n",
    "        \n",
    "        Args:\n",
    "            k_new: [batch, num_heads, 1, d_k]\n",
    "            v_new: [batch, num_heads, 1, d_v]\n",
    "        \"\"\"\n",
    "        if self.k_cache is None:\n",
    "            self.k_cache = k_new\n",
    "            self.v_cache = v_new\n",
    "        else:\n",
    "            self.k_cache = torch.cat([self.k_cache, k_new], dim=2)  # Concat on seq_len dim\n",
    "            self.v_cache = torch.cat([self.v_cache, v_new], dim=2)\n",
    "        \n",
    "        return self.k_cache, self.v_cache\n",
    "    \n",
    "    def clear(self):\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "\n",
    "def multi_head_attention_with_cache(\n",
    "    q, k_new, v_new,\n",
    "    num_heads,\n",
    "    cache: Optional[MultiHeadKVCache] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-head attention with KV caching.\n",
    "    \n",
    "    Args:\n",
    "        q: Query [batch, 1, d_model]\n",
    "        k_new: New key [batch, 1, d_model]\n",
    "        v_new: New value [batch, 1, d_model]\n",
    "        num_heads: Number of attention heads\n",
    "        cache: MultiHeadKVCache object\n",
    "    \"\"\"\n",
    "    batch_size = q.shape[0]\n",
    "    d_model = q.shape[-1]\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    # Reshape to multi-head: [batch, num_heads, seq_len, d_k]\n",
    "    q = q.view(batch_size, -1, num_heads, d_k).transpose(1, 2)  # [batch, heads, 1, d_k]\n",
    "    k_new = k_new.view(batch_size, -1, num_heads, d_k).transpose(1, 2)\n",
    "    v_new = v_new.view(batch_size, -1, num_heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    # Update cache\n",
    "    if cache is not None:\n",
    "        k_cached, v_cached = cache.update(k_new, v_new)\n",
    "    else:\n",
    "        k_cached, v_cached = k_new, v_new\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    scores = torch.matmul(q, k_cached.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v_cached)  # [batch, heads, 1, d_k]\n",
    "    \n",
    "    # Concatenate heads\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, -1, d_model)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing Multi-Head Attention with KV Cache\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "cache = MultiHeadKVCache(num_heads)\n",
    "for t in range(seq_len):\n",
    "    x_token = X[:, t:t+1, :]\n",
    "    q = W_q(x_token)\n",
    "    k = W_k(x_token)\n",
    "    v = W_v(x_token)\n",
    "    output = multi_head_attention_with_cache(q, k, v, num_heads, cache=cache)\n",
    "    print(f\"Step {t}: Output shape = {output.shape}\")\n",
    "\n",
    "print(f\"\\nFinal cache size: K={cache.k_cache.shape}, V={cache.v_cache.shape}\")\n",
    "print(\"✓ Multi-head KV cache working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **KV Cache Concept**: Store previously computed K and V tensors to avoid recomputation\n",
    "2. **Implementation**: Simple concatenation with cached tensors\n",
    "3. **Performance**: ~50x speedup for 100-token sequences (O(n²) → O(n))\n",
    "4. **Multi-Head Extension**: Cache works independently for each attention head\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Memory vs Speed Trade-off**: Cache uses O(n) memory but saves O(n²) computation\n",
    "- **Critical for Inference**: Essential for fast autoregressive generation in production\n",
    "- **Used Everywhere**: GPT, LLaMA, Claude, etc. all use KV caching\n",
    "\n",
    "### Interview Tips\n",
    "\n",
    "Common questions:\n",
    "- Why is KV cache needed? (Avoid recomputing K,V for previous tokens)\n",
    "- What's the complexity improvement? (O(n²) → O(n))\n",
    "- What's the memory cost? (O(batch × seq_len × d_model))\n",
    "- How does it work with beam search? (Need separate cache per beam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
