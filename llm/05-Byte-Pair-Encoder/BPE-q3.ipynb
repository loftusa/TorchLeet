{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî§ Implement Byte Pair Encoding from Scratch\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Implement **Byte Pair Encoding (BPE)**, the subword tokenization algorithm used in GPT-2, RoBERTa, and many modern NLP models. BPE learns a vocabulary by iteratively merging the most frequent character pairs, creating an efficient subword representation.\n",
    "\n",
    "By the end of this notebook, you'll understand how to build a tokenizer that handles unknown words gracefully while keeping vocabulary size manageable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: The Tokenization Problem\n",
    "\n",
    "**Why do we need tokenization?**\n",
    "\n",
    "Neural networks don't understand text directly - they need numbers. Tokenization converts text into discrete units (tokens) that can be mapped to numbers.\n",
    "\n",
    "**Three main approaches:**\n",
    "\n",
    "1. **Character-level tokenization**\n",
    "   - Split: `\"hello\"` ‚Üí `['h', 'e', 'l', 'l', 'o']`\n",
    "   - ‚úÖ Pros: Tiny vocabulary (~100 chars), handles any word\n",
    "   - ‚ùå Cons: Long sequences, loses semantic info (\"un\" + \"happy\" = related to \"happy\")\n",
    "\n",
    "2. **Word-level tokenization**\n",
    "   - Split: `\"hello world\"` ‚Üí `['hello', 'world']`\n",
    "   - ‚úÖ Pros: Preserves word meanings\n",
    "   - ‚ùå Cons: Huge vocabulary (~100K+ words), can't handle unknown words (\"asdfghjkl\" ‚Üí ???)\n",
    "\n",
    "3. **Subword tokenization (BPE, WordPiece, SentencePiece)**\n",
    "   - Split: `\"unhappiness\"` ‚Üí `['un', 'happiness']` or `['un', 'happi', 'ness']`\n",
    "   - ‚úÖ Pros: Medium vocabulary (~32K-50K), handles unknown words by breaking into pieces\n",
    "   - ‚úÖ Example: Unknown word \"Transformerization\" ‚Üí `['Transform', 'er', 'ization']`\n",
    "\n",
    "**BPE is the sweet spot!** Used in GPT-2, GPT-3, RoBERTa, BART, and more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How BPE Works: Step-by-Step Example\n",
    "\n",
    "Let's walk through BPE on a tiny corpus: `[\"low\", \"lower\", \"newest\", \"widest\"]`\n",
    "\n",
    "#### Step 0: Initialization\n",
    "Split each word into characters + end-of-word marker `</w>`:\n",
    "```\n",
    "low    ‚Üí ('l', 'o', 'w', '</w>')\n",
    "lower  ‚Üí ('l', 'o', 'w', 'e', 'r', '</w>')\n",
    "newest ‚Üí ('n', 'e', 'w', 'e', 's', 't', '</w>')\n",
    "widest ‚Üí ('w', 'i', 'd', 'e', 's', 't', '</w>')\n",
    "```\n",
    "\n",
    "**Why `</w>`?** To distinguish word endings. Otherwise \"er\" in \"lower\" vs \"er\" as a standalone word would be ambiguous.\n",
    "\n",
    "#### Step 1: Count all adjacent pairs\n",
    "```\n",
    "('e', 's'): 2  ‚Üê appears in \"newest\" and \"widest\"\n",
    "('e', 'r'): 1  ‚Üê appears in \"lower\"\n",
    "('s', 't'): 2  ‚Üê appears in \"newest\" and \"widest\"\n",
    "...\n",
    "```\n",
    "\n",
    "#### Step 2: Merge the most frequent pair\n",
    "Most frequent: `('e', 's')` with count 2\n",
    "```\n",
    "Before: ('n', 'e', 'w', 'e', 's', 't', '</w>')\n",
    "After:  ('n', 'e', 'w', 'es', 't', '</w>')     ‚Üê 'e' and 's' merged into 'es'\n",
    "```\n",
    "\n",
    "#### Step 3: Repeat\n",
    "Count pairs again, merge most frequent, repeat for N merges.\n",
    "\n",
    "Each merge creates a new subword token. After 10 merges, you might have tokens like:\n",
    "```\n",
    "['l', 'o', 'w', 'e', 'r', 's', 't', 'i', 'd', 'n', 'es', 'est', 'est</w>', ...]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Pseudocode\n",
    "\n",
    "```python\n",
    "def byte_pair_encoding(corpus, num_merges):\n",
    "    # 1. Initialize: split words into characters\n",
    "    vocab = {word ‚Üí (char_tuple, frequency)}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        # 2. Count all adjacent pairs across all words\n",
    "        pairs = count_pairs(vocab)\n",
    "        \n",
    "        if no pairs:\n",
    "            break\n",
    "        \n",
    "        # 3. Find most frequent pair\n",
    "        best_pair = max(pairs, key=frequency)\n",
    "        \n",
    "        # 4. Merge that pair in all words\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        \n",
    "        # 5. Record this merge operation\n",
    "        merges.append(best_pair)\n",
    "    \n",
    "    return vocab, merges\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "1. ‚úÖ Understand why subword tokenization is superior to character or word-level\n",
    "2. ‚úÖ Implement the core BPE algorithm from scratch\n",
    "3. ‚úÖ Learn to manage vocabulary as frequency-weighted tuples\n",
    "4. ‚úÖ Handle the end-of-word marker convention correctly\n",
    "5. ‚úÖ Understand how merge operations create a learned vocabulary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Implement the following 4 functions:\n",
    "\n",
    "1. **`get_vocab(corpus)`** - Initialize vocabulary from a list of words\n",
    "   - Input: `[\"low\", \"lower\"]`\n",
    "   - Output: `{('l','o','w','</w>'): 1, ('l','o','w','e','r','</w>'): 1}`\n",
    "\n",
    "2. **`get_stats(vocab)`** - Count frequency of all adjacent character pairs\n",
    "   - Input: `{('l','o','w','</w>'): 1}`\n",
    "   - Output: `{('l','o'): 1, ('o','w'): 1, ('w','</w>'): 1}`\n",
    "\n",
    "3. **`merge_vocab(pair, vocab)`** - Merge a specific pair across all words\n",
    "   - Input: `pair=('o','w')`, `vocab={('l','o','w','</w>'): 1}`\n",
    "   - Output: `{('l','ow','</w>'): 1}`\n",
    "\n",
    "4. **`byte_pair_encoding(corpus, num_merges)`** - Main BPE algorithm\n",
    "   - Orchestrates the above functions to perform N merge operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 1: Vocabulary Structure</summary>\n",
    "  \n",
    "  The vocabulary is a dictionary mapping **tuples** of characters to their **frequency**:\n",
    "  ```python\n",
    "  vocab = {\n",
    "      ('l', 'o', 'w', '</w>'): 1,      # \"low\" appears once\n",
    "      ('l', 'o', 'w', 'e', 'r', '</w>'): 1  # \"lower\" appears once\n",
    "  }\n",
    "  ```\n",
    "  \n",
    "  Use `Counter()` to count word frequencies, then convert each word to a tuple of characters.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 2: Counting Pairs</summary>\n",
    "  \n",
    "  To count adjacent pairs in a word tuple `('l', 'o', 'w', '</w>')`, iterate with a sliding window:\n",
    "  ```python\n",
    "  for i in range(len(word) - 1):\n",
    "      pair = (word[i], word[i+1])  # ('l','o'), ('o','w'), ('w','</w>')\n",
    "      pairs[pair] += freq  # Weight by word frequency!\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 3: Merging Pairs</summary>\n",
    "  \n",
    "  To merge `('o', 'w')` in `('l', 'o', 'w', '</w>')` ‚Üí `('l', 'ow', '</w>')`:\n",
    "  \n",
    "  1. Convert tuple to space-separated string: `\"l o w </w>\"`\n",
    "  2. Replace bigram: `\"l o w </w>\".replace(\"o w\", \"ow\")` ‚Üí `\"l ow </w>\"`\n",
    "  3. Split back to tuple: `('l', 'ow', '</w>')`\n",
    "  \n",
    "  This handles all occurrences of the pair in a single operation!\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Hint 4: Main Loop</summary>\n",
    "  \n",
    "  The main BPE loop:\n",
    "  ```python\n",
    "  for iteration in range(num_merges):\n",
    "      pairs = get_stats(vocab)\n",
    "      if not pairs:  # No more pairs to merge\n",
    "          break\n",
    "      best = max(pairs, key=pairs.get)  # Most frequent pair\n",
    "      vocab = merge_vocab(best, vocab)\n",
    "      merges.append(best)\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.404706Z",
     "iopub.status.busy": "2026-01-10T02:14:15.404556Z",
     "iopub.status.idle": "2026-01-10T02:14:15.407031Z",
     "shell.execute_reply": "2026-01-10T02:14:15.406543Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.408516Z",
     "iopub.status.busy": "2026-01-10T02:14:15.408377Z",
     "iopub.status.idle": "2026-01-10T02:14:15.411024Z",
     "shell.execute_reply": "2026-01-10T02:14:15.410532Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Initialize vocabulary from a corpus of words.\n",
    "    \n",
    "    Each word is split into characters with an end-of-word marker '</w>'.\n",
    "    Returns a dictionary mapping character tuples to their frequency.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of words (strings), e.g., [\"low\", \"lower\", \"newest\"]\n",
    "    \n",
    "    Returns:\n",
    "        vocab: Dict mapping tuples to counts\n",
    "               e.g., {('l','o','w','</w>'): 1, ('l','o','w','e','r','</w>'): 1}\n",
    "    \n",
    "    Example:\n",
    "        >>> get_vocab([\"low\", \"low\"])\n",
    "        {('l', 'o', 'w', '</w>'): 2}\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for word in corpus:\n",
    "        # Convert word to tuple of characters + end marker\n",
    "        tokens = tuple(list(word) + ['</w>'])\n",
    "        vocab[tokens] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.412294Z",
     "iopub.status.busy": "2026-01-10T02:14:15.412154Z",
     "iopub.status.idle": "2026-01-10T02:14:15.415147Z",
     "shell.execute_reply": "2026-01-10T02:14:15.414567Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Count the frequency of all adjacent character pairs in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        vocab: Dict mapping character tuples to frequencies\n",
    "               e.g., {('l','o','w','</w>'): 1}\n",
    "    \n",
    "    Returns:\n",
    "        pairs: Dict mapping character pairs to their total frequency\n",
    "               e.g., {('l','o'): 1, ('o','w'): 1, ('w','</w>'): 1}\n",
    "    \n",
    "    Example:\n",
    "        >>> get_stats({('t','e','s','t','</w>'): 1})\n",
    "        {('t','e'): 1, ('e','s'): 1, ('s','t'): 1, ('t','</w>'): 1}\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        # Count all adjacent pairs in this word\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pairs[pair] += freq  # Weight by word frequency\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.416408Z",
     "iopub.status.busy": "2026-01-10T02:14:15.416273Z",
     "iopub.status.idle": "2026-01-10T02:14:15.418905Z",
     "shell.execute_reply": "2026-01-10T02:14:15.418511Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge a specific character pair across all words in the vocabulary.\n",
    "    \n",
    "    Replaces all occurrences of the pair with a single merged token.\n",
    "    \n",
    "    Args:\n",
    "        pair: Tuple of two characters to merge, e.g., ('e', 's')\n",
    "        vocab: Current vocabulary dict\n",
    "    \n",
    "    Returns:\n",
    "        new_vocab: Updated vocabulary with merged pairs\n",
    "    \n",
    "    Example:\n",
    "        >>> merge_vocab(('e','s'), {('t','e','s','t','</w>'): 1})\n",
    "        {('t', 'es', 't', '</w>'): 1}\n",
    "    \"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)  # e.g., \"e s\"\n",
    "    replacement = ''.join(pair)  # e.g., \"es\"\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        # Convert tuple to space-separated string\n",
    "        word_str = ' '.join(word)\n",
    "        # Replace bigram with merged symbol\n",
    "        new_word_str = word_str.replace(bigram, replacement)\n",
    "        # Convert back to tuple\n",
    "        new_vocab[tuple(new_word_str.split())] = freq\n",
    "    \n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.420192Z",
     "iopub.status.busy": "2026-01-10T02:14:15.420061Z",
     "iopub.status.idle": "2026-01-10T02:14:15.422845Z",
     "shell.execute_reply": "2026-01-10T02:14:15.422422Z"
    }
   },
   "outputs": [],
   "source": [
    "def byte_pair_encoding(corpus, num_merges=10):\n",
    "    \"\"\"\n",
    "    Perform Byte Pair Encoding on a corpus.\n",
    "    \n",
    "    Iteratively merges the most frequent character pairs to build a vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of words\n",
    "        num_merges: Number of merge operations to perform (vocab size growth)\n",
    "    \n",
    "    Returns:\n",
    "        vocab: Final vocabulary after all merges\n",
    "        merges: List of merge operations (pairs) in order performed\n",
    "    \n",
    "    Example:\n",
    "        >>> vocab, merges = byte_pair_encoding([\"low\", \"lower\"], num_merges=2)\n",
    "        >>> len(merges)\n",
    "        2\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(corpus)\n",
    "    merges = []\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "        print(f\"Merge {i + 1}: {best}\")\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "Test the implementation on a sample corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.424068Z",
     "iopub.status.busy": "2026-01-10T02:14:15.423940Z",
     "iopub.status.idle": "2026-01-10T02:14:15.426705Z",
     "shell.execute_reply": "2026-01-10T02:14:15.426300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('l', 'o')\n",
      "Merge 2: ('lo', 'w')\n",
      "Merge 3: ('e', 'r')\n",
      "Merge 4: ('er', '</w>')\n",
      "Merge 5: ('low', '</w>')\n",
      "Merge 6: ('low', 'e')\n",
      "Merge 7: ('lowe', 's')\n",
      "Merge 8: ('lowes', 't')\n",
      "Merge 9: ('lowest', '</w>')\n",
      "Merge 10: ('n', 'e')\n",
      "\n",
      "Final Vocabulary:\n",
      "  low</w> : 1\n",
      "  lowest</w> : 1\n",
      "  ne w er</w> : 1\n",
      "  w i d er</w> : 1\n"
     ]
    }
   ],
   "source": [
    "# Example corpus\n",
    "corpus = [\"low\", \"lowest\", \"newer\", \"wider\"]\n",
    "\n",
    "# Run BPE\n",
    "final_vocab, merge_operations = byte_pair_encoding(corpus, num_merges=10)\n",
    "\n",
    "print(\"\\nFinal Vocabulary:\")\n",
    "for word, freq in final_vocab.items():\n",
    "    print(f\"  {' '.join(word)} : {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Implementation\n",
    "\n",
    "Run these tests to verify correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:14:15.452839Z",
     "iopub.status.busy": "2026-01-10T02:14:15.452685Z",
     "iopub.status.idle": "2026-01-10T02:14:15.458167Z",
     "shell.execute_reply": "2026-01-10T02:14:15.457729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì test_get_vocab passed\n",
      "‚úì test_get_stats passed\n",
      "‚úì test_merge_vocab passed\n",
      "Merge 1: ('l', 'o')\n",
      "Merge 2: ('lo', 'w')\n",
      "Merge 3: ('e', 's')\n",
      "Merge 4: ('es', 't')\n",
      "Merge 5: ('est', '</w>')\n",
      "‚úì test_bpe_sequence passed\n",
      "\n",
      "============================================================\n",
      "‚úì All tests passed! Your BPE implementation is correct.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def test_get_vocab():\n",
    "    \"\"\"Test vocabulary initialization.\"\"\"\n",
    "    corpus = [\"test\"]\n",
    "    vocab = get_vocab(corpus)\n",
    "    expected = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    assert vocab == expected, f\"Expected {expected}, got {vocab}\"\n",
    "    print(\"‚úì test_get_vocab passed\")\n",
    "\n",
    "def test_get_stats():\n",
    "    \"\"\"Test pair counting.\"\"\"\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    stats = get_stats(vocab)\n",
    "    expected = {\n",
    "        ('t', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', 't'): 1,\n",
    "        ('t', '</w>'): 1\n",
    "    }\n",
    "    assert stats == expected, f\"Expected {expected}, got {stats}\"\n",
    "    print(\"‚úì test_get_stats passed\")\n",
    "\n",
    "def test_merge_vocab():\n",
    "    \"\"\"Test pair merging.\"\"\"\n",
    "    vocab = {('t', 'e', 's', 't', '</w>'): 1}\n",
    "    merged = merge_vocab(('e', 's'), vocab)\n",
    "    expected = {('t', 'es', 't', '</w>'): 1}\n",
    "    assert merged == expected, f\"Expected {expected}, got {merged}\"\n",
    "    print(\"‚úì test_merge_vocab passed\")\n",
    "\n",
    "def test_bpe_sequence():\n",
    "    \"\"\"Test full BPE algorithm.\"\"\"\n",
    "    corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "    final_vocab, merges = byte_pair_encoding(corpus, num_merges=5)\n",
    "    \n",
    "    # Check that we got 5 merge operations\n",
    "    assert len(merges) == 5, f\"Expected 5 merges, got {len(merges)}\"\n",
    "    \n",
    "    # Check that all merges are tuples of length 2\n",
    "    assert all(isinstance(pair, tuple) and len(pair) == 2 for pair in merges), \\\n",
    "        \"All merges should be tuples of length 2\"\n",
    "    \n",
    "    # Check that vocabulary is a dict\n",
    "    assert isinstance(final_vocab, dict), \"Vocabulary should be a dictionary\"\n",
    "    \n",
    "    print(\"‚úì test_bpe_sequence passed\")\n",
    "\n",
    "# Run all tests\n",
    "test_get_vocab()\n",
    "test_get_stats()\n",
    "test_merge_vocab()\n",
    "test_bpe_sequence()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì All tests passed! Your BPE implementation is correct.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Your Results\n",
    "\n",
    "After running BPE, you should see merge operations like:\n",
    "\n",
    "```\n",
    "Merge 1: ('e', 's')    ‚Üê 'es' appears in \"newest\" and \"widest\"\n",
    "Merge 2: ('es', 't')   ‚Üê 'est' is common\n",
    "Merge 3: ('est', '</w>') ‚Üê 'est</w>' is a word ending pattern\n",
    "...\n",
    "```\n",
    "\n",
    "Notice how BPE:\n",
    "- Discovers common subwords automatically (\"est\", \"er\")\n",
    "- Learns word endings (\"est</w>\", \"er</w>\")\n",
    "- Builds from frequent patterns first\n",
    "\n",
    "This is exactly how GPT-2's tokenizer was trained on web text!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Subword Tokenization**: Splits words into meaningful units smaller than words but larger than characters\n",
    "- **BPE Algorithm**: Iteratively merges most frequent character pairs to build vocabulary\n",
    "- **End-of-Word Marker**: `</w>` distinguishes word boundaries (\"er\" in \"lower\" vs standalone \"er\")\n",
    "- **Vocabulary Control**: Number of merges directly controls vocabulary size\n",
    "- **OOV Handling**: Unknown words can always be broken down into known subwords\n",
    "\n",
    "### Why BPE Works\n",
    "\n",
    "1. **Frequency-based**: Common words stay whole, rare words get split\n",
    "2. **Data-driven**: Learns patterns from your corpus automatically\n",
    "3. **Compresses well**: Efficient representation of text\n",
    "4. **Generalizes**: Handles morphology (\"unhappiness\" ‚Üí \"un\" + \"happiness\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "Be ready to answer:\n",
    "\n",
    "**Q: Why use BPE instead of word-level tokenization?**\n",
    "- A: BPE handles out-of-vocabulary (OOV) words by breaking them into known subwords. Word-level fails on rare/new words. BPE also has a much smaller vocabulary (32K vs 100K+ words).\n",
    "\n",
    "**Q: Why use BPE instead of character-level tokenization?**\n",
    "- A: Character-level creates very long sequences (inefficient) and loses semantic information. BPE preserves common subword units like \"ing\", \"tion\", \"un\" which carry meaning.\n",
    "\n",
    "**Q: What's the time complexity of BPE?**\n",
    "- A: O(N √ó M) where N = number of merges and M = corpus size. Each merge requires scanning the corpus to count pairs and update vocabulary.\n",
    "\n",
    "**Q: Why do we need the `</w>` end-of-word marker?**\n",
    "- A: To distinguish word boundaries. Without it, \"er\" at the end of \"lower\" would be indistinguishable from \"er\" as a standalone word. This matters for proper tokenization during inference.\n",
    "\n",
    "**Q: How does BPE handle a completely unknown word?**\n",
    "- A: It recursively breaks it down using the learned merge operations (in reverse - apply splits). Worst case, it falls back to individual characters, which are always in the vocabulary.\n",
    "\n",
    "**Q: What's the difference between BPE and WordPiece?**\n",
    "- A: BPE merges based on frequency. WordPiece (used in BERT) merges based on likelihood - chooses pairs that maximize probability of the training data. BPE is simpler and works well in practice.\n",
    "\n",
    "**Q: How do you choose the number of merges?**\n",
    "- A: It's a hyperparameter balancing vocabulary size vs sequence length. Common choices: 32K (GPT-2), 50K (RoBERTa). More merges = larger vocab but shorter sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2016)](https://arxiv.org/abs/1508.07909) - Original BPE paper\n",
    "- [Language Models are Unsupervised Multitask Learners (Radford et al., 2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) - GPT-2 paper using BPE\n",
    "- [SentencePiece: A simple and language independent approach to subword tokenization](https://github.com/google/sentencepiece) - Modern implementation\n",
    "- [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers/) - Fast BPE implementation\n",
    "- [Practical BPE Tutorial](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
