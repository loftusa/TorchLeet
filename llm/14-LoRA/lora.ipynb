{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA: Low-Rank Adaptation of Large Language Models\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning technique that adds trainable low-rank matrices to frozen pretrained weights.\n",
    "\n",
    "## Background\n",
    "\n",
    "### The Problem with Full Fine-Tuning\n",
    "\n",
    "Fine-tuning all parameters of a large model is:\n",
    "- **Memory intensive**: Need to store gradients for billions of parameters\n",
    "- **Storage heavy**: Each task requires a full model copy\n",
    "- **Slow**: Updating all parameters takes significant time\n",
    "\n",
    "For a 7B parameter model with fp32:\n",
    "- Model weights: 28 GB\n",
    "- Gradients: 28 GB  \n",
    "- Optimizer states (Adam): 56 GB\n",
    "- **Total: ~112 GB just for training!**\n",
    "\n",
    "### LoRA: The Key Insight\n",
    "\n",
    "The weight updates during fine-tuning have **low intrinsic rank**. Instead of updating the full weight matrix $W \\in \\mathbb{R}^{d \\times k}$, we can approximate the update as:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (down-projection)\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (up-projection)\n",
    "- $r \\ll \\min(d, k)$ is the rank (typically 4-64)\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Fewer trainable parameters**: From $d \\times k$ to $r \\times (d + k)$\n",
    "2. **No inference latency**: Merge $BA$ into $W$ at deployment\n",
    "3. **Task switching**: Swap LoRA adapters without reloading base model\n",
    "4. **Memory efficient**: Only store/update low-rank matrices\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Standard Linear Layer\n",
    "\n",
    "$$h = Wx$$\n",
    "\n",
    "### With LoRA\n",
    "\n",
    "$$h = Wx + \\frac{\\alpha}{r}BAx$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is a scaling factor (typically equals $r$)\n",
    "- $\\frac{\\alpha}{r}$ normalizes the contribution\n",
    "- $A$ is initialized from $\\mathcal{N}(0, \\sigma^2)$\n",
    "- $B$ is initialized to zeros (so $\\Delta W = 0$ initially)\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "For a weight matrix $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "- Original: $d \\times k$ parameters\n",
    "- LoRA: $r \\times (d + k)$ parameters\n",
    "- Reduction: $\\frac{r(d+k)}{dk} \\approx \\frac{r}{\\min(d,k)}$ for large matrices\n",
    "\n",
    "Example (GPT-3 175B attention):\n",
    "- $d = k = 12288$, $r = 8$\n",
    "- Original: 150M params per layer\n",
    "- LoRA: 196K params per layer (0.13%)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why low-rank adaptation works\n",
    "2. Implement a LoRA linear layer from scratch\n",
    "3. Learn proper initialization (A ~ N(0,1), B = 0)\n",
    "4. Understand merging for inference\n",
    "5. Know common hyperparameter choices (rank, alpha, target modules)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. `LoRALayer` class that wraps a linear layer with low-rank adapters\n",
    "2. Proper initialization (B=0, A~N(0,1))\n",
    "3. `merge()` and `unmerge()` methods for inference\n",
    "4. Demonstration of parameter efficiency\n",
    "\n",
    "## Hints\n",
    "\n",
    "1. Initialize B to zeros so initial output equals pretrained model\n",
    "2. Use `nn.Parameter` for trainable A and B matrices\n",
    "3. Freeze the original weight with `requires_grad = False`\n",
    "4. The scaling factor is typically $\\alpha / r$ where $\\alpha = r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) layer that wraps a linear layer.\n",
    "    \n",
    "    The forward pass computes:\n",
    "        h = Wx + (alpha/r) * BAx\n",
    "    \n",
    "    where W is frozen and only A, B are trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension\n",
    "            out_features: Output dimension\n",
    "            rank: Rank of the low-rank matrices (r)\n",
    "            alpha: Scaling factor (typically set equal to rank)\n",
    "            dropout: Dropout probability for LoRA path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Original frozen weight (pretrained)\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        # LoRA matrices\n",
    "        # A: down-projection (in_features -> rank)\n",
    "        # B: up-projection (rank -> out_features)\n",
    "        self.lora_A = nn.Parameter(torch.empty(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Track if weights are merged\n",
    "        self.merged = False\n",
    "        \n",
    "        # Initialize\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        # Initialize main weight like nn.Linear\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \n",
    "        # LoRA initialization:\n",
    "        # A: random Gaussian\n",
    "        # B: zeros (so delta_W starts at 0)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: h = Wx + (alpha/r) * B(Ax)\n",
    "        \"\"\"\n",
    "        # Base linear transformation\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "        \n",
    "        if not self.merged:\n",
    "            # LoRA path: x -> A -> dropout -> B -> scale\n",
    "            lora_out = self.dropout(x)\n",
    "            lora_out = F.linear(lora_out, self.lora_A)  # x @ A.T\n",
    "            lora_out = F.linear(lora_out, self.lora_B)  # (xA.T) @ B.T = x @ (BA).T\n",
    "            result = result + self.scaling * lora_out\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def merge(self):\n",
    "        \"\"\"Merge LoRA weights into main weight for inference.\"\"\"\n",
    "        if not self.merged:\n",
    "            # W' = W + (alpha/r) * BA\n",
    "            delta_w = self.scaling * (self.lora_B @ self.lora_A)\n",
    "            self.weight.data += delta_w\n",
    "            self.merged = True\n",
    "    \n",
    "    def unmerge(self):\n",
    "        \"\"\"Unmerge LoRA weights (restore original for training).\"\"\"\n",
    "        if self.merged:\n",
    "            delta_w = self.scaling * (self.lora_B @ self.lora_A)\n",
    "            self.weight.data -= delta_w\n",
    "            self.merged = False\n",
    "    \n",
    "    def lora_parameters(self):\n",
    "        \"\"\"Return only LoRA parameters for optimizer.\"\"\"\n",
    "        return [self.lora_A, self.lora_B]\n",
    "    \n",
    "    @property\n",
    "    def num_lora_params(self) -> int:\n",
    "        \"\"\"Number of trainable LoRA parameters.\"\"\"\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "    \n",
    "    @property\n",
    "    def num_base_params(self) -> int:\n",
    "        \"\"\"Number of base (frozen) parameters.\"\"\"\n",
    "        return self.weight.numel() + self.bias.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(\n",
    "    model: nn.Module,\n",
    "    rank: int = 4,\n",
    "    alpha: float = 1.0,\n",
    "    target_modules: Optional[list] = None,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply LoRA to specific modules in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "        target_modules: List of module names to apply LoRA to\n",
    "                       (default: all Linear layers)\n",
    "    \n",
    "    Returns:\n",
    "        Modified model with LoRA layers\n",
    "    \"\"\"\n",
    "    if target_modules is None:\n",
    "        target_modules = []\n",
    "    \n",
    "    # Freeze all parameters first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace target linear layers with LoRA versions\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check if this module should get LoRA\n",
    "            if not target_modules or any(t in name for t in target_modules):\n",
    "                # Create LoRA layer with same dimensions\n",
    "                lora_layer = LoRALayer(\n",
    "                    in_features=module.in_features,\n",
    "                    out_features=module.out_features,\n",
    "                    rank=rank,\n",
    "                    alpha=alpha,\n",
    "                )\n",
    "                \n",
    "                # Copy pretrained weights\n",
    "                lora_layer.weight.data = module.weight.data.clone()\n",
    "                if module.bias is not None:\n",
    "                    lora_layer.bias.data = module.bias.data.clone()\n",
    "                \n",
    "                # Freeze base weights\n",
    "                lora_layer.weight.requires_grad = False\n",
    "                lora_layer.bias.requires_grad = False\n",
    "                \n",
    "                # Replace in parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                if parent_name:\n",
    "                    parent = dict(model.named_modules())[parent_name]\n",
    "                else:\n",
    "                    parent = model\n",
    "                setattr(parent, child_name, lora_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> tuple:\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic LoRA layer\n",
    "print(\"=== Testing LoRA Layer ===\")\n",
    "\n",
    "in_features = 768\n",
    "out_features = 768\n",
    "rank = 8\n",
    "batch_size = 4\n",
    "\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "print(f\"Input dim: {in_features}, Output dim: {out_features}, Rank: {rank}\")\n",
    "print(f\"Base parameters: {lora_layer.num_base_params:,}\")\n",
    "print(f\"LoRA parameters: {lora_layer.num_lora_params:,}\")\n",
    "print(f\"Parameter reduction: {lora_layer.num_lora_params / lora_layer.num_base_params * 100:.2f}%\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(batch_size, in_features)\n",
    "y = lora_layer(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "assert y.shape == (batch_size, out_features)\n",
    "print(\"LoRA layer forward pass: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that initial output matches base model (B=0)\n",
    "print(\"\\n=== Testing Initial Equivalence ===\")\n",
    "\n",
    "# Create fresh LoRA layer\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "# Create equivalent standard linear layer\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "linear.weight.data = lora_layer.weight.data.clone()\n",
    "linear.bias.data = lora_layer.bias.data.clone()\n",
    "\n",
    "x = torch.randn(batch_size, in_features)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lora_out = lora_layer(x)\n",
    "    linear_out = linear(x)\n",
    "\n",
    "# Since B is initialized to 0, outputs should be identical\n",
    "max_diff = (lora_out - linear_out).abs().max().item()\n",
    "print(f\"Max difference (should be ~0): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-6, \"Initial LoRA output should match linear!\"\n",
    "print(\"Initial equivalence: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merge/unmerge\n",
    "print(\"\\n=== Testing Merge/Unmerge ===\")\n",
    "\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "# Simulate some training by modifying LoRA weights\n",
    "with torch.no_grad():\n",
    "    lora_layer.lora_B.data = torch.randn_like(lora_layer.lora_B) * 0.01\n",
    "\n",
    "x = torch.randn(batch_size, in_features)\n",
    "\n",
    "# Get output before merge\n",
    "with torch.no_grad():\n",
    "    out_before = lora_layer(x)\n",
    "\n",
    "# Merge\n",
    "lora_layer.merge()\n",
    "print(f\"Merged: {lora_layer.merged}\")\n",
    "\n",
    "# Get output after merge\n",
    "with torch.no_grad():\n",
    "    out_merged = lora_layer(x)\n",
    "\n",
    "# Outputs should be the same\n",
    "max_diff = (out_before - out_merged).abs().max().item()\n",
    "print(f\"Max diff (before vs merged): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-5, \"Merged output should match!\"\n",
    "\n",
    "# Unmerge\n",
    "lora_layer.unmerge()\n",
    "print(f\"Merged after unmerge: {lora_layer.merged}\")\n",
    "\n",
    "# Output should still match\n",
    "with torch.no_grad():\n",
    "    out_unmerged = lora_layer(x)\n",
    "\n",
    "max_diff = (out_before - out_unmerged).abs().max().item()\n",
    "print(f\"Max diff (before vs unmerged): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-5, \"Unmerged output should match original!\"\n",
    "print(\"Merge/Unmerge: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test applying LoRA to a model\n",
    "print(\"\\n=== Testing LoRA on a Model ===\")\n",
    "\n",
    "# Create a simple transformer-like block\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Simplified attention (just for testing)\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        attn = F.softmax(q @ k.transpose(-2, -1) / math.sqrt(q.size(-1)), dim=-1)\n",
    "        x = x + self.out_proj(attn @ v)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleTransformerBlock(d_model=512)\n",
    "\n",
    "# Count parameters before LoRA\n",
    "trainable_before, total_before = count_parameters(model)\n",
    "print(f\"Before LoRA: {trainable_before:,} trainable / {total_before:,} total\")\n",
    "\n",
    "# Apply LoRA to q, k, v projections only\n",
    "model = apply_lora_to_model(model, rank=8, alpha=8, target_modules=['q_proj', 'k_proj', 'v_proj'])\n",
    "\n",
    "# Count parameters after LoRA\n",
    "trainable_after, total_after = count_parameters(model)\n",
    "print(f\"After LoRA:  {trainable_after:,} trainable / {total_after:,} total\")\n",
    "print(f\"Trainable parameter reduction: {(1 - trainable_after/trainable_before)*100:.1f}%\")\n",
    "\n",
    "# Verify forward pass works\n",
    "x = torch.randn(2, 10, 512)  # batch=2, seq=10, dim=512\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "print(f\"\\nForward pass shape: {x.shape} -> {y.shape}\")\n",
    "assert y.shape == x.shape\n",
    "print(\"LoRA on model: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate training with LoRA\n",
    "print(\"\\n=== Training Demonstration ===\")\n",
    "\n",
    "# Create model with LoRA\n",
    "model = SimpleTransformerBlock(d_model=256)\n",
    "model = apply_lora_to_model(model, rank=4, alpha=4)\n",
    "\n",
    "# Only optimize LoRA parameters\n",
    "lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-3)\n",
    "\n",
    "print(f\"Optimizing {len(lora_params)} LoRA parameter tensors\")\n",
    "print(f\"Total LoRA params: {sum(p.numel() for p in lora_params):,}\")\n",
    "\n",
    "# Simple training loop\n",
    "x = torch.randn(4, 8, 256)\n",
    "target = torch.randn(4, 8, 256)\n",
    "\n",
    "losses = []\n",
    "for step in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = F.mse_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"\\nInitial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "assert losses[-1] < losses[0], \"Loss should decrease!\"\n",
    "print(\"Training demonstration: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameter efficiency at scale\n",
    "print(\"\\n=== Parameter Efficiency at Scale ===\")\n",
    "\n",
    "# Simulate different model sizes\n",
    "model_sizes = [\n",
    "    (\"GPT-2 Small\", 768, 12),\n",
    "    (\"GPT-2 Medium\", 1024, 24),\n",
    "    (\"GPT-2 Large\", 1280, 36),\n",
    "    (\"LLaMA-7B\", 4096, 32),\n",
    "    (\"LLaMA-13B\", 5120, 40),\n",
    "]\n",
    "\n",
    "rank = 8\n",
    "\n",
    "print(f\"{'Model':<15} {'Full Params':>15} {'LoRA Params':>15} {'Ratio':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, d_model, n_layers in model_sizes:\n",
    "    # 4 projection matrices per layer (q, k, v, o)\n",
    "    full_params = 4 * d_model * d_model * n_layers\n",
    "    lora_params = 4 * rank * (d_model + d_model) * n_layers\n",
    "    ratio = lora_params / full_params * 100\n",
    "    \n",
    "    print(f\"{name:<15} {full_params:>15,} {lora_params:>15,} {ratio:>9.2f}%\")\n",
    "\n",
    "print(\"\\nLoRA achieves ~0.1-0.5% of full fine-tuning parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All LoRA tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Low-Rank Decomposition**: $\\Delta W = BA$ where $r \\ll \\min(d, k)$\n",
    "   - Reduces trainable parameters by orders of magnitude\n",
    "   - Based on observation that updates have low intrinsic rank\n",
    "\n",
    "2. **Initialization**:\n",
    "   - $A \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "   - $B = 0$ (so initial $\\Delta W = 0$)\n",
    "   - This ensures model starts identical to pretrained\n",
    "\n",
    "3. **Scaling Factor**: $\\frac{\\alpha}{r}$\n",
    "   - Normalizes contribution regardless of rank\n",
    "   - Typically $\\alpha = r$ (scaling = 1)\n",
    "\n",
    "4. **Merging for Inference**:\n",
    "   - $W' = W + \\frac{\\alpha}{r}BA$\n",
    "   - No additional latency at inference time\n",
    "   - Can swap adapters by unmerging/remerging\n",
    "\n",
    "### Common Hyperparameters\n",
    "\n",
    "| Parameter | Typical Values | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| rank | 4-64 | Higher = more capacity |\n",
    "| alpha | = rank | Keeps scaling at 1 |\n",
    "| target_modules | q, k, v, o | Attention projections |\n",
    "| dropout | 0-0.1 | Regularization |\n",
    "\n",
    "### When to Use LoRA\n",
    "\n",
    "- Fine-tuning on limited compute\n",
    "- Multiple task-specific adapters\n",
    "- When full fine-tuning is too expensive\n",
    "- Collaborative fine-tuning (share adapters)\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "1. **Why does LoRA work?** Fine-tuning updates are low-rank; we can approximate them efficiently\n",
    "\n",
    "2. **Why initialize B=0?** So initial output equals pretrained model (no disruption)\n",
    "\n",
    "3. **What is the rank?** Bottleneck dimension; controls capacity vs efficiency tradeoff\n",
    "\n",
    "4. **Where to apply LoRA?** Typically attention projections (Q, K, V, O); sometimes MLP\n",
    "\n",
    "5. **Inference overhead?** None after merging! $W' = W + BA$ is precomputed\n",
    "\n",
    "6. **vs Full fine-tuning?** Slightly lower quality but 10-1000x fewer parameters\n",
    "\n",
    "7. **QLoRA?** Quantized base model + LoRA adapters for even more efficiency\n",
    "\n",
    "## References\n",
    "\n",
    "1. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Hu et al., 2021\n",
    "2. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Dettmers et al., 2023\n",
    "3. [PEFT Library](https://github.com/huggingface/peft) - HuggingFace implementation\n",
    "4. [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
