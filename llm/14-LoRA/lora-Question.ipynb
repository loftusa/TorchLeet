{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA: Low-Rank Adaptation of Large Language Models\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning technique that adds trainable low-rank matrices to frozen pretrained weights.\n",
    "\n",
    "## Background\n",
    "\n",
    "### The Problem with Full Fine-Tuning\n",
    "\n",
    "Fine-tuning all parameters of a large model is:\n",
    "- **Memory intensive**: Need to store gradients for billions of parameters\n",
    "- **Storage heavy**: Each task requires a full model copy\n",
    "- **Slow**: Updating all parameters takes significant time\n",
    "\n",
    "For a 7B parameter model with fp32:\n",
    "- Model weights: 28 GB\n",
    "- Gradients: 28 GB  \n",
    "- Optimizer states (Adam): 56 GB\n",
    "- **Total: ~112 GB just for training!**\n",
    "\n",
    "### LoRA: The Key Insight\n",
    "\n",
    "The weight updates during fine-tuning have **low intrinsic rank**. Instead of updating the full weight matrix $W \\in \\mathbb{R}^{d \\times k}$, we can approximate the update as:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (down-projection)\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (up-projection)\n",
    "- $r \\ll \\min(d, k)$ is the rank (typically 4-64)\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Fewer trainable parameters**: From $d \\times k$ to $r \\times (d + k)$\n",
    "2. **No inference latency**: Merge $BA$ into $W$ at deployment\n",
    "3. **Task switching**: Swap LoRA adapters without reloading base model\n",
    "4. **Memory efficient**: Only store/update low-rank matrices\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Standard Linear Layer\n",
    "\n",
    "$$h = Wx$$\n",
    "\n",
    "### With LoRA\n",
    "\n",
    "$$h = Wx + \\frac{\\alpha}{r}BAx$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is a scaling factor (typically equals $r$)\n",
    "- $\\frac{\\alpha}{r}$ normalizes the contribution\n",
    "- $A$ is initialized from $\\mathcal{N}(0, \\sigma^2)$\n",
    "- $B$ is initialized to zeros (so $\\Delta W = 0$ initially)\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "For a weight matrix $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "- Original: $d \\times k$ parameters\n",
    "- LoRA: $r \\times (d + k)$ parameters\n",
    "- Reduction: $\\frac{r(d+k)}{dk} \\approx \\frac{r}{\\min(d,k)}$ for large matrices\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why low-rank adaptation works\n",
    "2. Implement a LoRA linear layer from scratch\n",
    "3. Learn proper initialization (A ~ N(0,1), B = 0)\n",
    "4. Understand merging for inference\n",
    "5. Know common hyperparameter choices (rank, alpha, target modules)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. `LoRALayer` class that wraps a linear layer with low-rank adapters\n",
    "2. Proper initialization (B=0, A~N(0,1))\n",
    "3. `merge()` and `unmerge()` methods for inference\n",
    "4. Demonstration of parameter efficiency\n",
    "\n",
    "## Hints\n",
    "\n",
    "1. Initialize B to zeros so initial output equals pretrained model\n",
    "2. Use `nn.Parameter` for trainable A and B matrices\n",
    "3. Freeze the original weight with `requires_grad = False`\n",
    "4. The scaling factor is typically $\\alpha / r$ where $\\alpha = r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) layer that wraps a linear layer.\n",
    "    \n",
    "    The forward pass computes:\n",
    "        h = Wx + (alpha/r) * BAx\n",
    "    \n",
    "    where W is frozen and only A, B are trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension\n",
    "            out_features: Output dimension\n",
    "            rank: Rank of the low-rank matrices (r)\n",
    "            alpha: Scaling factor (typically set equal to rank)\n",
    "            dropout: Dropout probability for LoRA path\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Store dimensions and compute scaling factor\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = None  # TODO: alpha / rank\n",
    "        \n",
    "        # TODO: Create original frozen weight (pretrained)\n",
    "        # self.weight = ...\n",
    "        # self.bias = ...\n",
    "        \n",
    "        # TODO: Create LoRA matrices\n",
    "        # self.lora_A = ... (shape: rank x in_features)\n",
    "        # self.lora_B = ... (shape: out_features x rank)\n",
    "        \n",
    "        # TODO: Track if weights are merged\n",
    "        self.merged = False\n",
    "        \n",
    "        # TODO: Initialize parameters\n",
    "        pass\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        # TODO: Initialize main weight like nn.Linear\n",
    "        # TODO: Initialize lora_A with random values\n",
    "        # TODO: Initialize lora_B to zeros (critical!)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: h = Wx + (alpha/r) * B(Ax)\n",
    "        \"\"\"\n",
    "        # TODO: Compute base linear transformation\n",
    "        # TODO: If not merged, add LoRA contribution\n",
    "        pass\n",
    "    \n",
    "    def merge(self):\n",
    "        \"\"\"Merge LoRA weights into main weight for inference.\"\"\"\n",
    "        # TODO: W' = W + (alpha/r) * BA\n",
    "        pass\n",
    "    \n",
    "    def unmerge(self):\n",
    "        \"\"\"Unmerge LoRA weights (restore original for training).\"\"\"\n",
    "        # TODO: W = W' - (alpha/r) * BA\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> tuple:\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic LoRA layer\n",
    "print(\"=== Testing LoRA Layer ===\")\n",
    "\n",
    "in_features = 768\n",
    "out_features = 768\n",
    "rank = 8\n",
    "batch_size = 4\n",
    "\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "print(f\"Input dim: {in_features}, Output dim: {out_features}, Rank: {rank}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(batch_size, in_features)\n",
    "y = lora_layer(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "assert y.shape == (batch_size, out_features)\n",
    "print(\"LoRA layer forward pass: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that initial output matches base model (B=0)\n",
    "print(\"\\n=== Testing Initial Equivalence ===\")\n",
    "\n",
    "# Create fresh LoRA layer\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "# Create equivalent standard linear layer\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "linear.weight.data = lora_layer.weight.data.clone()\n",
    "linear.bias.data = lora_layer.bias.data.clone()\n",
    "\n",
    "x = torch.randn(batch_size, in_features)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lora_out = lora_layer(x)\n",
    "    linear_out = linear(x)\n",
    "\n",
    "# Since B is initialized to 0, outputs should be identical\n",
    "max_diff = (lora_out - linear_out).abs().max().item()\n",
    "print(f\"Max difference (should be ~0): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-6, \"Initial LoRA output should match linear!\"\n",
    "print(\"Initial equivalence: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merge/unmerge\n",
    "print(\"\\n=== Testing Merge/Unmerge ===\")\n",
    "\n",
    "lora_layer = LoRALayer(in_features, out_features, rank=rank, alpha=rank)\n",
    "\n",
    "# Simulate some training by modifying LoRA weights\n",
    "with torch.no_grad():\n",
    "    lora_layer.lora_B.data = torch.randn_like(lora_layer.lora_B) * 0.01\n",
    "\n",
    "x = torch.randn(batch_size, in_features)\n",
    "\n",
    "# Get output before merge\n",
    "with torch.no_grad():\n",
    "    out_before = lora_layer(x)\n",
    "\n",
    "# Merge\n",
    "lora_layer.merge()\n",
    "print(f\"Merged: {lora_layer.merged}\")\n",
    "\n",
    "# Get output after merge\n",
    "with torch.no_grad():\n",
    "    out_merged = lora_layer(x)\n",
    "\n",
    "# Outputs should be the same\n",
    "max_diff = (out_before - out_merged).abs().max().item()\n",
    "print(f\"Max diff (before vs merged): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-5, \"Merged output should match!\"\n",
    "\n",
    "# Unmerge\n",
    "lora_layer.unmerge()\n",
    "print(f\"Merged after unmerge: {lora_layer.merged}\")\n",
    "\n",
    "# Output should still match\n",
    "with torch.no_grad():\n",
    "    out_unmerged = lora_layer(x)\n",
    "\n",
    "max_diff = (out_before - out_unmerged).abs().max().item()\n",
    "print(f\"Max diff (before vs unmerged): {max_diff:.2e}\")\n",
    "assert max_diff < 1e-5, \"Unmerged output should match original!\"\n",
    "print(\"Merge/Unmerge: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameter efficiency at scale\n",
    "print(\"\\n=== Parameter Efficiency at Scale ===\")\n",
    "\n",
    "# Simulate different model sizes\n",
    "model_sizes = [\n",
    "    (\"GPT-2 Small\", 768, 12),\n",
    "    (\"GPT-2 Medium\", 1024, 24),\n",
    "    (\"GPT-2 Large\", 1280, 36),\n",
    "    (\"LLaMA-7B\", 4096, 32),\n",
    "    (\"LLaMA-13B\", 5120, 40),\n",
    "]\n",
    "\n",
    "rank = 8\n",
    "\n",
    "print(f\"{'Model':<15} {'Full Params':>15} {'LoRA Params':>15} {'Ratio':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, d_model, n_layers in model_sizes:\n",
    "    # 4 projection matrices per layer (q, k, v, o)\n",
    "    full_params = 4 * d_model * d_model * n_layers\n",
    "    lora_params = 4 * rank * (d_model + d_model) * n_layers\n",
    "    ratio = lora_params / full_params * 100\n",
    "    \n",
    "    print(f\"{name:<15} {full_params:>15,} {lora_params:>15,} {ratio:>9.2f}%\")\n",
    "\n",
    "print(\"\\nLoRA achieves ~0.1-0.5% of full fine-tuning parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All LoRA tests passed!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Low-Rank Decomposition**: $\\Delta W = BA$ where $r \\ll \\min(d, k)$\n",
    "   - Reduces trainable parameters by orders of magnitude\n",
    "   - Based on observation that updates have low intrinsic rank\n",
    "\n",
    "2. **Initialization**:\n",
    "   - $A \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "   - $B = 0$ (so initial $\\Delta W = 0$)\n",
    "   - This ensures model starts identical to pretrained\n",
    "\n",
    "3. **Scaling Factor**: $\\frac{\\alpha}{r}$\n",
    "   - Normalizes contribution regardless of rank\n",
    "   - Typically $\\alpha = r$ (scaling = 1)\n",
    "\n",
    "4. **Merging for Inference**:\n",
    "   - $W' = W + \\frac{\\alpha}{r}BA$\n",
    "   - No additional latency at inference time\n",
    "   - Can swap adapters by unmerging/remerging\n",
    "\n",
    "### Common Hyperparameters\n",
    "\n",
    "| Parameter | Typical Values | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| rank | 4-64 | Higher = more capacity |\n",
    "| alpha | = rank | Keeps scaling at 1 |\n",
    "| target_modules | q, k, v, o | Attention projections |\n",
    "| dropout | 0-0.1 | Regularization |\n",
    "\n",
    "### When to Use LoRA\n",
    "\n",
    "- Fine-tuning on limited compute\n",
    "- Multiple task-specific adapters\n",
    "- When full fine-tuning is too expensive\n",
    "- Collaborative fine-tuning (share adapters)\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "1. **Why does LoRA work?** Fine-tuning updates are low-rank; we can approximate them efficiently\n",
    "\n",
    "2. **Why initialize B=0?** So initial output equals pretrained model (no disruption)\n",
    "\n",
    "3. **What is the rank?** Bottleneck dimension; controls capacity vs efficiency tradeoff\n",
    "\n",
    "4. **Where to apply LoRA?** Typically attention projections (Q, K, V, O); sometimes MLP\n",
    "\n",
    "5. **Inference overhead?** None after merging! $W' = W + BA$ is precomputed\n",
    "\n",
    "6. **vs Full fine-tuning?** Slightly lower quality but 10-1000x fewer parameters\n",
    "\n",
    "7. **QLoRA?** Quantized base model + LoRA adapters for even more efficiency\n",
    "\n",
    "## References\n",
    "\n",
    "1. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Hu et al., 2021\n",
    "2. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Dettmers et al., 2023\n",
    "3. [PEFT Library](https://github.com/huggingface/peft) - HuggingFace implementation\n",
    "4. [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
