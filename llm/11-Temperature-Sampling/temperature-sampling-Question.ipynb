{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ¡ï¸ Implement Temperature Sampling for LLM Decoding\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Implement **Temperature Sampling**, the fundamental technique for controlling randomness in language model text generation. Temperature scaling modifies the probability distribution over the vocabulary, allowing you to trade off between creative/diverse outputs and deterministic/focused outputs.\n",
    "\n",
    "This is one of the most commonly asked topics in AI engineer interviews because it's essential for understanding how ChatGPT, Claude, and other LLMs generate text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Why Temperature?\n",
    "\n",
    "**The Problem with Greedy Decoding**\n",
    "\n",
    "Language models output **logits** (raw scores) for each token in the vocabulary. The naive approach is to always pick the highest-scoring token (greedy decoding), but this leads to:\n",
    "- Repetitive, boring text\n",
    "- Getting stuck in loops\n",
    "- Missing creative or contextually appropriate alternatives\n",
    "\n",
    "**The Solution: Sampling with Temperature**\n",
    "\n",
    "Instead of always picking the argmax, we:\n",
    "1. Convert logits to probabilities using softmax\n",
    "2. **Scale the logits by temperature** before softmax\n",
    "3. Sample from the resulting distribution\n",
    "\n",
    "**Temperature Effects:**\n",
    "\n",
    "| Temperature | Effect | Use Case |\n",
    "|-------------|--------|----------|\n",
    "| T â†’ 0 | Approaches greedy (argmax) | Factual Q&A, code generation |\n",
    "| T = 1.0 | Original distribution | Balanced generation |\n",
    "| T > 1.0 | Flatter distribution (more random) | Creative writing, brainstorming |\n",
    "\n",
    "**Real-World Defaults:**\n",
    "- ChatGPT: T â‰ˆ 0.7 for chat, T â‰ˆ 0.2 for code\n",
    "- Claude: T â‰ˆ 1.0 with other sampling methods\n",
    "- Code generation: T â‰ˆ 0.0-0.2 for deterministic output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "**Standard Softmax:**\n",
    "$$p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "**Softmax with Temperature:**\n",
    "$$p_i = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ = logit for token $i$\n",
    "- $T$ = temperature parameter\n",
    "- $p_i$ = probability of token $i$\n",
    "\n",
    "**Why Division by T Works:**\n",
    "\n",
    "- **T < 1**: Dividing by a small number **amplifies** differences between logits â†’ sharper distribution\n",
    "- **T = 1**: No change â†’ original distribution\n",
    "- **T > 1**: Dividing by a large number **reduces** differences between logits â†’ flatter distribution\n",
    "\n",
    "**Limit Behavior:**\n",
    "- As T â†’ 0: Distribution becomes one-hot (argmax)\n",
    "- As T â†’ âˆž: Distribution becomes uniform\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "1. âœ… Understand how temperature affects probability distributions\n",
    "2. âœ… Implement temperature-scaled softmax from scratch\n",
    "3. âœ… Visualize the effect of different temperatures\n",
    "4. âœ… Sample tokens from the modified distribution\n",
    "5. âœ… Understand the trade-off between creativity and determinism\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Implement the following functions:\n",
    "\n",
    "1. **`softmax_with_temperature(logits, temperature)`**\n",
    "   - Apply temperature scaling to logits\n",
    "   - Return probability distribution\n",
    "\n",
    "2. **`sample_with_temperature(logits, temperature)`**\n",
    "   - Get probabilities using temperature\n",
    "   - Sample a token index from the distribution\n",
    "\n",
    "3. **`generate_with_temperature(model_fn, prompt_tokens, max_tokens, temperature)`**\n",
    "   - Autoregressive generation loop\n",
    "   - Use temperature sampling at each step\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint 1: Temperature Scaling</summary>\n",
    "  \n",
    "  Simply divide the logits by temperature before applying softmax:\n",
    "  ```python\n",
    "  scaled_logits = logits / temperature\n",
    "  probs = F.softmax(scaled_logits, dim=-1)\n",
    "  ```\n",
    "  \n",
    "  Handle the edge case where temperature is very small (use a minimum like 1e-8).\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint 2: Sampling from Distribution</summary>\n",
    "  \n",
    "  Use `torch.multinomial` to sample from a probability distribution:\n",
    "  ```python\n",
    "  token_idx = torch.multinomial(probs, num_samples=1)\n",
    "  ```\n",
    "  \n",
    "  This randomly selects an index with probability proportional to `probs`.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint 3: Numerical Stability</summary>\n",
    "  \n",
    "  For numerical stability with very low temperatures, subtract the max logit before exponentiating:\n",
    "  ```python\n",
    "  scaled = logits / temperature\n",
    "  scaled = scaled - scaled.max(dim=-1, keepdim=True).values\n",
    "  ```\n",
    "  \n",
    "  This prevents overflow in `exp()` while preserving the softmax result.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>ðŸ’¡ Hint 4: Generation Loop</summary>\n",
    "  \n",
    "  The autoregressive generation pattern:\n",
    "  ```python\n",
    "  for _ in range(max_tokens):\n",
    "      logits = model(tokens)[:, -1, :]  # Get logits for last position\n",
    "      next_token = sample_with_temperature(logits, temperature)\n",
    "      tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply temperature-scaled softmax to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw model outputs, shape (batch_size, vocab_size) or (vocab_size,)\n",
    "        temperature: Temperature parameter (default 1.0)\n",
    "                    - T < 1.0: Sharper distribution (more deterministic)\n",
    "                    - T = 1.0: Original distribution\n",
    "                    - T > 1.0: Flatter distribution (more random)\n",
    "    \n",
    "    Returns:\n",
    "        Probability distribution over vocabulary, same shape as input\n",
    "    \n",
    "    Example:\n",
    "        >>> logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "        >>> softmax_with_temperature(logits, temperature=0.5)  # Sharper\n",
    "        >>> softmax_with_temperature(logits, temperature=2.0)  # Flatter\n",
    "    \"\"\"\n",
    "    # TODO: Implement temperature-scaled softmax\n",
    "    # 1. Ensure temperature is positive (avoid division by zero)\n",
    "    # 2. Scale logits by temperature\n",
    "    # 3. Apply softmax\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample a token index from logits using temperature scaling.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw model outputs, shape (batch_size, vocab_size) or (vocab_size,)\n",
    "        temperature: Temperature parameter for controlling randomness\n",
    "    \n",
    "    Returns:\n",
    "        Sampled token index, shape (batch_size, 1) or (1,)\n",
    "    \n",
    "    Example:\n",
    "        >>> logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "        >>> sample_with_temperature(logits, temperature=1.0)\n",
    "        tensor([0])  # Likely to be 0 (highest logit)\n",
    "    \"\"\"\n",
    "    # TODO: Implement temperature sampling\n",
    "    # 1. Get temperature-scaled probabilities\n",
    "    # 2. Handle 1D case (add batch dimension)\n",
    "    # 3. Sample from the distribution using torch.multinomial\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(\n",
    "    model_fn,\n",
    "    prompt_tokens: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    "    eos_token_id: int = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate tokens autoregressively using temperature sampling.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that takes tokens and returns logits\n",
    "                  Signature: model_fn(tokens) -> logits of shape (batch, seq_len, vocab_size)\n",
    "        prompt_tokens: Initial tokens, shape (batch_size, seq_len)\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Temperature for sampling\n",
    "        eos_token_id: Optional end-of-sequence token ID to stop generation\n",
    "    \n",
    "    Returns:\n",
    "        Generated tokens including prompt, shape (batch_size, seq_len + generated)\n",
    "    \"\"\"\n",
    "    # TODO: Implement autoregressive generation\n",
    "    # 1. Clone prompt tokens\n",
    "    # 2. Loop for max_new_tokens:\n",
    "    #    a. Get model predictions\n",
    "    #    b. Extract logits for last position\n",
    "    #    c. Sample next token with temperature\n",
    "    #    d. Append to sequence\n",
    "    #    e. Check for EOS\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Temperature Effects\n",
    "\n",
    "Let's visualize how temperature affects the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample logits (simulating model output for 10 tokens)\n",
    "torch.manual_seed(42)\n",
    "logits = torch.tensor([2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0, -1.5, -2.0])\n",
    "token_names = [f\"token_{i}\" for i in range(len(logits))]\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n",
    "fig.suptitle('Effect of Temperature on Probability Distribution', fontsize=14)\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    probs = softmax_with_temperature(logits, temperature=temp)\n",
    "    ax.bar(range(len(probs)), probs.numpy())\n",
    "    ax.set_title(f'T = {temp}')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Token')\n",
    "    if temp == temperatures[0]:\n",
    "        ax.set_ylabel('Probability')\n",
    "    \n",
    "    # Show entropy\n",
    "    entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
    "    ax.text(0.5, 0.95, f'H={entropy:.2f}', transform=ax.transAxes, \n",
    "            verticalalignment='top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- T=0.1: Almost deterministic (near-argmax)\")\n",
    "print(\"- T=0.5: Sharp but still some variation\")\n",
    "print(\"- T=1.0: Original distribution\")\n",
    "print(\"- T=2.0: Flatter, more random\")\n",
    "print(\"- T=5.0: Nearly uniform distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax_with_temperature():\n",
    "    \"\"\"Test that temperature scaling works correctly.\"\"\"\n",
    "    logits = torch.tensor([2.0, 1.0, 0.0])\n",
    "    \n",
    "    # Test 1: Temperature = 1.0 should equal standard softmax\n",
    "    probs_t1 = softmax_with_temperature(logits, temperature=1.0)\n",
    "    probs_std = F.softmax(logits, dim=-1)\n",
    "    assert torch.allclose(probs_t1, probs_std, atol=1e-6), \"T=1.0 should equal standard softmax\"\n",
    "    print(\"âœ“ T=1.0 equals standard softmax\")\n",
    "    \n",
    "    # Test 2: Probabilities should sum to 1\n",
    "    for temp in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
    "        probs = softmax_with_temperature(logits, temperature=temp)\n",
    "        assert torch.allclose(probs.sum(), torch.tensor(1.0), atol=1e-6), f\"Probs should sum to 1 at T={temp}\"\n",
    "    print(\"âœ“ Probabilities sum to 1 for all temperatures\")\n",
    "    \n",
    "    # Test 3: Lower temperature should make distribution sharper\n",
    "    probs_low = softmax_with_temperature(logits, temperature=0.1)\n",
    "    probs_high = softmax_with_temperature(logits, temperature=2.0)\n",
    "    assert probs_low.max() > probs_high.max(), \"Lower T should have higher max probability\"\n",
    "    print(\"âœ“ Lower temperature creates sharper distribution\")\n",
    "    \n",
    "    # Test 4: Very low temperature should approach argmax\n",
    "    probs_tiny = softmax_with_temperature(logits, temperature=0.01)\n",
    "    assert probs_tiny.argmax() == logits.argmax(), \"Very low T should select argmax\"\n",
    "    assert probs_tiny.max() > 0.99, \"Very low T should have near-1.0 max probability\"\n",
    "    print(\"âœ“ Very low temperature approaches argmax\")\n",
    "    \n",
    "    print(\"\\nâœ“ All softmax_with_temperature tests passed!\")\n",
    "\n",
    "\n",
    "def test_sample_with_temperature():\n",
    "    \"\"\"Test sampling behavior.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    logits = torch.tensor([10.0, 0.0, 0.0, 0.0, 0.0])  # Strong preference for first token\n",
    "    \n",
    "    # Test 1: Low temperature should almost always select argmax\n",
    "    samples_low_t = [sample_with_temperature(logits, temperature=0.01).item() for _ in range(100)]\n",
    "    assert sum(s == 0 for s in samples_low_t) > 95, \"Low T should select argmax most of the time\"\n",
    "    print(\"âœ“ Low temperature selects argmax consistently\")\n",
    "    \n",
    "    # Test 2: High temperature should give more variety\n",
    "    samples_high_t = [sample_with_temperature(logits, temperature=5.0).item() for _ in range(100)]\n",
    "    unique_samples = len(set(samples_high_t))\n",
    "    assert unique_samples > 1, \"High T should produce variety\"\n",
    "    print(f\"âœ“ High temperature produces {unique_samples} unique samples\")\n",
    "    \n",
    "    # Test 3: Output shape should be correct\n",
    "    batch_logits = torch.randn(4, 100)  # Batch of 4, vocab size 100\n",
    "    samples = sample_with_temperature(batch_logits, temperature=1.0)\n",
    "    assert samples.shape == (4, 1), f\"Expected shape (4, 1), got {samples.shape}\"\n",
    "    print(\"âœ“ Batch sampling produces correct shape\")\n",
    "    \n",
    "    print(\"\\nâœ“ All sample_with_temperature tests passed!\")\n",
    "\n",
    "\n",
    "def test_generate_with_temperature():\n",
    "    \"\"\"Test generation loop.\"\"\"\n",
    "    # Create a simple mock model\n",
    "    vocab_size = 10\n",
    "    def mock_model(tokens):\n",
    "        # Returns random logits for testing\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        return torch.randn(batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = torch.tensor([[0, 1, 2]])  # Batch size 1, 3 prompt tokens\n",
    "    generated = generate_with_temperature(mock_model, prompt, max_new_tokens=5, temperature=1.0)\n",
    "    \n",
    "    assert generated.shape == (1, 8), f\"Expected shape (1, 8), got {generated.shape}\"\n",
    "    assert (generated[:, :3] == prompt).all(), \"Prompt should be preserved\"\n",
    "    print(\"âœ“ Generation produces correct output shape\")\n",
    "    print(\"âœ“ Prompt tokens are preserved\")\n",
    "    \n",
    "    print(\"\\nâœ“ All generate_with_temperature tests passed!\")\n",
    "\n",
    "\n",
    "# Run all tests\n",
    "test_softmax_with_temperature()\n",
    "print()\n",
    "test_sample_with_temperature()\n",
    "print()\n",
    "test_generate_with_temperature()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ All tests passed! Temperature sampling implementation is correct.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Sampling Diversity\n",
    "\n",
    "Let's see how temperature affects the diversity of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate vocabulary with meaningful tokens\n",
    "vocab = [\"the\", \"a\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"lazy\", \"dog\", \"cat\"]\n",
    "\n",
    "# Simulated logits favoring certain tokens\n",
    "logits = torch.tensor([3.0, 2.5, 1.0, 1.5, 0.5, 0.8, 0.3, 0.2, -0.5, -1.0])\n",
    "\n",
    "print(\"Sampling 10 tokens at different temperatures:\\n\")\n",
    "\n",
    "for temp in [0.1, 0.5, 1.0, 2.0]:\n",
    "    samples = [vocab[sample_with_temperature(logits, temp).item()] for _ in range(10)]\n",
    "    unique_count = len(set(samples))\n",
    "    print(f\"T={temp}: {samples}\")\n",
    "    print(f\"        Unique tokens: {unique_count}/10\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Temperature Scaling**: Divide logits by temperature before softmax to control distribution sharpness\n",
    "- **Low Temperature (T < 1)**: More deterministic, picks high-probability tokens\n",
    "- **High Temperature (T > 1)**: More random, flatter distribution, more diverse outputs\n",
    "- **T = 1.0**: Original model distribution\n",
    "- **Sampling**: Use `torch.multinomial` to sample from the probability distribution\n",
    "\n",
    "### When to Use Different Temperatures\n",
    "\n",
    "| Use Case | Temperature | Why |\n",
    "|----------|-------------|-----|\n",
    "| Code generation | 0.0-0.2 | Deterministic, correct syntax |\n",
    "| Factual Q&A | 0.3-0.5 | Focused but not repetitive |\n",
    "| General chat | 0.7-0.9 | Balanced creativity |\n",
    "| Creative writing | 1.0-1.5 | More diverse outputs |\n",
    "| Brainstorming | 1.5-2.0 | Maximum creativity |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Tips\n",
    "\n",
    "Be ready to answer:\n",
    "\n",
    "**Q: What is temperature in LLM sampling?**\n",
    "- A: Temperature is a hyperparameter that scales logits before softmax, controlling the randomness of text generation. Lower temperatures make the model more deterministic (sharper distribution), higher temperatures make it more random (flatter distribution).\n",
    "\n",
    "**Q: What happens when temperature approaches 0?**\n",
    "- A: The distribution becomes increasingly peaked, approaching a one-hot vector at the argmax. This is equivalent to greedy decoding. Mathematically, dividing by a very small T amplifies differences between logits.\n",
    "\n",
    "**Q: What happens when temperature is very high?**\n",
    "- A: The distribution becomes increasingly uniform. Dividing by a large T compresses all logits toward zero, making the softmax output approach 1/vocab_size for all tokens.\n",
    "\n",
    "**Q: Why do we divide logits by temperature instead of multiplying?**\n",
    "- A: Division makes the intuition clearer: T<1 \"sharpens\" and T>1 \"softens\". Mathematically, `logits/T` with T<1 amplifies differences between logits, making high logits even more dominant.\n",
    "\n",
    "**Q: What's the trade-off between low and high temperature?**\n",
    "- A: Low temperature: more accurate/focused but potentially repetitive and boring. High temperature: more creative/diverse but may produce incoherent or off-topic text. The optimal value depends on the task.\n",
    "\n",
    "**Q: How does temperature interact with other sampling methods like top-k and top-p?**\n",
    "- A: Temperature is typically applied first (scaling logits), then top-k/top-p filtering is applied to the resulting probabilities. Common combination: temperature=0.7 with top_p=0.9. The order matters!\n",
    "\n",
    "**Q: What temperature would you use for code generation vs creative writing?**\n",
    "- A: Code generation: Tâ‰ˆ0.0-0.2 for deterministic, syntactically correct output. Creative writing: Tâ‰ˆ1.0-1.5 for more diverse, unexpected word choices. Always validate empirically for your specific use case.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [The Curious Case of Neural Text Degeneration (Holtzman et al., 2020)](https://arxiv.org/abs/1904.09751) - Introduces nucleus sampling, discusses temperature\n",
    "- [Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165) - GPT-3 paper, discusses sampling strategies\n",
    "- [How to Generate Text: Using Different Decoding Methods (HuggingFace)](https://huggingface.co/blog/how-to-generate) - Practical guide to generation\n",
    "- [Temperature in Softmax (Wikipedia)](https://en.wikipedia.org/wiki/Softmax_function#Softmax_with_temperature) - Mathematical background"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
