{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 Quantization from Scratch\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Large language models require massive memory and compute. A 70B parameter model in FP16 needs ~140GB just for weights! **Quantization** reduces this by using lower-precision data types.\n",
    "\n",
    "Your task is to implement **INT8 quantization from scratch** and understand the trade-offs between memory savings and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "### Why Quantization?\n",
    "\n",
    "| Precision | Bits | Memory per 7B params | Typical Use |\n",
    "|-----------|------|---------------------|-------------|\n",
    "| FP32 | 32 | 28 GB | Training |\n",
    "| FP16/BF16 | 16 | 14 GB | Training/Inference |\n",
    "| INT8 | 8 | 7 GB | Inference |\n",
    "| INT4 | 4 | 3.5 GB | Inference |\n",
    "\n",
    "### Quantization Types\n",
    "\n",
    "1. **Post-Training Quantization (PTQ)**: Quantize after training (what we'll implement)\n",
    "2. **Quantization-Aware Training (QAT)**: Train with quantization in the loop\n",
    "\n",
    "### Quantization Schemes\n",
    "\n",
    "1. **Symmetric**: Zero-point is 0, range is `[-max, max]`\n",
    "2. **Asymmetric**: Zero-point can be non-zero, range is `[min, max]`\n",
    "\n",
    "### The Math\n",
    "\n",
    "**Symmetric Quantization:**\n",
    "```\n",
    "scale = max(|x|) / 127\n",
    "x_quant = round(x / scale)  # clamp to [-128, 127]\n",
    "x_dequant = x_quant * scale\n",
    "```\n",
    "\n",
    "**Asymmetric Quantization:**\n",
    "```\n",
    "scale = (max(x) - min(x)) / 255\n",
    "zero_point = round(-min(x) / scale)\n",
    "x_quant = round(x / scale) + zero_point  # clamp to [0, 255]\n",
    "x_dequant = (x_quant - zero_point) * scale\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Implement `symmetric_quantize()` and `symmetric_dequantize()`\n",
    "2. Implement `asymmetric_quantize()` and `asymmetric_dequantize()`\n",
    "3. Create `QuantizedLinear` layer that stores INT8 weights\n",
    "4. Compare memory usage and accuracy vs FP32\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Scale Calculation</summary>\n",
    "\n",
    "For symmetric quantization to INT8:\n",
    "- Range is [-128, 127], so use 127 as the max quantized value\n",
    "- Scale = max(|tensor|) / 127\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Clamping</summary>\n",
    "\n",
    "After dividing by scale and rounding, you must clamp:\n",
    "- Symmetric: `torch.clamp(x, -128, 127).to(torch.int8)`\n",
    "- Asymmetric: `torch.clamp(x, 0, 255).to(torch.uint8)`\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3: Per-Channel vs Per-Tensor</summary>\n",
    "\n",
    "Per-tensor uses one scale for entire tensor. Per-channel uses one scale per output channel, giving better accuracy for weight quantization.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Symmetric Quantization\n",
    "\n",
    "Symmetric quantization is simpler and more common for weights:\n",
    "- Zero-point is always 0\n",
    "- Range is symmetric around zero: `[-max, max]`\n",
    "- Uses signed INT8: `[-128, 127]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_quantize(x: torch.Tensor, num_bits: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Symmetric quantization to signed integers.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (float)\n",
    "        num_bits: Number of bits (default 8 for INT8)\n",
    "    \n",
    "    Returns:\n",
    "        x_quant: Quantized tensor (int8)\n",
    "        scale: Scale factor for dequantization\n",
    "    \"\"\"\n",
    "    # Calculate the maximum quantized value (127 for 8-bit)\n",
    "    qmax = 2 ** (num_bits - 1) - 1  # 127 for 8-bit\n",
    "    qmin = -qmax - 1  # -128 for 8-bit\n",
    "    \n",
    "    # Calculate scale: largest absolute value maps to qmax\n",
    "    max_val = x.abs().max()\n",
    "    scale = max_val / qmax\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if scale == 0:\n",
    "        scale = torch.tensor(1.0)\n",
    "    \n",
    "    # Quantize: divide by scale, round, and clamp\n",
    "    x_quant = torch.round(x / scale)\n",
    "    x_quant = torch.clamp(x_quant, qmin, qmax).to(torch.int8)\n",
    "    \n",
    "    return x_quant, scale\n",
    "\n",
    "\n",
    "def symmetric_dequantize(x_quant: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dequantize from signed integers back to float.\n",
    "    \n",
    "    Args:\n",
    "        x_quant: Quantized tensor (int8)\n",
    "        scale: Scale factor\n",
    "    \n",
    "    Returns:\n",
    "        x_dequant: Dequantized tensor (float)\n",
    "    \"\"\"\n",
    "    return x_quant.float() * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test symmetric quantization\n",
    "print(\"=== Testing Symmetric Quantization ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(4, 4) * 2  # Random values roughly in [-4, 4]\n",
    "print(f\"Original tensor (FP32):\")\n",
    "print(x)\n",
    "print(f\"\\nOriginal dtype: {x.dtype}\")\n",
    "print(f\"Original memory: {x.numel() * 4} bytes\")\n",
    "\n",
    "# Quantize\n",
    "x_quant, scale = symmetric_quantize(x)\n",
    "print(f\"\\nQuantized tensor (INT8):\")\n",
    "print(x_quant)\n",
    "print(f\"\\nScale: {scale.item():.6f}\")\n",
    "print(f\"Quantized dtype: {x_quant.dtype}\")\n",
    "print(f\"Quantized memory: {x_quant.numel() * 1} bytes (4x smaller!)\")\n",
    "\n",
    "# Dequantize and check error\n",
    "x_dequant = symmetric_dequantize(x_quant, scale)\n",
    "print(f\"\\nDequantized tensor:\")\n",
    "print(x_dequant)\n",
    "\n",
    "error = (x - x_dequant).abs()\n",
    "print(f\"\\nMax absolute error: {error.max().item():.6f}\")\n",
    "print(f\"Mean absolute error: {error.mean().item():.6f}\")\n",
    "print(f\"Relative error: {(error / x.abs().clamp(min=1e-6)).mean().item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Asymmetric Quantization\n",
    "\n",
    "Asymmetric quantization is better for activations (often positive):\n",
    "- Zero-point can be non-zero\n",
    "- Range is `[min, max]`\n",
    "- Uses unsigned INT8: `[0, 255]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantize(x: torch.Tensor, num_bits: int = 8) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Asymmetric quantization to unsigned integers.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (float)\n",
    "        num_bits: Number of bits (default 8)\n",
    "    \n",
    "    Returns:\n",
    "        x_quant: Quantized tensor (uint8)\n",
    "        scale: Scale factor\n",
    "        zero_point: Zero point offset\n",
    "    \"\"\"\n",
    "    qmax = 2 ** num_bits - 1  # 255 for 8-bit\n",
    "    qmin = 0\n",
    "    \n",
    "    # Find min and max of input\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    \n",
    "    # Calculate scale and zero point\n",
    "    scale = (x_max - x_min) / qmax\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if scale == 0:\n",
    "        scale = torch.tensor(1.0)\n",
    "    \n",
    "    # Zero point: where 0.0 maps to in quantized space\n",
    "    zero_point = torch.round(-x_min / scale)\n",
    "    zero_point = torch.clamp(zero_point, qmin, qmax)\n",
    "    \n",
    "    # Quantize\n",
    "    x_quant = torch.round(x / scale) + zero_point\n",
    "    x_quant = torch.clamp(x_quant, qmin, qmax).to(torch.uint8)\n",
    "    \n",
    "    return x_quant, scale, zero_point\n",
    "\n",
    "\n",
    "def asymmetric_dequantize(x_quant: torch.Tensor, scale: torch.Tensor, zero_point: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dequantize from unsigned integers back to float.\n",
    "    \n",
    "    Args:\n",
    "        x_quant: Quantized tensor (uint8)\n",
    "        scale: Scale factor\n",
    "        zero_point: Zero point offset\n",
    "    \n",
    "    Returns:\n",
    "        x_dequant: Dequantized tensor (float)\n",
    "    \"\"\"\n",
    "    return (x_quant.float() - zero_point) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test asymmetric quantization - good for ReLU activations (mostly positive)\n",
    "print(\"=== Testing Asymmetric Quantization ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Simulate ReLU activations (positive values)\n",
    "x = F.relu(torch.randn(4, 4) * 2)\n",
    "print(f\"Original tensor (post-ReLU):\")\n",
    "print(x)\n",
    "\n",
    "# Quantize\n",
    "x_quant, scale, zero_point = asymmetric_quantize(x)\n",
    "print(f\"\\nQuantized tensor (UINT8):\")\n",
    "print(x_quant)\n",
    "print(f\"\\nScale: {scale.item():.6f}\")\n",
    "print(f\"Zero point: {zero_point.item()}\")\n",
    "\n",
    "# Dequantize and check error\n",
    "x_dequant = asymmetric_dequantize(x_quant, scale, zero_point)\n",
    "print(f\"\\nDequantized tensor:\")\n",
    "print(x_dequant)\n",
    "\n",
    "error = (x - x_dequant).abs()\n",
    "print(f\"\\nMax absolute error: {error.max().item():.6f}\")\n",
    "print(f\"Mean absolute error: {error.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Per-Channel Quantization\n",
    "\n",
    "Per-tensor quantization uses a single scale for the entire tensor. Per-channel quantization uses a different scale for each output channel, giving better accuracy.\n",
    "\n",
    "This is especially important for weights where different channels may have very different magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_channel_symmetric_quantize(\n",
    "    weight: torch.Tensor, \n",
    "    axis: int = 0,\n",
    "    num_bits: int = 8\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Per-channel symmetric quantization for weights.\n",
    "    \n",
    "    Args:\n",
    "        weight: Weight tensor (out_features, in_features)\n",
    "        axis: Channel axis (0 for output channels)\n",
    "        num_bits: Number of bits\n",
    "    \n",
    "    Returns:\n",
    "        weight_quant: Quantized weights (int8)\n",
    "        scales: Per-channel scales\n",
    "    \"\"\"\n",
    "    qmax = 2 ** (num_bits - 1) - 1\n",
    "    qmin = -qmax - 1\n",
    "    \n",
    "    # Calculate per-channel max absolute values\n",
    "    # For (out_features, in_features), reduce over in_features\n",
    "    reduce_dims = [i for i in range(weight.dim()) if i != axis]\n",
    "    max_vals = weight.abs().amax(dim=reduce_dims, keepdim=True)\n",
    "    \n",
    "    # Calculate scales\n",
    "    scales = max_vals / qmax\n",
    "    scales = torch.where(scales == 0, torch.ones_like(scales), scales)\n",
    "    \n",
    "    # Quantize\n",
    "    weight_quant = torch.round(weight / scales)\n",
    "    weight_quant = torch.clamp(weight_quant, qmin, qmax).to(torch.int8)\n",
    "    \n",
    "    return weight_quant, scales.squeeze()\n",
    "\n",
    "\n",
    "def per_channel_symmetric_dequantize(\n",
    "    weight_quant: torch.Tensor,\n",
    "    scales: torch.Tensor,\n",
    "    axis: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dequantize per-channel quantized weights.\n",
    "    \"\"\"\n",
    "    # Reshape scales for broadcasting\n",
    "    shape = [1] * weight_quant.dim()\n",
    "    shape[axis] = -1\n",
    "    scales = scales.view(*shape)\n",
    "    \n",
    "    return weight_quant.float() * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare per-tensor vs per-channel quantization\n",
    "print(\"=== Per-Tensor vs Per-Channel Quantization ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Create weights with varying magnitudes per channel\n",
    "weight = torch.randn(4, 8)\n",
    "weight[0] *= 10  # First channel has much larger values\n",
    "weight[3] *= 0.1  # Last channel has much smaller values\n",
    "\n",
    "print(f\"Weight tensor (varying magnitudes per channel):\")\n",
    "print(f\"Channel 0 max: {weight[0].abs().max().item():.4f}\")\n",
    "print(f\"Channel 3 max: {weight[3].abs().max().item():.4f}\")\n",
    "\n",
    "# Per-tensor quantization\n",
    "w_quant_tensor, scale_tensor = symmetric_quantize(weight)\n",
    "w_dequant_tensor = symmetric_dequantize(w_quant_tensor, scale_tensor)\n",
    "error_tensor = (weight - w_dequant_tensor).abs()\n",
    "\n",
    "print(f\"\\n--- Per-Tensor Quantization ---\")\n",
    "print(f\"Single scale: {scale_tensor.item():.6f}\")\n",
    "print(f\"Mean error: {error_tensor.mean().item():.6f}\")\n",
    "print(f\"Max error: {error_tensor.max().item():.6f}\")\n",
    "print(f\"Channel 3 error (small values lost): {error_tensor[3].mean().item():.6f}\")\n",
    "\n",
    "# Per-channel quantization\n",
    "w_quant_channel, scales_channel = per_channel_symmetric_quantize(weight)\n",
    "w_dequant_channel = per_channel_symmetric_dequantize(w_quant_channel, scales_channel)\n",
    "error_channel = (weight - w_dequant_channel).abs()\n",
    "\n",
    "print(f\"\\n--- Per-Channel Quantization ---\")\n",
    "print(f\"Scales: {scales_channel.tolist()}\")\n",
    "print(f\"Mean error: {error_channel.mean().item():.6f}\")\n",
    "print(f\"Max error: {error_channel.max().item():.6f}\")\n",
    "print(f\"Channel 3 error (preserved!): {error_channel[3].mean().item():.6f}\")\n",
    "\n",
    "print(f\"\\n>>> Per-channel reduces error by {error_tensor.mean() / error_channel.mean():.1f}x!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Quantized Linear Layer\n",
    "\n",
    "Now let's create a quantized linear layer that stores INT8 weights but computes in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with INT8 quantized weights.\n",
    "    \n",
    "    Stores weights as INT8 to save memory.\n",
    "    Dequantizes to FP32 for computation (can be optimized with INT8 matmul kernels).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Quantized weights (INT8) - registered as buffer, not parameter\n",
    "        self.register_buffer('weight_quant', torch.zeros(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer('weight_scales', torch.zeros(out_features))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer('bias', torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_float(cls, linear: nn.Linear) -> 'QuantizedLinear':\n",
    "        \"\"\"\n",
    "        Create a quantized linear layer from a float linear layer.\n",
    "        \"\"\"\n",
    "        quant_linear = cls(linear.in_features, linear.out_features, bias=linear.bias is not None)\n",
    "        \n",
    "        # Quantize weights using per-channel quantization\n",
    "        weight_quant, scales = per_channel_symmetric_quantize(linear.weight.data)\n",
    "        quant_linear.weight_quant.copy_(weight_quant)\n",
    "        quant_linear.weight_scales.copy_(scales)\n",
    "        \n",
    "        if linear.bias is not None:\n",
    "            quant_linear.bias.copy_(linear.bias.data)\n",
    "        \n",
    "        return quant_linear\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with dequantized weights.\n",
    "        \n",
    "        Note: In production, you'd use INT8 matmul kernels for speed.\n",
    "        Here we dequantize for simplicity.\n",
    "        \"\"\"\n",
    "        # Dequantize weights\n",
    "        weight = per_channel_symmetric_dequantize(self.weight_quant, self.weight_scales)\n",
    "        \n",
    "        # Compute linear\n",
    "        return F.linear(x, weight, self.bias)\n",
    "    \n",
    "    def memory_bytes(self) -> int:\n",
    "        \"\"\"Calculate memory usage of quantized weights.\"\"\"\n",
    "        weight_bytes = self.weight_quant.numel() * 1  # INT8 = 1 byte\n",
    "        scale_bytes = self.weight_scales.numel() * 4  # FP32 = 4 bytes\n",
    "        bias_bytes = self.bias.numel() * 4 if self.bias is not None else 0\n",
    "        return weight_bytes + scale_bytes + bias_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quantized linear layer\n",
    "print(\"=== Testing Quantized Linear Layer ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "in_features = 512\n",
    "out_features = 256\n",
    "batch_size = 4\n",
    "\n",
    "# Create float linear layer\n",
    "linear_fp32 = nn.Linear(in_features, out_features)\n",
    "\n",
    "# Quantize it\n",
    "linear_int8 = QuantizedLinear.from_float(linear_fp32)\n",
    "\n",
    "# Compare memory\n",
    "fp32_bytes = linear_fp32.weight.numel() * 4 + (linear_fp32.bias.numel() * 4 if linear_fp32.bias is not None else 0)\n",
    "int8_bytes = linear_int8.memory_bytes()\n",
    "\n",
    "print(f\"FP32 linear: {fp32_bytes:,} bytes ({fp32_bytes / 1024:.1f} KB)\")\n",
    "print(f\"INT8 linear: {int8_bytes:,} bytes ({int8_bytes / 1024:.1f} KB)\")\n",
    "print(f\"Compression: {fp32_bytes / int8_bytes:.2f}x\")\n",
    "\n",
    "# Compare outputs\n",
    "x = torch.randn(batch_size, in_features)\n",
    "out_fp32 = linear_fp32(x)\n",
    "out_int8 = linear_int8(x)\n",
    "\n",
    "error = (out_fp32 - out_int8).abs()\n",
    "print(f\"\\nOutput comparison:\")\n",
    "print(f\"Max absolute error: {error.max().item():.6f}\")\n",
    "print(f\"Mean absolute error: {error.mean().item():.6f}\")\n",
    "print(f\"Relative error: {(error / out_fp32.abs().clamp(min=1e-6)).mean().item() * 100:.4f}%\")\n",
    "\n",
    "print(\"\\n✓ Quantized linear layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Quantize a Full Model\n",
    "\n",
    "Let's quantize a small MLP and see the impact on accuracy and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def quantize_model(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Quantize all linear layers in a model.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, QuantizedLinear.from_float(module))\n",
    "        else:\n",
    "            quantize_model(module)\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters_bytes(model: nn.Module) -> int:\n",
    "    \"\"\"Count total bytes for all parameters and buffers.\"\"\"\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total += param.numel() * param.element_size()\n",
    "    for name, buffer in model.named_buffers():\n",
    "        total += buffer.numel() * buffer.element_size()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and quantize model\n",
    "print(\"=== Quantizing Full Model ===\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input_dim = 784\n",
    "hidden_dim = 512\n",
    "output_dim = 10\n",
    "\n",
    "# Original model\n",
    "model_fp32 = SimpleMLP(input_dim, hidden_dim, output_dim)\n",
    "fp32_bytes = count_parameters_bytes(model_fp32)\n",
    "\n",
    "# Quantized model (copy first to preserve original)\n",
    "import copy\n",
    "model_int8 = copy.deepcopy(model_fp32)\n",
    "model_int8 = quantize_model(model_int8)\n",
    "int8_bytes = count_parameters_bytes(model_int8)\n",
    "\n",
    "print(f\"FP32 model size: {fp32_bytes:,} bytes ({fp32_bytes / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"INT8 model size: {int8_bytes:,} bytes ({int8_bytes / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"Compression ratio: {fp32_bytes / int8_bytes:.2f}x\")\n",
    "\n",
    "# Test accuracy\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, input_dim)\n",
    "\n",
    "model_fp32.eval()\n",
    "model_int8.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_fp32 = model_fp32(x)\n",
    "    out_int8 = model_int8(x)\n",
    "\n",
    "# Check if predictions match\n",
    "pred_fp32 = out_fp32.argmax(dim=1)\n",
    "pred_int8 = out_int8.argmax(dim=1)\n",
    "accuracy = (pred_fp32 == pred_int8).float().mean().item()\n",
    "\n",
    "print(f\"\\nPrediction agreement: {accuracy * 100:.1f}%\")\n",
    "print(f\"Output MSE: {F.mse_loss(out_fp32, out_int8).item():.6f}\")\n",
    "\n",
    "print(\"\\n✓ Model quantization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Quantization reduces memory** by representing weights in lower precision (INT8 = 4x smaller than FP32)\n",
    "\n",
    "2. **Symmetric quantization** is simpler, good for weights centered around zero:\n",
    "   - `scale = max(|x|) / 127`\n",
    "   - `x_quant = round(x / scale)`\n",
    "\n",
    "3. **Asymmetric quantization** is better for non-centered data (like ReLU activations):\n",
    "   - Uses both scale and zero_point\n",
    "   - Maps `[min, max]` to `[0, 255]`\n",
    "\n",
    "4. **Per-channel quantization** uses a separate scale per output channel, reducing quantization error significantly\n",
    "\n",
    "5. **Trade-offs:**\n",
    "   - Memory: 4x reduction with INT8\n",
    "   - Accuracy: Small degradation, usually <1% for well-tuned quantization\n",
    "   - Speed: Can be faster with INT8 kernels (not implemented here)\n",
    "\n",
    "---\n",
    "\n",
    "## Interview Tips\n",
    "\n",
    "**Q: What is the difference between symmetric and asymmetric quantization?**\n",
    "A: Symmetric uses zero as the center point and maps to signed integers [-128, 127]. Asymmetric uses a zero_point offset and maps to unsigned integers [0, 255]. Symmetric is simpler and better for weights; asymmetric is better for activations that are often positive.\n",
    "\n",
    "**Q: Why is per-channel quantization better than per-tensor?**\n",
    "A: Different channels can have very different value ranges. Per-tensor uses one scale for all, so small-magnitude channels lose precision. Per-channel gives each channel its own scale, preserving precision.\n",
    "\n",
    "**Q: What is the memory savings of INT8 quantization?**\n",
    "A: Theoretically 4x (32 bits → 8 bits). In practice ~3.5-3.8x due to scale storage overhead.\n",
    "\n",
    "**Q: When does quantization fail?**\n",
    "A: When weights have outliers (very large values) that dominate the scale. Solutions: clip outliers, use mixed precision for sensitive layers, or use techniques like SmoothQuant.\n",
    "\n",
    "**Q: What is dynamic vs static quantization?**\n",
    "A: Static: calibrate scales once using calibration data. Dynamic: compute scales on-the-fly for each input. Dynamic is simpler but slower.\n",
    "\n",
    "**Q: What is GPTQ/AWQ?**\n",
    "A: Advanced weight quantization methods that minimize reconstruction error by considering correlations between weights. GPTQ uses Hessian-based optimization; AWQ protects important weights based on activation magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) (Jacob et al., 2018)\n",
    "- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339) (Dettmers et al., 2022)\n",
    "- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (Frantar et al., 2022)\n",
    "- [AWQ: Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978) (Lin et al., 2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
