{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-Judge: Evaluation Pipeline from Scratch\n",
    "\n",
    "## Background\n",
    "\n",
    "**LLM-as-Judge** is a paradigm where a language model evaluates the outputs of other language models. This is central to companies like Vals AI that build LLM benchmarking infrastructure.\n",
    "\n",
    "### Why LLM-as-Judge?\n",
    "\n",
    "Traditional evaluation methods (BLEU, ROUGE, exact match) fail to capture semantic correctness for open-ended generation. Human evaluation is expensive and slow. LLM-as-Judge sits in between: cheaper than humans, more nuanced than string matching.\n",
    "\n",
    "### Key Challenges\n",
    "\n",
    "1. **Calibration**: Does the judge's score distribution match human expectations?\n",
    "2. **Consistency**: Does the same input get the same score across runs / prompt variants?\n",
    "3. **Bias**: Position bias, verbosity bias, self-preference bias\n",
    "4. **Cost/Accuracy tradeoff**: Expensive judges are more accurate but slower\n",
    "\n",
    "### What We Build\n",
    "\n",
    "This notebook implements a complete LLM-as-Judge evaluation pipeline:\n",
    "1. Rubric-based scoring with structured prompts\n",
    "2. Batch evaluation over synthetic datasets\n",
    "3. Inter-rater agreement metrics (Cohen's kappa, confusion matrices, F1) from scratch\n",
    "4. Judge sensitivity analysis (flip rate, Krippendorff's alpha)\n",
    "5. Cost/latency/accuracy tradeoff analysis with Pareto frontiers\n",
    "\n",
    "**Note**: All judge scoring uses deterministic mock judges (no API keys, no external services).\n",
    "\n",
    "### References\n",
    "- [Zheng et al. - Judging LLM-as-a-Judge (2023)](https://arxiv.org/abs/2306.05685)\n",
    "- [Cohen's Kappa on Wikipedia](https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n",
    "- [Krippendorff's Alpha on Wikipedia](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Rubric-Based Scoring (Foundations)\n",
    "\n",
    "The foundation of LLM-as-Judge is a well-structured rubric that tells the judge *how* to evaluate. A rubric consists of:\n",
    "- A system prompt defining the judge's role\n",
    "- Criteria to evaluate against\n",
    "- A scoring scale with clear anchors\n",
    "- A structured output format for reliable parsing\n",
    "\n",
    "We define dataclasses for the rubric, the judge's result, and evaluation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JudgePrompt:\n",
    "    \"\"\"Configuration for an LLM judge's evaluation prompt.\"\"\"\n",
    "    system_prompt: str\n",
    "    rubric_criteria: list[str]\n",
    "    scoring_scale: tuple[int, int]  # (min_score, max_score)\n",
    "    output_format: str  # e.g. \"Score: {score}\\nReasoning: {reasoning}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JudgeResult:\n",
    "    \"\"\"Output from a judge evaluation.\"\"\"\n",
    "    score: int\n",
    "    reasoning: str\n",
    "    raw_response: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalSample:\n",
    "    \"\"\"A single evaluation sample with ground truth.\"\"\"\n",
    "    question: str\n",
    "    reference_answer: str\n",
    "    model_output: str\n",
    "    human_label: int  # 1 = correct, 0 = incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_judge_prompt(\n",
    "    question: str,\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    rubric: JudgePrompt,\n",
    ") -> str:\n",
    "    \"\"\"Render a full grading prompt from the rubric template.\n",
    "\n",
    "    Combines the system prompt, rubric criteria, scoring scale,\n",
    "    and the actual question/reference/candidate into a single\n",
    "    prompt string ready for the judge.\n",
    "    \"\"\"\n",
    "    criteria_str = \"\\n\".join(f\"  - {c}\" for c in rubric.rubric_criteria)\n",
    "    min_s, max_s = rubric.scoring_scale\n",
    "\n",
    "    prompt = (\n",
    "        f\"{rubric.system_prompt}\\n\\n\"\n",
    "        f\"## Evaluation Criteria\\n{criteria_str}\\n\\n\"\n",
    "        f\"## Scoring Scale\\n\"\n",
    "        f\"Rate from {min_s} (worst) to {max_s} (best).\\n\\n\"\n",
    "        f\"## Question\\n{question}\\n\\n\"\n",
    "        f\"## Reference Answer\\n{reference}\\n\\n\"\n",
    "        f\"## Candidate Answer\\n{candidate}\\n\\n\"\n",
    "        f\"## Output Format\\n{rubric.output_format}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def parse_judge_response(response_text: str) -> JudgeResult:\n",
    "    \"\"\"Extract score and reasoning from structured judge output.\n",
    "\n",
    "    Expects format like:\n",
    "        Score: 4\n",
    "        Reasoning: The answer is mostly correct but...\n",
    "\n",
    "    Falls back to score=0 with error reasoning if parsing fails.\n",
    "    \"\"\"\n",
    "    score_match = re.search(r\"Score:\\s*(\\d+)\", response_text)\n",
    "    reasoning_match = re.search(r\"Reasoning:\\s*(.+)\", response_text, re.DOTALL)\n",
    "\n",
    "    if score_match is None:\n",
    "        return JudgeResult(\n",
    "            score=0,\n",
    "            reasoning=\"Failed to parse score from response\",\n",
    "            raw_response=response_text,\n",
    "        )\n",
    "\n",
    "    score = int(score_match.group(1))\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"No reasoning provided\"\n",
    "\n",
    "    return JudgeResult(score=score, reasoning=reasoning, raw_response=response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tests for Part 1 -----\n",
    "\n",
    "# Test JudgePrompt construction\n",
    "rubric = JudgePrompt(\n",
    "    system_prompt=\"You are a math grading assistant.\",\n",
    "    rubric_criteria=[\"Correctness of final answer\", \"Clarity of reasoning\"],\n",
    "    scoring_scale=(1, 5),\n",
    "    output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    ")\n",
    "assert rubric.scoring_scale == (1, 5)\n",
    "assert len(rubric.rubric_criteria) == 2\n",
    "\n",
    "# Test format_judge_prompt round-trip\n",
    "prompt = format_judge_prompt(\n",
    "    question=\"What is 2 + 2?\",\n",
    "    reference=\"4\",\n",
    "    candidate=\"4\",\n",
    "    rubric=rubric,\n",
    ")\n",
    "assert \"What is 2 + 2?\" in prompt\n",
    "assert \"Reference Answer\" in prompt\n",
    "assert \"Candidate Answer\" in prompt\n",
    "assert \"Correctness of final answer\" in prompt\n",
    "assert \"1 (worst) to 5 (best)\" in prompt\n",
    "print(\"Format test passed.\")\n",
    "\n",
    "# Test parse_judge_response - well-formed\n",
    "good_response = \"Score: 4\\nReasoning: The answer is correct and clearly stated.\"\n",
    "result = parse_judge_response(good_response)\n",
    "assert result.score == 4\n",
    "assert \"correct\" in result.reasoning\n",
    "assert result.raw_response == good_response\n",
    "print(\"Parse (good) test passed.\")\n",
    "\n",
    "# Test parse_judge_response - malformed (no score)\n",
    "bad_response = \"I think this is pretty good overall.\"\n",
    "result_bad = parse_judge_response(bad_response)\n",
    "assert result_bad.score == 0\n",
    "assert \"Failed to parse\" in result_bad.reasoning\n",
    "print(\"Parse (malformed) test passed.\")\n",
    "\n",
    "# Test parse_judge_response - score but no reasoning\n",
    "partial_response = \"Score: 3\"\n",
    "result_partial = parse_judge_response(partial_response)\n",
    "assert result_partial.score == 3\n",
    "assert result_partial.reasoning == \"No reasoning provided\"\n",
    "print(\"Parse (partial) test passed.\")\n",
    "\n",
    "print(\"\\nAll Part 1 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Evaluation Dataset & Batch Scoring\n",
    "\n",
    "We generate a synthetic math QA dataset with known correct/incorrect answers, then build a deterministic mock judge that scores based on string similarity.\n",
    "\n",
    "This simulates the real pipeline: dataset of (question, reference, candidate) triples with human labels, scored by an automated judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_math_dataset(n: int = 50, seed: int = 42) -> list[EvalSample]:\n",
    "    \"\"\"Generate synthetic math QA samples with known correctness.\n",
    "\n",
    "    Creates multiplication problems. Model outputs are either:\n",
    "    - Exact correct answer (human_label=1)\n",
    "    - Close but wrong answer (human_label=0)\n",
    "    - Completely wrong answer (human_label=0)\n",
    "    - Correct answer with extra text (human_label=1)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    samples: list[EvalSample] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        a = int(rng.randint(2, 50))\n",
    "        b = int(rng.randint(2, 50))\n",
    "        correct = a * b\n",
    "        question = f\"What is {a} * {b}?\"\n",
    "        reference = str(correct)\n",
    "\n",
    "        case = rng.choice([\"exact\", \"close_wrong\", \"far_wrong\", \"verbose_correct\"], p=[0.35, 0.25, 0.2, 0.2])\n",
    "\n",
    "        if case == \"exact\":\n",
    "            model_output = str(correct)\n",
    "            human_label = 1\n",
    "        elif case == \"close_wrong\":\n",
    "            offset = int(rng.choice([-2, -1, 1, 2]))\n",
    "            model_output = str(correct + offset)\n",
    "            human_label = 0\n",
    "        elif case == \"far_wrong\":\n",
    "            model_output = str(int(rng.randint(1, 100)))\n",
    "            # Unlikely but possible that random number equals correct\n",
    "            human_label = 1 if model_output == reference else 0\n",
    "        else:  # verbose_correct\n",
    "            model_output = f\"The answer is {correct}.\"\n",
    "            human_label = 1\n",
    "\n",
    "        samples.append(EvalSample(\n",
    "            question=question,\n",
    "            reference_answer=reference,\n",
    "            model_output=model_output,\n",
    "            human_label=human_label,\n",
    "        ))\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "dataset = generate_math_dataset(n=50)\n",
    "n_correct = sum(s.human_label for s in dataset)\n",
    "print(f\"Generated {len(dataset)} samples: {n_correct} correct, {len(dataset) - n_correct} incorrect\")\n",
    "print(f\"\\nExample samples:\")\n",
    "for s in dataset[:5]:\n",
    "    print(f\"  Q: {s.question}  Ref: {s.reference_answer}  Model: {s.model_output}  Label: {s.human_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockJudge:\n",
    "    \"\"\"Deterministic judge that scores based on string similarity.\n",
    "\n",
    "    Scoring logic:\n",
    "      - Exact match between model output and reference -> 5\n",
    "      - Reference string is contained in model output (verbose correct) -> 4\n",
    "      - Model output is numerically close (within 5) to reference -> 3\n",
    "      - Otherwise -> 1\n",
    "\n",
    "    This simulates an LLM judge without any API calls.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"default\"):\n",
    "        self.name = name\n",
    "\n",
    "    def score(self, sample: EvalSample, rubric: JudgePrompt) -> JudgeResult:\n",
    "        ref = sample.reference_answer.strip()\n",
    "        out = sample.model_output.strip()\n",
    "\n",
    "        if out == ref:\n",
    "            s = 5\n",
    "            reasoning = \"Exact match with reference answer.\"\n",
    "        elif ref in out:\n",
    "            s = 4\n",
    "            reasoning = \"Reference answer found within model output (verbose but correct).\"\n",
    "        else:\n",
    "            # Try numeric comparison\n",
    "            try:\n",
    "                # Extract first number from model output\n",
    "                out_nums = re.findall(r\"-?\\d+\", out)\n",
    "                ref_num = int(ref)\n",
    "                if out_nums and abs(int(out_nums[0]) - ref_num) <= 5:\n",
    "                    s = 3\n",
    "                    reasoning = f\"Numerically close (off by {abs(int(out_nums[0]) - ref_num)}).\"\n",
    "                else:\n",
    "                    s = 1\n",
    "                    reasoning = \"Incorrect answer.\"\n",
    "            except (ValueError, IndexError):\n",
    "                s = 1\n",
    "                reasoning = \"Could not parse numeric answer.\"\n",
    "\n",
    "        raw = f\"Score: {s}\\nReasoning: {reasoning}\"\n",
    "        return JudgeResult(score=s, reasoning=reasoning, raw_response=raw)\n",
    "\n",
    "\n",
    "def run_judge(\n",
    "    samples: list[EvalSample],\n",
    "    judge_fn: Callable[[EvalSample, JudgePrompt], JudgeResult],\n",
    "    rubric: JudgePrompt,\n",
    ") -> list[JudgeResult]:\n",
    "    \"\"\"Batch-score a list of samples using the given judge function.\"\"\"\n",
    "    return [judge_fn(s, rubric) for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tests for Part 2 -----\n",
    "\n",
    "rubric = JudgePrompt(\n",
    "    system_prompt=\"You are a math grading assistant.\",\n",
    "    rubric_criteria=[\"Correctness of final answer\"],\n",
    "    scoring_scale=(1, 5),\n",
    "    output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    ")\n",
    "\n",
    "judge = MockJudge()\n",
    "results = run_judge(dataset, judge.score, rubric)\n",
    "\n",
    "# Check output length\n",
    "assert len(results) == len(dataset), f\"Expected {len(dataset)} results, got {len(results)}\"\n",
    "print(f\"Scored {len(results)} samples.\")\n",
    "\n",
    "# Check all scores in valid range\n",
    "min_s, max_s = rubric.scoring_scale\n",
    "for r in results:\n",
    "    assert min_s <= r.score <= max_s, f\"Score {r.score} out of range [{min_s}, {max_s}]\"\n",
    "print(f\"All scores in range [{min_s}, {max_s}].\")\n",
    "\n",
    "# Check score distribution\n",
    "scores = [r.score for r in results]\n",
    "print(f\"\\nScore distribution:\")\n",
    "for s in sorted(set(scores)):\n",
    "    count = scores.count(s)\n",
    "    print(f\"  Score {s}: {count} samples ({count/len(scores)*100:.0f}%)\")\n",
    "\n",
    "# Verify exact match samples get score 5\n",
    "for sample, result in zip(dataset, results):\n",
    "    if sample.model_output.strip() == sample.reference_answer.strip():\n",
    "        assert result.score == 5, f\"Exact match should get 5, got {result.score}\"\n",
    "print(\"\\nExact-match scoring verified.\")\n",
    "\n",
    "print(\"\\nAll Part 2 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Inter-Rater Agreement (Statistical Rigor)\n",
    "\n",
    "The core question: **does the judge agree with humans?**\n",
    "\n",
    "We implement agreement metrics from scratch, then validate against sklearn.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "- **Confusion Matrix**: Counts of (predicted, actual) pairs\n",
    "- **Accuracy**: Fraction of correct predictions\n",
    "- **Precision / Recall / F1**: For binary classification\n",
    "- **Cohen's Kappa**: Agreement adjusted for chance\n",
    "  - $\\kappa = \\frac{p_o - p_e}{1 - p_e}$ where $p_o$ = observed agreement, $p_e$ = expected by chance\n",
    "  - $\\kappa = 1$: perfect agreement; $\\kappa = 0$: chance agreement; $\\kappa < 0$: worse than chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels_a: list[int], labels_b: list[int], num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Build confusion matrix from two raters' labels.\n",
    "\n",
    "    Entry cm[i][j] = number of items where rater A said i and rater B said j.\n",
    "    \"\"\"\n",
    "    assert len(labels_a) == len(labels_b), \"Label lists must have same length\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for a, b in zip(labels_a, labels_b):\n",
    "        cm[a][b] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def cohens_kappa(rater1: list[int], rater2: list[int]) -> float:\n",
    "    \"\"\"Cohen's kappa: (p_o - p_e) / (1 - p_e).\n",
    "\n",
    "    Measures inter-rater agreement adjusted for chance.\n",
    "    \"\"\"\n",
    "    assert len(rater1) == len(rater2), \"Rater lists must have same length\"\n",
    "    n = len(rater1)\n",
    "    classes = sorted(set(rater1) | set(rater2))\n",
    "\n",
    "    # p_o = observed agreement\n",
    "    p_o = sum(a == b for a, b in zip(rater1, rater2)) / n\n",
    "\n",
    "    # p_e = expected agreement by chance\n",
    "    p_e = sum(\n",
    "        (rater1.count(c) / n) * (rater2.count(c) / n)\n",
    "        for c in classes\n",
    "    )\n",
    "\n",
    "    if p_e == 1.0:\n",
    "        return 1.0\n",
    "    return (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "\n",
    "def accuracy(predicted: list[int], actual: list[int]) -> float:\n",
    "    \"\"\"Fraction of predictions that match actual labels.\"\"\"\n",
    "    assert len(predicted) == len(actual), \"Lists must have same length\"\n",
    "    return sum(p == a for p, a in zip(predicted, actual)) / len(actual)\n",
    "\n",
    "\n",
    "def precision_recall_f1(\n",
    "    predicted: list[int],\n",
    "    actual: list[int],\n",
    "    positive_label: int = 1,\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"Compute precision, recall, and F1 for binary classification.\"\"\"\n",
    "    tp = sum(p == positive_label and a == positive_label for p, a in zip(predicted, actual))\n",
    "    fp = sum(p == positive_label and a != positive_label for p, a in zip(predicted, actual))\n",
    "    fn = sum(p != positive_label and a == positive_label for p, a in zip(predicted, actual))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert judge scores to binary labels for comparison with human labels\n",
    "# Threshold: score >= 4 -> 1 (correct), else -> 0 (incorrect)\n",
    "SCORE_THRESHOLD = 4\n",
    "\n",
    "judge_labels = [1 if r.score >= SCORE_THRESHOLD else 0 for r in results]\n",
    "human_labels = [s.human_label for s in dataset]\n",
    "\n",
    "# Compute all metrics\n",
    "cm = confusion_matrix(human_labels, judge_labels, num_classes=2)\n",
    "kappa = cohens_kappa(human_labels, judge_labels)\n",
    "acc = accuracy(judge_labels, human_labels)\n",
    "prec, rec, f1 = precision_recall_f1(judge_labels, human_labels, positive_label=1)\n",
    "\n",
    "print(\"Agreement between Mock Judge and Human Labels\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nConfusion Matrix (rows=human, cols=judge):\")\n",
    "print(f\"              Judge=0  Judge=1\")\n",
    "print(f\"  Human=0     {cm[0,0]:5d}    {cm[0,1]:5d}\")\n",
    "print(f\"  Human=1     {cm[1,0]:5d}    {cm[1,1]:5d}\")\n",
    "print(f\"\\nAccuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tests for Part 3: validate against sklearn -----\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score as sk_kappa,\n",
    "    confusion_matrix as sk_cm,\n",
    "    accuracy_score as sk_acc,\n",
    "    precision_recall_fscore_support as sk_prfs,\n",
    ")\n",
    "\n",
    "# Test confusion matrix\n",
    "sk_cm_result = sk_cm(human_labels, judge_labels)\n",
    "assert np.array_equal(cm, sk_cm_result), (\n",
    "    f\"Confusion matrix mismatch:\\nOurs:\\n{cm}\\nsklearn:\\n{sk_cm_result}\"\n",
    ")\n",
    "print(\"Confusion matrix matches sklearn.\")\n",
    "\n",
    "# Test Cohen's kappa\n",
    "sk_kappa_val = sk_kappa(human_labels, judge_labels)\n",
    "assert abs(kappa - sk_kappa_val) < 1e-10, (\n",
    "    f\"Kappa mismatch: ours={kappa:.6f}, sklearn={sk_kappa_val:.6f}\"\n",
    ")\n",
    "print(f\"Cohen's kappa matches sklearn: {kappa:.6f}\")\n",
    "\n",
    "# Test accuracy\n",
    "sk_acc_val = sk_acc(human_labels, judge_labels)\n",
    "assert abs(acc - sk_acc_val) < 1e-10, f\"Accuracy mismatch: {acc} vs {sk_acc_val}\"\n",
    "print(f\"Accuracy matches sklearn: {acc:.6f}\")\n",
    "\n",
    "# Test precision/recall/F1\n",
    "sk_prec, sk_rec, sk_f1, _ = sk_prfs(human_labels, judge_labels, pos_label=1, average=\"binary\")\n",
    "assert abs(prec - sk_prec) < 1e-10, f\"Precision mismatch: {prec} vs {sk_prec}\"\n",
    "assert abs(rec - sk_rec) < 1e-10, f\"Recall mismatch: {rec} vs {sk_rec}\"\n",
    "assert abs(f1 - sk_f1) < 1e-10, f\"F1 mismatch: {f1} vs {sk_f1}\"\n",
    "print(f\"Precision/Recall/F1 matches sklearn.\")\n",
    "\n",
    "# Test perfect agreement edge case\n",
    "perfect_a = [0, 0, 1, 1, 0, 1]\n",
    "perfect_b = [0, 0, 1, 1, 0, 1]\n",
    "assert cohens_kappa(perfect_a, perfect_b) == 1.0, \"Perfect agreement should give kappa=1.0\"\n",
    "print(\"Perfect agreement kappa=1.0 verified.\")\n",
    "\n",
    "print(\"\\nAll Part 3 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Judge Sensitivity Analysis\n",
    "\n",
    "A reliable judge should give consistent scores regardless of minor prompt variations. In practice, LLM judges are sensitive to:\n",
    "- Rubric wording\n",
    "- System prompt tone\n",
    "- Scoring scale presentation\n",
    "\n",
    "We test sensitivity by running multiple prompt variants and measuring:\n",
    "- **Flip rate**: fraction of samples that change binary label across variants\n",
    "- **Score variance per sample**: how much each sample's score varies\n",
    "- **Krippendorff's alpha**: inter-rater reliability across all variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyMockJudge:\n",
    "    \"\"\"Mock judge with slight per-variant randomness to simulate sensitivity.\n",
    "\n",
    "    Uses a base MockJudge score and adds small deterministic noise\n",
    "    based on the variant_id and sample index. This simulates the\n",
    "    prompt-sensitivity of real LLM judges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variant_id: int, noise_level: float = 0.3):\n",
    "        self.variant_id = variant_id\n",
    "        self.noise_level = noise_level\n",
    "        self.base_judge = MockJudge(name=f\"variant_{variant_id}\")\n",
    "        self._rng = np.random.RandomState(seed=variant_id * 1000)\n",
    "\n",
    "    def score(self, sample: EvalSample, rubric: JudgePrompt, sample_idx: int = 0) -> JudgeResult:\n",
    "        base_result = self.base_judge.score(sample, rubric)\n",
    "        # Add noise: deterministic per (variant_id, sample_idx)\n",
    "        noise_rng = np.random.RandomState(seed=self.variant_id * 10000 + sample_idx)\n",
    "        noise = int(np.round(noise_rng.normal(0, self.noise_level * 2)))\n",
    "        min_s, max_s = rubric.scoring_scale\n",
    "        noisy_score = int(np.clip(base_result.score + noise, min_s, max_s))\n",
    "\n",
    "        raw = f\"Score: {noisy_score}\\nReasoning: {base_result.reasoning} [variant {self.variant_id}]\"\n",
    "        return JudgeResult(score=noisy_score, reasoning=base_result.reasoning, raw_response=raw)\n",
    "\n",
    "\n",
    "# Define 5 prompt variants (different rubric wordings)\n",
    "prompt_variants = [\n",
    "    JudgePrompt(\n",
    "        system_prompt=\"You are a strict math grader. Only exact answers get full marks.\",\n",
    "        rubric_criteria=[\"Exact correctness of the numerical answer\"],\n",
    "        scoring_scale=(1, 5),\n",
    "        output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    "    ),\n",
    "    JudgePrompt(\n",
    "        system_prompt=\"You are a lenient math tutor. Give credit for effort and partial answers.\",\n",
    "        rubric_criteria=[\"Approximate correctness\", \"Shows understanding of the problem\"],\n",
    "        scoring_scale=(1, 5),\n",
    "        output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    "    ),\n",
    "    JudgePrompt(\n",
    "        system_prompt=\"Evaluate the mathematical response objectively.\",\n",
    "        rubric_criteria=[\"Correctness\", \"Completeness\"],\n",
    "        scoring_scale=(1, 5),\n",
    "        output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    "    ),\n",
    "    JudgePrompt(\n",
    "        system_prompt=\"You are an exam proctor. Grade precisely.\",\n",
    "        rubric_criteria=[\"Final answer matches expected result exactly\"],\n",
    "        scoring_scale=(1, 5),\n",
    "        output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    "    ),\n",
    "    JudgePrompt(\n",
    "        system_prompt=\"Rate how well the student answered the math question.\",\n",
    "        rubric_criteria=[\"Answer quality\", \"Numerical accuracy\"],\n",
    "        scoring_scale=(1, 5),\n",
    "        output_format=\"Score: {score}\\nReasoning: {reasoning}\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run all variants\n",
    "n_variants = len(prompt_variants)\n",
    "n_samples = len(dataset)\n",
    "# results_matrix[i, j] = score from variant i on sample j\n",
    "results_matrix = np.zeros((n_variants, n_samples), dtype=int)\n",
    "\n",
    "for v_idx, rubric_v in enumerate(prompt_variants):\n",
    "    noisy_judge = NoisyMockJudge(variant_id=v_idx, noise_level=0.3)\n",
    "    for s_idx, sample in enumerate(dataset):\n",
    "        result = noisy_judge.score(sample, rubric_v, sample_idx=s_idx)\n",
    "        results_matrix[v_idx, s_idx] = result.score\n",
    "\n",
    "print(f\"Results matrix shape: {results_matrix.shape} (variants x samples)\")\n",
    "print(f\"Score range: [{results_matrix.min()}, {results_matrix.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_rate(results_matrix: np.ndarray, threshold: int = 4) -> float:\n",
    "    \"\"\"Fraction of samples that change binary label across variants.\n",
    "\n",
    "    A sample 'flips' if at least one variant labels it differently\n",
    "    (above/below threshold) from at least one other variant.\n",
    "    \"\"\"\n",
    "    n_variants, n_samples = results_matrix.shape\n",
    "    binary = (results_matrix >= threshold).astype(int)  # (n_variants, n_samples)\n",
    "    # A sample flips if not all variants agree\n",
    "    flips = 0\n",
    "    for j in range(n_samples):\n",
    "        labels_for_sample = binary[:, j]\n",
    "        if labels_for_sample.min() != labels_for_sample.max():\n",
    "            flips += 1\n",
    "    return flips / n_samples\n",
    "\n",
    "\n",
    "def score_variance_per_sample(results_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Variance of scores per sample across all variants.\n",
    "\n",
    "    Returns array of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Variance along axis 0 (across variants) for each sample\n",
    "    return np.var(results_matrix, axis=0)\n",
    "\n",
    "\n",
    "def krippendorff_alpha(results_matrix: np.ndarray) -> float:\n",
    "    \"\"\"Krippendorff's alpha for interval data.\n",
    "\n",
    "    alpha = 1 - (observed_disagreement / expected_disagreement)\n",
    "\n",
    "    For interval metric:\n",
    "      - observed = mean squared difference within each unit (sample)\n",
    "      - expected = overall variance of all ratings\n",
    "\n",
    "    Handles missing data: any NaN entries are excluded.\n",
    "    \"\"\"\n",
    "    n_raters, n_units = results_matrix.shape\n",
    "\n",
    "    # Observed disagreement: average pairwise squared difference within each unit\n",
    "    observed_sum = 0.0\n",
    "    n_pairs_total = 0\n",
    "\n",
    "    for j in range(n_units):\n",
    "        ratings = results_matrix[:, j].astype(float)\n",
    "        # Remove NaN if any\n",
    "        ratings = ratings[~np.isnan(ratings)]\n",
    "        m = len(ratings)\n",
    "        if m < 2:\n",
    "            continue\n",
    "        # Sum of squared pairwise differences\n",
    "        for i in range(m):\n",
    "            for k in range(i + 1, m):\n",
    "                observed_sum += (ratings[i] - ratings[k]) ** 2\n",
    "                n_pairs_total += 1\n",
    "\n",
    "    if n_pairs_total == 0:\n",
    "        return 1.0  # No data to disagree on\n",
    "\n",
    "    D_o = observed_sum / n_pairs_total\n",
    "\n",
    "    # Expected disagreement: pairwise squared differences across ALL ratings\n",
    "    all_ratings = results_matrix.flatten().astype(float)\n",
    "    all_ratings = all_ratings[~np.isnan(all_ratings)]\n",
    "    n_total = len(all_ratings)\n",
    "\n",
    "    expected_sum = 0.0\n",
    "    n_expected_pairs = 0\n",
    "    # Efficient computation: Var = E[X^2] - E[X]^2, and\n",
    "    # mean pairwise squared diff = 2 * Var\n",
    "    D_e = 2.0 * np.var(all_ratings)\n",
    "\n",
    "    if D_e == 0.0:\n",
    "        return 1.0  # All ratings identical\n",
    "\n",
    "    alpha = 1.0 - (D_o / D_e)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sensitivity metrics\n",
    "fr = flip_rate(results_matrix, threshold=SCORE_THRESHOLD)\n",
    "sv = score_variance_per_sample(results_matrix)\n",
    "alpha = krippendorff_alpha(results_matrix)\n",
    "\n",
    "print(\"Judge Sensitivity Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Flip rate (threshold={SCORE_THRESHOLD}): {fr:.4f} ({fr*100:.1f}% of samples change label)\")\n",
    "print(f\"Mean score variance per sample: {sv.mean():.4f}\")\n",
    "print(f\"Max score variance per sample:  {sv.max():.4f}\")\n",
    "print(f\"Krippendorff's alpha: {alpha:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if alpha > 0.8:\n",
    "    print(f\"  alpha={alpha:.2f} > 0.80: Good reliability.\")\n",
    "elif alpha > 0.667:\n",
    "    print(f\"  alpha={alpha:.2f} > 0.667: Acceptable for tentative conclusions.\")\n",
    "else:\n",
    "    print(f\"  alpha={alpha:.2f} < 0.667: Low reliability, results should be treated cautiously.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: heatmap of per-sample scores across variants (Tufte style)\n",
    "fig, ax = plt.subplots(figsize=(14, 3.5))\n",
    "\n",
    "im = ax.imshow(results_matrix, aspect=\"auto\", cmap=\"YlOrRd\", vmin=1, vmax=5)\n",
    "\n",
    "ax.set_xlabel(\"Sample Index\", fontsize=10)\n",
    "ax.set_ylabel(\"Prompt Variant\", fontsize=10)\n",
    "ax.set_yticks(range(n_variants))\n",
    "ax.set_yticklabels([f\"V{i}\" for i in range(n_variants)], fontsize=9)\n",
    "ax.set_title(\"Judge Scores by Prompt Variant\", fontsize=12, pad=10)\n",
    "\n",
    "# Minimal colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.8, pad=0.02)\n",
    "cbar.set_label(\"Score\", fontsize=9)\n",
    "cbar.set_ticks([1, 2, 3, 4, 5])\n",
    "\n",
    "# Remove chart junk (Tufte style)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.tick_params(length=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tests for Part 4 -----\n",
    "\n",
    "# Test flip_rate bounds\n",
    "assert 0.0 <= fr <= 1.0, f\"Flip rate {fr} out of [0, 1]\"\n",
    "print(f\"Flip rate in valid range: {fr:.4f}\")\n",
    "\n",
    "# Test flip_rate with identical results -> 0.0\n",
    "identical = np.full((3, 10), 5)\n",
    "assert flip_rate(identical) == 0.0, \"Identical results should have flip_rate=0\"\n",
    "print(\"Flip rate = 0.0 for identical results: verified.\")\n",
    "\n",
    "# Test alpha bounds\n",
    "assert -1.0 <= alpha <= 1.0, f\"Alpha {alpha} out of [-1, 1]\"\n",
    "print(f\"Krippendorff's alpha in valid range: {alpha:.4f}\")\n",
    "\n",
    "# Test alpha = 1.0 for identical results\n",
    "alpha_perfect = krippendorff_alpha(identical)\n",
    "assert alpha_perfect == 1.0, f\"Identical results should give alpha=1.0, got {alpha_perfect}\"\n",
    "print(\"Alpha = 1.0 for identical results: verified.\")\n",
    "\n",
    "# Test score_variance_per_sample shape\n",
    "assert sv.shape == (n_samples,), f\"Expected shape ({n_samples},), got {sv.shape}\"\n",
    "assert np.all(sv >= 0), \"Variance must be non-negative\"\n",
    "print(f\"Score variance shape and non-negativity: verified.\")\n",
    "\n",
    "# Test score_variance = 0 for identical results\n",
    "sv_identical = score_variance_per_sample(identical)\n",
    "assert np.all(sv_identical == 0.0), \"Identical results should have zero variance\"\n",
    "print(\"Variance = 0 for identical results: verified.\")\n",
    "\n",
    "print(\"\\nAll Part 4 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Cost / Latency / Accuracy Tradeoffs\n",
    "\n",
    "In production, you must choose between judge quality and cost. The key insight: **you can often get 90% of the accuracy at 10% of the cost** using smart strategies like majority voting with cheap models.\n",
    "\n",
    "We simulate three evaluation strategies and analyze their Pareto efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalReport:\n",
    "    \"\"\"Summary of an evaluation strategy's performance and cost.\"\"\"\n",
    "    model_name: str\n",
    "    accuracy: float\n",
    "    cost_per_eval: float       # dollars\n",
    "    latency_per_eval: float    # seconds\n",
    "    throughput: float           # evals per second\n",
    "    accuracy_per_dollar: float  # accuracy / cost\n",
    "\n",
    "\n",
    "class CheapMockJudge:\n",
    "    \"\"\"Lower-accuracy judge simulating a cheap/fast model.\n",
    "\n",
    "    Has a fixed error rate: randomly flips some correct/incorrect decisions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, error_rate: float = 0.22, seed: int = 0):\n",
    "        self.error_rate = error_rate\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        self.base_judge = MockJudge(name=\"cheap\")\n",
    "\n",
    "    def score(self, sample: EvalSample, rubric: JudgePrompt) -> JudgeResult:\n",
    "        base = self.base_judge.score(sample, rubric)\n",
    "        # Randomly flip score between high and low\n",
    "        if self._rng.random() < self.error_rate:\n",
    "            flipped_score = 1 if base.score >= 4 else 5\n",
    "            raw = f\"Score: {flipped_score}\\nReasoning: {base.reasoning} [flipped]\"\n",
    "            return JudgeResult(score=flipped_score, reasoning=base.reasoning, raw_response=raw)\n",
    "        return base\n",
    "\n",
    "\n",
    "class ExpensiveMockJudge:\n",
    "    \"\"\"High-accuracy judge simulating an expensive/slow model.\n",
    "\n",
    "    Very low error rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, error_rate: float = 0.05, seed: int = 100):\n",
    "        self.error_rate = error_rate\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        self.base_judge = MockJudge(name=\"expensive\")\n",
    "\n",
    "    def score(self, sample: EvalSample, rubric: JudgePrompt) -> JudgeResult:\n",
    "        base = self.base_judge.score(sample, rubric)\n",
    "        if self._rng.random() < self.error_rate:\n",
    "            flipped_score = 1 if base.score >= 4 else 5\n",
    "            raw = f\"Score: {flipped_score}\\nReasoning: {base.reasoning} [flipped]\"\n",
    "            return JudgeResult(score=flipped_score, reasoning=base.reasoning, raw_response=raw)\n",
    "        return base\n",
    "\n",
    "\n",
    "def majority_vote_labels(\n",
    "    all_results: list[list[JudgeResult]],\n",
    "    threshold: int = 4,\n",
    ") -> list[int]:\n",
    "    \"\"\"Compute majority vote binary labels from multiple judge runs.\n",
    "\n",
    "    Each inner list is one run's results. Returns binary labels (0/1)\n",
    "    based on majority of runs agreeing above/below threshold.\n",
    "    \"\"\"\n",
    "    n_samples = len(all_results[0])\n",
    "    n_runs = len(all_results)\n",
    "    labels = []\n",
    "    for j in range(n_samples):\n",
    "        votes = sum(1 for run in all_results if run[j].score >= threshold)\n",
    "        labels.append(1 if votes > n_runs / 2 else 0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run three evaluation strategies\n",
    "rubric_default = prompt_variants[0]  # Use first variant as default\n",
    "\n",
    "# Strategy 1: Expensive-accurate judge\n",
    "expensive_judge = ExpensiveMockJudge(error_rate=0.05, seed=100)\n",
    "expensive_results = run_judge(dataset, expensive_judge.score, rubric_default)\n",
    "expensive_labels = [1 if r.score >= SCORE_THRESHOLD else 0 for r in expensive_results]\n",
    "expensive_acc = accuracy(expensive_labels, human_labels)\n",
    "\n",
    "# Strategy 2: Cheap-fast judge\n",
    "cheap_judge = CheapMockJudge(error_rate=0.22, seed=200)\n",
    "cheap_results = run_judge(dataset, cheap_judge.score, rubric_default)\n",
    "cheap_labels = [1 if r.score >= SCORE_THRESHOLD else 0 for r in cheap_results]\n",
    "cheap_acc = accuracy(cheap_labels, human_labels)\n",
    "\n",
    "# Strategy 3: Majority vote (3 cheap runs)\n",
    "vote_runs = []\n",
    "for run_seed in [300, 301, 302]:\n",
    "    j = CheapMockJudge(error_rate=0.22, seed=run_seed)\n",
    "    vote_runs.append(run_judge(dataset, j.score, rubric_default))\n",
    "vote_labels = majority_vote_labels(vote_runs, threshold=SCORE_THRESHOLD)\n",
    "vote_acc = accuracy(vote_labels, human_labels)\n",
    "\n",
    "print(f\"Strategy Accuracies vs Human Labels:\")\n",
    "print(f\"  Expensive-accurate: {expensive_acc:.4f}\")\n",
    "print(f\"  Cheap-fast:         {cheap_acc:.4f}\")\n",
    "print(f\"  Majority-vote-3:    {vote_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_economics(\n",
    "    accuracies: list[float],\n",
    "    model_names: list[str],\n",
    "    costs: list[float],\n",
    "    latencies: list[float],\n",
    ") -> list[EvalReport]:\n",
    "    \"\"\"Build EvalReport objects for each evaluation strategy.\"\"\"\n",
    "    reports = []\n",
    "    for name, acc, cost, lat in zip(model_names, accuracies, costs, latencies):\n",
    "        throughput = 1.0 / lat if lat > 0 else float(\"inf\")\n",
    "        acc_per_dollar = acc / cost if cost > 0 else float(\"inf\")\n",
    "        reports.append(EvalReport(\n",
    "            model_name=name,\n",
    "            accuracy=acc,\n",
    "            cost_per_eval=cost,\n",
    "            latency_per_eval=lat,\n",
    "            throughput=throughput,\n",
    "            accuracy_per_dollar=acc_per_dollar,\n",
    "        ))\n",
    "    return reports\n",
    "\n",
    "\n",
    "reports = eval_economics(\n",
    "    accuracies=[expensive_acc, cheap_acc, vote_acc],\n",
    "    model_names=[\"expensive-accurate\", \"cheap-fast\", \"majority-vote-3\"],\n",
    "    costs=[0.03, 0.001, 0.003],\n",
    "    latencies=[2.0, 0.1, 0.3],\n",
    ")\n",
    "\n",
    "print(\"Evaluation Economics\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<20} {'Accuracy':>8} {'Cost ($)':>8} {'Latency(s)':>10} {'Throughput':>10} {'Acc/$':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in reports:\n",
    "    print(\n",
    "        f\"{r.model_name:<20} {r.accuracy:>8.4f} {r.cost_per_eval:>8.3f} \"\n",
    "        f\"{r.latency_per_eval:>10.1f} {r.throughput:>10.1f} {r.accuracy_per_dollar:>10.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_frontier(\n",
    "    costs: list[float],\n",
    "    accuracies: list[float],\n",
    ") -> list[int]:\n",
    "    \"\"\"Find indices of Pareto-optimal points (minimize cost, maximize accuracy).\n",
    "\n",
    "    A point is Pareto-optimal if no other point has both lower cost AND higher accuracy.\n",
    "    \"\"\"\n",
    "    n = len(costs)\n",
    "    is_pareto = [True] * n\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            # j dominates i if j has lower or equal cost AND higher or equal accuracy\n",
    "            # with at least one strict inequality\n",
    "            if (costs[j] <= costs[i] and accuracies[j] >= accuracies[i]) and (\n",
    "                costs[j] < costs[i] or accuracies[j] > accuracies[i]\n",
    "            ):\n",
    "                is_pareto[i] = False\n",
    "                break\n",
    "    return [i for i in range(n) if is_pareto[i]]\n",
    "\n",
    "\n",
    "# Compute Pareto frontier\n",
    "costs = [r.cost_per_eval for r in reports]\n",
    "accs = [r.accuracy for r in reports]\n",
    "names = [r.model_name for r in reports]\n",
    "pareto_idx = pareto_frontier(costs, accs)\n",
    "\n",
    "print(f\"Pareto-optimal strategies: {[names[i] for i in pareto_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Pareto frontier scatter (Tufte style)\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Plot all points\n",
    "for i, (c, a, name) in enumerate(zip(costs, accs, names)):\n",
    "    marker = \"o\" if i in pareto_idx else \"x\"\n",
    "    color = \"#333333\" if i in pareto_idx else \"#999999\"\n",
    "    size = 80 if i in pareto_idx else 50\n",
    "    ax.scatter(c, a, s=size, marker=marker, c=color, zorder=3)\n",
    "    ax.annotate(\n",
    "        name,\n",
    "        (c, a),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(8, 6),\n",
    "        fontsize=9,\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "# Draw Pareto frontier line\n",
    "if len(pareto_idx) > 1:\n",
    "    pareto_points = sorted([(costs[i], accs[i]) for i in pareto_idx])\n",
    "    px, py = zip(*pareto_points)\n",
    "    ax.plot(px, py, \"--\", color=\"#666666\", linewidth=1, alpha=0.7, zorder=2)\n",
    "\n",
    "ax.set_xlabel(\"Cost per Evaluation ($)\", fontsize=10)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=10)\n",
    "ax.set_title(\"Accuracy vs Cost: Pareto Frontier\", fontsize=12, pad=10)\n",
    "\n",
    "# Tufte style: remove top/right spines, minimal grid\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.tick_params(length=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tests for Part 5 -----\n",
    "\n",
    "# Test EvalReport construction\n",
    "for r in reports:\n",
    "    assert 0.0 <= r.accuracy <= 1.0, f\"Accuracy {r.accuracy} out of range\"\n",
    "    assert r.cost_per_eval > 0, f\"Cost must be positive\"\n",
    "    assert r.latency_per_eval > 0, f\"Latency must be positive\"\n",
    "    assert r.throughput > 0, f\"Throughput must be positive\"\n",
    "    assert r.accuracy_per_dollar > 0, f\"Accuracy per dollar must be positive\"\n",
    "print(\"EvalReport fields validated.\")\n",
    "\n",
    "# Test Pareto ordering: expensive-accurate should be Pareto-optimal\n",
    "# (highest accuracy, so nothing dominates it on accuracy)\n",
    "assert 0 in pareto_idx, \"Expensive-accurate should be on Pareto frontier\"\n",
    "print(\"Expensive-accurate is Pareto-optimal: verified.\")\n",
    "\n",
    "# Test: majority vote accuracy >= single cheap judge accuracy\n",
    "assert vote_acc >= cheap_acc - 0.01, (\n",
    "    f\"Majority vote ({vote_acc:.4f}) should be >= cheap judge ({cheap_acc:.4f}) \"\n",
    "    f\"(with small tolerance for randomness)\"\n",
    ")\n",
    "print(f\"Majority vote ({vote_acc:.4f}) >= cheap judge ({cheap_acc:.4f}): verified.\")\n",
    "\n",
    "# Test Pareto frontier with trivially dominated point\n",
    "test_costs = [0.01, 0.02, 0.03]\n",
    "test_accs = [0.9, 0.85, 0.95]  # Point 1 (0.02, 0.85) is dominated by point 0 (0.01, 0.9)\n",
    "test_pareto = pareto_frontier(test_costs, test_accs)\n",
    "assert 1 not in test_pareto, \"Point (0.02, 0.85) should be dominated\"\n",
    "assert 0 in test_pareto and 2 in test_pareto, \"Points 0 and 2 should be Pareto-optimal\"\n",
    "print(\"Pareto frontier logic verified.\")\n",
    "\n",
    "print(\"\\nAll Part 5 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interview Tips: LLM-as-Judge in Practice\n",
    "\n",
    "### When to use LLM-as-Judge vs other approaches\n",
    "\n",
    "| Method | Best for | Weaknesses |\n",
    "|--------|----------|------------|\n",
    "| **Exact match** | Factual QA, code correctness | Misses semantically correct paraphrases |\n",
    "| **BLEU/ROUGE** | Translation, summarization | Poor correlation with human judgment on open-ended tasks |\n",
    "| **LLM-as-Judge** | Open-ended generation, style, reasoning | Expensive, biased, needs calibration |\n",
    "| **Human eval** | Gold standard for subjective tasks | Slow, expensive, not scalable |\n",
    "\n",
    "**Rule of thumb**: Use exact match when you can. Use LLM-as-Judge when the answer space is too large for exact match but too costly for human eval. Always validate the judge against human labels on a held-out set.\n",
    "\n",
    "### Known Failure Modes\n",
    "\n",
    "1. **Position bias**: Judges tend to prefer the first (or last) response in pairwise comparisons. Mitigation: swap positions and average.\n",
    "2. **Verbosity bias**: Longer responses get higher scores even when less accurate. Mitigation: include length-penalizing criteria in rubric.\n",
    "3. **Self-preference**: GPT-4 rates GPT-4 outputs higher than equivalent Claude outputs. Mitigation: use a different model family for judging than for generation.\n",
    "4. **Anchoring to rubric examples**: If the rubric shows a score-5 example, the judge calibrates around that specific style. Mitigation: diverse rubric examples.\n",
    "\n",
    "### Validating Calibration\n",
    "\n",
    "To check if your judge is well-calibrated:\n",
    "1. Collect N human-labeled samples (100+ is ideal)\n",
    "2. Run the judge on those samples\n",
    "3. Compute Cohen's kappa and F1 against human labels\n",
    "4. **Bootstrap confidence intervals**: resample with replacement B=1000 times, compute metric each time, report 95% CI\n",
    "5. If kappa < 0.6, the judge is not reliable enough for production use\n",
    "\n",
    "### Connection to Vals AI Methodology\n",
    "\n",
    "Vals AI focuses on building reliable LLM benchmarks. Key principles:\n",
    "- **Multi-judge ensembles**: Reduce variance by combining multiple judges (like our majority voting approach)\n",
    "- **Rubric engineering**: The rubric is as important as the model choice. Precise criteria reduce judge variance.\n",
    "- **Sensitivity testing**: Always measure how much your results change with prompt perturbations (Krippendorff's alpha)\n",
    "- **Cost-aware evaluation**: Production benchmarking requires balancing quality against budget. The Pareto frontier analysis in Part 5 is exactly the framework used in practice.\n",
    "- **Continuous calibration**: Re-validate judge accuracy against fresh human labels regularly, especially after model updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}