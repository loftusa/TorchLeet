{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation Harness\n",
    "\n",
    "## Background\n",
    "\n",
    "Evaluating LLM-based agents is fundamentally harder than evaluating static question-answering. An agent interacts with **tools** over **multiple steps**, and its performance depends on both the underlying model and the **scaffold** (prompt templates, tool definitions, retry logic, max steps). Companies like Vals AI run benchmarks such as SWE-bench, Finance Agent, and Terminal-Bench to measure agent capabilities in a standardized way.\n",
    "\n",
    "The key insight: **you cannot attribute a benchmark score solely to the model**. The scaffold matters enormously. This notebook builds a minimal but complete evaluation harness that lets you disentangle scaffold effects from model effects.\n",
    "\n",
    "### What We Build\n",
    "\n",
    "1. **Tool Registry & Execution** -- safe, validated tool dispatch\n",
    "2. **ReAct Agent Loop** -- thought-action-observation cycle with trajectory recording\n",
    "3. **Evaluation Dataset & Scoring** -- tasks with multiple answer types and partial credit\n",
    "4. **Harness-Level Metrics** -- accuracy, failure classification, stratified analysis\n",
    "5. **Scaffold vs Model Disentanglement** -- ablation studies showing scaffold sensitivity\n",
    "\n",
    "Everything uses deterministic mock agents (no API keys, no external services).\n",
    "\n",
    "### References\n",
    "\n",
    "- [Yao et al. - ReAct: Synergizing Reasoning and Acting (2023)](https://arxiv.org/abs/2210.03629)\n",
    "- [Jimenez et al. - SWE-bench (2024)](https://arxiv.org/abs/2310.06770)\n",
    "- [Vals AI Benchmarks](https://www.vals.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Tool Registry & Execution\n",
    "\n",
    "An agent evaluation harness needs a **tool registry** that maps tool names to callable functions, validates arguments, and handles errors gracefully. This is the foundation: if tool execution is unreliable, nothing downstream works.\n",
    "\n",
    "Key design choices:\n",
    "- Tools return strings (serialized JSON) for uniformity\n",
    "- The calculator uses `ast.parse` to reject arbitrary code execution\n",
    "- The registry raises `KeyError` for unknown tools rather than silently failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Any, Optional\n",
    "from collections import Counter\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Knowledge Base for Deterministic Tools ---\n",
    "\n",
    "COMPANY_DB: dict[str, dict[str, Any]] = {\n",
    "    \"ACME\": {\"revenue\": 1500000, \"expenses\": 1200000, \"employees\": 50, \"sector\": \"tech\"},\n",
    "    \"GLOBEX\": {\"revenue\": 3200000, \"expenses\": 2800000, \"employees\": 120, \"sector\": \"finance\"},\n",
    "    \"INITECH\": {\"revenue\": 800000, \"expenses\": 750000, \"employees\": 30, \"sector\": \"consulting\"},\n",
    "    \"UMBRELLA\": {\"revenue\": 5000000, \"expenses\": 4200000, \"employees\": 200, \"sector\": \"biotech\"},\n",
    "    \"WAYNE\": {\"revenue\": 10000000, \"expenses\": 8500000, \"employees\": 500, \"sector\": \"defense\"},\n",
    "}\n",
    "\n",
    "\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for company info. Returns matching company data.\"\"\"\n",
    "    query_upper = query.upper()\n",
    "    for name, data in COMPANY_DB.items():\n",
    "        if name in query_upper:\n",
    "            return json.dumps({\"company\": name, **data})\n",
    "    return json.dumps({\"error\": \"No results found\"})\n",
    "\n",
    "\n",
    "def retrieve_document(doc_id: str) -> str:\n",
    "    \"\"\"Retrieve a financial document by ID.\"\"\"\n",
    "    docs = {\n",
    "        \"10K-ACME-2024\": \"ACME Corp Annual Report 2024. Revenue: $1.5M, Net Income: $300K, Employees: 50.\",\n",
    "        \"10K-GLOBEX-2024\": \"Globex Inc Annual Report 2024. Revenue: $3.2M, Net Income: $400K, Employees: 120.\",\n",
    "        \"10K-INITECH-2024\": \"Initech LLC Annual Report 2024. Revenue: $800K, Net Income: $50K, Employees: 30.\",\n",
    "        \"10K-UMBRELLA-2024\": \"Umbrella Corp Annual Report 2024. Revenue: $5M, Net Income: $800K, Employees: 200.\",\n",
    "        \"10K-WAYNE-2024\": \"Wayne Enterprises Annual Report 2024. Revenue: $10M, Net Income: $1.5M, Employees: 500.\",\n",
    "    }\n",
    "    return docs.get(doc_id, json.dumps({\"error\": f\"Document {doc_id} not found\"}))\n",
    "\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safe math evaluation using ast.parse for simple expressions.\"\"\"\n",
    "    try:\n",
    "        allowed_nodes = (\n",
    "            ast.Expression, ast.BinOp, ast.UnaryOp, ast.Constant,\n",
    "            ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Mod, ast.Pow,\n",
    "            ast.USub, ast.UAdd,\n",
    "        )\n",
    "        tree = ast.parse(expression, mode=\"eval\")\n",
    "        for node in ast.walk(tree):\n",
    "            if not isinstance(node, allowed_nodes):\n",
    "                return json.dumps({\"error\": f\"Unsafe operation: {type(node).__name__}\"})\n",
    "        result = eval(compile(tree, \"<calc>\", \"eval\"))\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "\n",
    "def submit_answer(answer: str) -> str:\n",
    "    \"\"\"Submit final answer.\"\"\"\n",
    "    return json.dumps({\"status\": \"submitted\", \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tool:\n",
    "    \"\"\"A tool that an agent can invoke.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: dict  # JSON-schema style\n",
    "    func: Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"A parsed tool invocation from agent output.\"\"\"\n",
    "    tool_name: str\n",
    "    arguments: dict\n",
    "    raw_text: str\n",
    "\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Registry that maps tool names to Tool objects and executes calls.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._tools: dict[str, Tool] = {}\n",
    "\n",
    "    def register(self, tool: Tool) -> None:\n",
    "        \"\"\"Add a tool to the registry.\"\"\"\n",
    "        self._tools[tool.name] = tool\n",
    "\n",
    "    def get(self, name: str) -> Tool:\n",
    "        \"\"\"Lookup a tool by name. Raises KeyError if not found.\"\"\"\n",
    "        if name not in self._tools:\n",
    "            raise KeyError(f\"Tool '{name}' not found in registry.\")\n",
    "        return self._tools[name]\n",
    "\n",
    "    def execute(self, call: ToolCall) -> str:\n",
    "        \"\"\"Validate and run a tool call. Catches exceptions and returns error JSON.\"\"\"\n",
    "        tool = self.get(call.tool_name)  # raises KeyError if missing\n",
    "        try:\n",
    "            return tool.func(**call.arguments)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Tool execution failed: {str(e)}\"})\n",
    "\n",
    "    def list_tools(self) -> list[str]:\n",
    "        \"\"\"Return names and descriptions for all registered tools.\"\"\"\n",
    "        return [f\"{t.name}: {t.description}\" for t in self._tools.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the default registry ---\n",
    "\n",
    "def build_default_registry() -> ToolRegistry:\n",
    "    \"\"\"Create a registry with all four standard tools.\"\"\"\n",
    "    registry = ToolRegistry()\n",
    "\n",
    "    registry.register(Tool(\n",
    "        name=\"search\",\n",
    "        description=\"Search for company info. Returns matching company data.\",\n",
    "        parameters={\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}},\n",
    "        func=search,\n",
    "    ))\n",
    "    registry.register(Tool(\n",
    "        name=\"retrieve_document\",\n",
    "        description=\"Retrieve a financial document by ID.\",\n",
    "        parameters={\"type\": \"object\", \"properties\": {\"doc_id\": {\"type\": \"string\"}}},\n",
    "        func=retrieve_document,\n",
    "    ))\n",
    "    registry.register(Tool(\n",
    "        name=\"calculate\",\n",
    "        description=\"Safe math evaluation for simple arithmetic expressions.\",\n",
    "        parameters={\"type\": \"object\", \"properties\": {\"expression\": {\"type\": \"string\"}}},\n",
    "        func=calculate,\n",
    "    ))\n",
    "    registry.register(Tool(\n",
    "        name=\"submit_answer\",\n",
    "        description=\"Submit the final answer to the evaluation harness.\",\n",
    "        parameters={\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}},\n",
    "        func=submit_answer,\n",
    "    ))\n",
    "\n",
    "    return registry\n",
    "\n",
    "\n",
    "registry = build_default_registry()\n",
    "print(\"Registered tools:\")\n",
    "for desc in registry.list_tools():\n",
    "    print(f\"  - {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tests for Part 1 ---\n",
    "\n",
    "print(\"Part 1 Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: All tools return strings\n",
    "assert isinstance(search(\"ACME\"), str)\n",
    "assert isinstance(retrieve_document(\"10K-ACME-2024\"), str)\n",
    "assert isinstance(calculate(\"2 + 3\"), str)\n",
    "assert isinstance(submit_answer(\"42\"), str)\n",
    "print(\"[PASS] All tools return str\")\n",
    "\n",
    "# Test 2: Search returns expected data\n",
    "result = json.loads(search(\"ACME revenue\"))\n",
    "assert result[\"company\"] == \"ACME\"\n",
    "assert result[\"revenue\"] == 1500000\n",
    "print(\"[PASS] search() returns expected company data\")\n",
    "\n",
    "# Test 3: Search with no match\n",
    "result = json.loads(search(\"NONEXISTENT\"))\n",
    "assert \"error\" in result\n",
    "print(\"[PASS] search() returns error for unknown company\")\n",
    "\n",
    "# Test 4: Calculate works for safe expressions\n",
    "assert json.loads(calculate(\"(1500000 - 1200000) / 1500000\"))[\"result\"] == 0.2\n",
    "assert json.loads(calculate(\"2 ** 10\"))[\"result\"] == 1024\n",
    "print(\"[PASS] calculate() evaluates safe expressions correctly\")\n",
    "\n",
    "# Test 5: Calculate rejects malicious code\n",
    "result_import = json.loads(calculate('__import__(\"os\").system(\"rm -rf /\")'))\n",
    "assert \"error\" in result_import, \"Should reject __import__\"\n",
    "print(f\"[PASS] calculate() rejects __import__: {result_import['error']}\")\n",
    "\n",
    "result_open = json.loads(calculate('open(\"/etc/passwd\")'))\n",
    "assert \"error\" in result_open, \"Should reject open()\"\n",
    "print(f\"[PASS] calculate() rejects open(): {result_open['error']}\")\n",
    "\n",
    "# Test 6: Invalid tool name raises KeyError\n",
    "try:\n",
    "    registry.get(\"nonexistent_tool\")\n",
    "    assert False, \"Should have raised KeyError\"\n",
    "except KeyError:\n",
    "    pass\n",
    "print(\"[PASS] registry.get() raises KeyError for unknown tool\")\n",
    "\n",
    "# Test 7: Registry execute with valid call\n",
    "call = ToolCall(tool_name=\"search\", arguments={\"query\": \"GLOBEX\"}, raw_text=\"\")\n",
    "result = json.loads(registry.execute(call))\n",
    "assert result[\"company\"] == \"GLOBEX\"\n",
    "print(\"[PASS] registry.execute() dispatches correctly\")\n",
    "\n",
    "# Test 8: Registry execute catches bad arguments gracefully\n",
    "bad_call = ToolCall(tool_name=\"search\", arguments={\"wrong_param\": \"x\"}, raw_text=\"\")\n",
    "result = json.loads(registry.execute(bad_call))\n",
    "assert \"error\" in result\n",
    "print(\"[PASS] registry.execute() catches bad arguments\")\n",
    "\n",
    "print(\"\\nAll Part 1 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: ReAct Agent Loop\n",
    "\n",
    "The [ReAct](https://arxiv.org/abs/2210.03629) framework interleaves **reasoning** (Thought) with **acting** (Action + Action Input). The agent generates text in a structured format, and the harness parses out the action, executes it, and feeds the observation back.\n",
    "\n",
    "A key design decision: we use **deterministic mock agents** with scripted action sequences. This lets us test the harness itself without any LLM variance. In production, you would swap in an LLM call.\n",
    "\n",
    "The trajectory dataclass records every step for post-hoc analysis -- this is critical for failure classification later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentStep:\n",
    "    \"\"\"A single step in an agent's trajectory.\"\"\"\n",
    "    thought: str\n",
    "    action: Optional[ToolCall]\n",
    "    observation: str\n",
    "    step_num: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentTrajectory:\n",
    "    \"\"\"Full record of an agent's execution on a task.\"\"\"\n",
    "    question: str\n",
    "    steps: list[AgentStep] = field(default_factory=list)\n",
    "    final_answer: Optional[str] = None\n",
    "    terminated: bool = False\n",
    "    termination_reason: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_action(text: str) -> Optional[ToolCall]:\n",
    "    \"\"\"Parse a ReAct-format response into a ToolCall.\n",
    "\n",
    "    Expected format:\n",
    "        Thought: <reasoning>\n",
    "        Action: <tool_name>\n",
    "        Action Input: <json_arguments>\n",
    "\n",
    "    Returns None if no valid action is found.\n",
    "    \"\"\"\n",
    "    action_match = re.search(r\"Action:\\s*(.+?)\\s*$\", text, re.MULTILINE)\n",
    "    input_match = re.search(r\"Action Input:\\s*(.+?)\\s*$\", text, re.MULTILINE)\n",
    "\n",
    "    if not action_match or not input_match:\n",
    "        return None\n",
    "\n",
    "    tool_name = action_match.group(1).strip()\n",
    "    raw_input = input_match.group(1).strip()\n",
    "\n",
    "    try:\n",
    "        arguments = json.loads(raw_input)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: treat the entire input as a single string argument.\n",
    "        # Determine the expected parameter name from the tool name.\n",
    "        param_map = {\n",
    "            \"search\": \"query\",\n",
    "            \"retrieve_document\": \"doc_id\",\n",
    "            \"calculate\": \"expression\",\n",
    "            \"submit_answer\": \"answer\",\n",
    "        }\n",
    "        param_name = param_map.get(tool_name, \"input\")\n",
    "        arguments = {param_name: raw_input}\n",
    "\n",
    "    return ToolCall(tool_name=tool_name, arguments=arguments, raw_text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockAgent:\n",
    "    \"\"\"Deterministic agent that follows a scripted sequence of actions.\"\"\"\n",
    "\n",
    "    def __init__(self, script: list[str]) -> None:\n",
    "        self.script = script\n",
    "        self.step = 0\n",
    "\n",
    "    def __call__(self, observation: str, history: list[AgentStep]) -> str:\n",
    "        if self.step < len(self.script):\n",
    "            response = self.script[self.step]\n",
    "            self.step += 1\n",
    "            return response\n",
    "        return (\n",
    "            'Thought: I have no more actions.\\n'\n",
    "            'Action: submit_answer\\n'\n",
    "            'Action Input: {\"answer\": \"unknown\"}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    question: str,\n",
    "    registry: ToolRegistry,\n",
    "    agent_fn: Callable,\n",
    "    max_steps: int = 10,\n",
    ") -> AgentTrajectory:\n",
    "    \"\"\"Execute a ReAct agent loop and record the full trajectory.\n",
    "\n",
    "    Args:\n",
    "        question: The task question to present to the agent.\n",
    "        registry: ToolRegistry with available tools.\n",
    "        agent_fn: Callable(observation, history) -> str.\n",
    "        max_steps: Maximum number of steps before forced termination.\n",
    "\n",
    "    Returns:\n",
    "        AgentTrajectory with all steps recorded.\n",
    "    \"\"\"\n",
    "    trajectory = AgentTrajectory(question=question)\n",
    "    observation = f\"Question: {question}\"\n",
    "\n",
    "    for step_num in range(1, max_steps + 1):\n",
    "        # Get agent response\n",
    "        response = agent_fn(observation, trajectory.steps)\n",
    "\n",
    "        # Extract thought\n",
    "        thought_match = re.search(r\"Thought:\\s*(.+?)(?:\\n|$)\", response)\n",
    "        thought = thought_match.group(1).strip() if thought_match else \"\"\n",
    "\n",
    "        # Parse action\n",
    "        action = parse_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            # No valid action parsed -- record and terminate\n",
    "            step = AgentStep(\n",
    "                thought=thought,\n",
    "                action=None,\n",
    "                observation=\"Parse error: no valid action found.\",\n",
    "                step_num=step_num,\n",
    "            )\n",
    "            trajectory.steps.append(step)\n",
    "            trajectory.terminated = True\n",
    "            trajectory.termination_reason = \"parse_error\"\n",
    "            break\n",
    "\n",
    "        # Check for submit_answer\n",
    "        if action.tool_name == \"submit_answer\":\n",
    "            answer = action.arguments.get(\"answer\", \"\")\n",
    "            obs = registry.execute(action)\n",
    "            step = AgentStep(\n",
    "                thought=thought,\n",
    "                action=action,\n",
    "                observation=obs,\n",
    "                step_num=step_num,\n",
    "            )\n",
    "            trajectory.steps.append(step)\n",
    "            trajectory.final_answer = answer\n",
    "            trajectory.terminated = True\n",
    "            trajectory.termination_reason = \"submitted\"\n",
    "            break\n",
    "\n",
    "        # Execute the tool\n",
    "        try:\n",
    "            obs = registry.execute(action)\n",
    "        except KeyError:\n",
    "            obs = json.dumps({\"error\": f\"Unknown tool: {action.tool_name}\"})\n",
    "\n",
    "        step = AgentStep(\n",
    "            thought=thought,\n",
    "            action=action,\n",
    "            observation=obs,\n",
    "            step_num=step_num,\n",
    "        )\n",
    "        trajectory.steps.append(step)\n",
    "        observation = f\"Observation: {obs}\"\n",
    "\n",
    "    # Max-steps termination\n",
    "    if not trajectory.terminated:\n",
    "        trajectory.terminated = True\n",
    "        trajectory.termination_reason = \"max_steps\"\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tests for Part 2 ---\n",
    "\n",
    "print(\"Part 2 Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: parse_action extracts valid action\n",
    "text = (\n",
    "    'Thought: I need to search for ACME.\\n'\n",
    "    'Action: search\\n'\n",
    "    'Action Input: {\"query\": \"ACME\"}'\n",
    ")\n",
    "action = parse_action(text)\n",
    "assert action is not None\n",
    "assert action.tool_name == \"search\"\n",
    "assert action.arguments == {\"query\": \"ACME\"}\n",
    "print(\"[PASS] parse_action extracts valid ReAct action\")\n",
    "\n",
    "# Test 2: parse_action returns None for invalid text\n",
    "assert parse_action(\"Just some random text with no action.\") is None\n",
    "print(\"[PASS] parse_action returns None for invalid format\")\n",
    "\n",
    "# Test 3: Agent terminates on submit_answer\n",
    "script = [\n",
    "    'Thought: I need to search for ACME.\\nAction: search\\nAction Input: {\"query\": \"ACME\"}',\n",
    "    'Thought: I have the answer.\\nAction: submit_answer\\nAction Input: {\"answer\": \"1500000\"}',\n",
    "]\n",
    "agent = MockAgent(script)\n",
    "traj = run_agent(\"What is ACME's revenue?\", registry, agent, max_steps=10)\n",
    "assert traj.terminated is True\n",
    "assert traj.termination_reason == \"submitted\"\n",
    "assert traj.final_answer == \"1500000\"\n",
    "assert len(traj.steps) == 2\n",
    "print(\"[PASS] Agent terminates on submit_answer with correct trajectory\")\n",
    "\n",
    "# Test 4: Max steps respected\n",
    "infinite_script = [\n",
    "    'Thought: Searching again.\\nAction: search\\nAction Input: {\"query\": \"ACME\"}'\n",
    "] * 20\n",
    "agent = MockAgent(infinite_script)\n",
    "traj = run_agent(\"Loop forever\", registry, agent, max_steps=3)\n",
    "assert traj.termination_reason == \"max_steps\"\n",
    "assert len(traj.steps) == 3\n",
    "print(\"[PASS] max_steps terminates agent correctly\")\n",
    "\n",
    "# Test 5: Parse error terminates\n",
    "bad_script = [\"This is gibberish with no action.\"]\n",
    "agent = MockAgent(bad_script)\n",
    "traj = run_agent(\"Will fail\", registry, agent, max_steps=5)\n",
    "assert traj.termination_reason == \"parse_error\"\n",
    "assert len(traj.steps) == 1\n",
    "print(\"[PASS] Parse error terminates agent\")\n",
    "\n",
    "# Test 6: Trajectory records observations from tools\n",
    "script = [\n",
    "    'Thought: Search WAYNE.\\nAction: search\\nAction Input: {\"query\": \"WAYNE\"}',\n",
    "    'Thought: Submit.\\nAction: submit_answer\\nAction Input: {\"answer\": \"10000000\"}',\n",
    "]\n",
    "agent = MockAgent(script)\n",
    "traj = run_agent(\"WAYNE revenue?\", registry, agent)\n",
    "obs_data = json.loads(traj.steps[0].observation)\n",
    "assert obs_data[\"company\"] == \"WAYNE\"\n",
    "assert obs_data[\"revenue\"] == 10000000\n",
    "print(\"[PASS] Trajectory records tool observations\")\n",
    "\n",
    "print(\"\\nAll Part 2 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Evaluation Dataset & Scoring\n",
    "\n",
    "A benchmark is only as good as its tasks and scoring. We define tasks at three difficulty levels:\n",
    "\n",
    "- **Easy**: single tool call (lookup and submit)\n",
    "- **Medium**: two-step reasoning (lookup, calculate, submit)\n",
    "- **Hard**: multi-document reasoning (multiple lookups, comparison, submit)\n",
    "\n",
    "Scoring supports three answer types:\n",
    "- `exact_match`: string equality (case-insensitive, stripped)\n",
    "- `numeric_tolerance`: partial credit with 5% full / 20% half thresholds\n",
    "- `contains`: substring check for open-ended answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentTask:\n",
    "    \"\"\"A single evaluation task for an agent.\"\"\"\n",
    "    task_id: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    answer_type: str  # \"exact_match\" | \"numeric_tolerance\" | \"contains\"\n",
    "    available_tools: list[str]\n",
    "    max_steps: int\n",
    "    difficulty: str  # \"easy\" | \"medium\" | \"hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS: list[AgentTask] = [\n",
    "    # --- Easy: single lookup ---\n",
    "    AgentTask(\"E1\", \"What is ACME's revenue?\", \"1500000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E2\", \"How many employees does GLOBEX have?\", \"120\", \"numeric_tolerance\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E3\", \"What sector is INITECH in?\", \"consulting\", \"exact_match\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E4\", \"What are UMBRELLA's expenses?\", \"4200000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E5\", \"What is WAYNE's revenue?\", \"10000000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E6\", \"What sector is WAYNE in?\", \"defense\", \"exact_match\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "    AgentTask(\"E7\", \"How many employees does ACME have?\", \"50\", \"numeric_tolerance\",\n",
    "             [\"search\", \"submit_answer\"], 5, \"easy\"),\n",
    "\n",
    "    # --- Medium: two-step (lookup + calculate) ---\n",
    "    AgentTask(\"M1\", \"What is ACME's profit margin (revenue - expenses) / revenue?\",\n",
    "             \"0.2\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M2\", \"What is GLOBEX's profit margin?\",\n",
    "             \"0.125\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M3\", \"What is INITECH's profit (revenue minus expenses)?\",\n",
    "             \"50000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M4\", \"What is UMBRELLA's revenue per employee?\",\n",
    "             \"25000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M5\", \"What is WAYNE's profit margin?\",\n",
    "             \"0.15\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M6\", \"What is ACME's revenue per employee?\",\n",
    "             \"30000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "    AgentTask(\"M7\", \"What is GLOBEX's profit (revenue minus expenses)?\",\n",
    "             \"400000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 7, \"medium\"),\n",
    "\n",
    "    # --- Hard: multi-doc reasoning ---\n",
    "    AgentTask(\"H1\", \"Which company has higher revenue, ACME or GLOBEX?\",\n",
    "             \"GLOBEX\", \"contains\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 10, \"hard\"),\n",
    "    AgentTask(\"H2\", \"Which company has more employees, INITECH or UMBRELLA?\",\n",
    "             \"UMBRELLA\", \"contains\",\n",
    "             [\"search\", \"submit_answer\"], 10, \"hard\"),\n",
    "    AgentTask(\"H3\", \"What is the combined revenue of ACME and GLOBEX?\",\n",
    "             \"4700000\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 10, \"hard\"),\n",
    "    AgentTask(\"H4\", \"Which company has the highest profit margin: ACME, GLOBEX, or WAYNE?\",\n",
    "             \"ACME\", \"contains\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 10, \"hard\"),\n",
    "    AgentTask(\"H5\", \"What is the total number of employees across ACME, GLOBEX, and INITECH?\",\n",
    "             \"200\", \"numeric_tolerance\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 10, \"hard\"),\n",
    "    AgentTask(\"H6\", \"Which company is more profitable per employee, WAYNE or UMBRELLA?\",\n",
    "             \"WAYNE\", \"contains\",\n",
    "             [\"search\", \"calculate\", \"submit_answer\"], 10, \"hard\"),\n",
    "]\n",
    "\n",
    "print(f\"Total tasks: {len(TASKS)}\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    count = sum(1 for t in TASKS if t.difficulty == diff)\n",
    "    print(f\"  {diff}: {count} tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(predicted: str, expected: str, answer_type: str) -> float:\n",
    "    \"\"\"Score a predicted answer against the expected answer.\n",
    "\n",
    "    Args:\n",
    "        predicted: The agent's answer string.\n",
    "        expected: The ground-truth answer string.\n",
    "        answer_type: One of 'exact_match', 'numeric_tolerance', 'contains'.\n",
    "\n",
    "    Returns:\n",
    "        Score in [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    if predicted is None:\n",
    "        return 0.0\n",
    "\n",
    "    predicted = predicted.strip()\n",
    "    expected = expected.strip()\n",
    "\n",
    "    if answer_type == \"exact_match\":\n",
    "        return 1.0 if predicted.lower() == expected.lower() else 0.0\n",
    "\n",
    "    elif answer_type == \"numeric_tolerance\":\n",
    "        try:\n",
    "            p_val = float(predicted)\n",
    "            e_val = float(expected)\n",
    "        except (ValueError, TypeError):\n",
    "            return 0.0\n",
    "        if e_val == 0:\n",
    "            return 1.0 if p_val == 0 else 0.0\n",
    "        rel_error = abs(p_val - e_val) / abs(e_val)\n",
    "        if rel_error <= 0.05:\n",
    "            return 1.0\n",
    "        elif rel_error <= 0.20:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    elif answer_type == \"contains\":\n",
    "        return 1.0 if expected.lower() in predicted.lower() else 0.0\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown answer_type: {answer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tests for Part 3 ---\n",
    "\n",
    "print(\"Part 3 Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Exact match\n",
    "assert score_answer(\"consulting\", \"consulting\", \"exact_match\") == 1.0\n",
    "assert score_answer(\"Consulting\", \"consulting\", \"exact_match\") == 1.0  # case insensitive\n",
    "assert score_answer(\"tech\", \"consulting\", \"exact_match\") == 0.0\n",
    "print(\"[PASS] exact_match scoring\")\n",
    "\n",
    "# Numeric tolerance\n",
    "assert score_answer(\"100\", \"100\", \"numeric_tolerance\") == 1.0  # exact\n",
    "assert score_answer(\"104.9\", \"100\", \"numeric_tolerance\") == 1.0  # 4.9% off -> full\n",
    "assert score_answer(\"105.1\", \"100\", \"numeric_tolerance\") == 0.5  # 5.1% off -> half\n",
    "assert score_answer(\"119.9\", \"100\", \"numeric_tolerance\") == 0.5  # 19.9% off -> half\n",
    "assert score_answer(\"120.1\", \"100\", \"numeric_tolerance\") == 0.0  # 20.1% off -> zero\n",
    "print(\"[PASS] numeric_tolerance boundaries (4.9% -> 1.0, 5.1% -> 0.5, 20.1% -> 0.0)\")\n",
    "\n",
    "# Contains\n",
    "assert score_answer(\"GLOBEX has higher revenue\", \"GLOBEX\", \"contains\") == 1.0\n",
    "assert score_answer(\"ACME\", \"GLOBEX\", \"contains\") == 0.0\n",
    "print(\"[PASS] contains scoring\")\n",
    "\n",
    "# None answer\n",
    "assert score_answer(None, \"100\", \"numeric_tolerance\") == 0.0\n",
    "print(\"[PASS] None answer returns 0.0\")\n",
    "\n",
    "# All scores in [0, 1]\n",
    "for score_val in [0.0, 0.5, 1.0]:\n",
    "    assert 0.0 <= score_val <= 1.0\n",
    "print(\"[PASS] All scores in [0, 1]\")\n",
    "\n",
    "# Task dataset integrity\n",
    "task_ids = [t.task_id for t in TASKS]\n",
    "assert len(task_ids) == len(set(task_ids)), \"Duplicate task IDs!\"\n",
    "assert len(TASKS) == 20\n",
    "for t in TASKS:\n",
    "    assert t.difficulty in (\"easy\", \"medium\", \"hard\")\n",
    "    assert t.answer_type in (\"exact_match\", \"numeric_tolerance\", \"contains\")\n",
    "print(\"[PASS] Task dataset integrity (20 tasks, unique IDs, valid fields)\")\n",
    "\n",
    "print(\"\\nAll Part 3 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Harness-Level Metrics\n",
    "\n",
    "A benchmark harness needs to go beyond raw accuracy. We need:\n",
    "\n",
    "- **Failure classification**: distinguish wrong answers from timeouts, tool errors, and parse errors\n",
    "- **Stratified accuracy**: performance broken down by difficulty, answer type, etc.\n",
    "- **Aggregate metrics**: average steps, tool usage patterns\n",
    "\n",
    "This mirrors what production benchmarks like SWE-bench compute. The failure taxonomy is especially important because it tells you *where* to invest effort: if most failures are timeouts, increase max_steps; if they are tool errors, fix the tools; if they are wrong answers, improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResults:\n",
    "    \"\"\"Aggregated evaluation results.\"\"\"\n",
    "    task_results: list[dict]  # per-task: task_id, score, trajectory, failure_mode\n",
    "    metrics: dict  # aggregate: accuracy, avg_steps, tool_usage, failure_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_failure(trajectory: AgentTrajectory, score: float) -> str:\n",
    "    \"\"\"Classify why a task failed (or succeeded).\n",
    "\n",
    "    Categories:\n",
    "        'success': score >= 0.5\n",
    "        'wrong_answer': submitted but score < 0.5\n",
    "        'timeout': hit max_steps\n",
    "        'tool_error': any observation contains 'error'\n",
    "        'parse_error': termination due to parse failure\n",
    "    \"\"\"\n",
    "    if score >= 0.5:\n",
    "        return \"success\"\n",
    "    if trajectory.termination_reason == \"parse_error\":\n",
    "        return \"parse_error\"\n",
    "    if trajectory.termination_reason == \"max_steps\":\n",
    "        return \"timeout\"\n",
    "    # Check for tool errors in observations\n",
    "    for step in trajectory.steps:\n",
    "        if \"error\" in step.observation.lower():\n",
    "            return \"tool_error\"\n",
    "    return \"wrong_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent_script_for_task(task: AgentTask, solve: bool = True) -> list[str]:\n",
    "    \"\"\"Generate a deterministic MockAgent script for a task.\n",
    "\n",
    "    If solve=True, generates a correct sequence. If solve=False,\n",
    "    generates a plausible but incorrect sequence.\n",
    "    \"\"\"\n",
    "    tid = task.task_id\n",
    "    script: list[str] = []\n",
    "\n",
    "    if task.difficulty == \"easy\":\n",
    "        # Single lookup tasks -- extract company name from question\n",
    "        company = None\n",
    "        for name in COMPANY_DB:\n",
    "            if name in task.question.upper():\n",
    "                company = name\n",
    "                break\n",
    "        if company and solve:\n",
    "            script.append(\n",
    "                f'Thought: I need to look up {company}.\\n'\n",
    "                f'Action: search\\n'\n",
    "                f'Action Input: {{\"query\": \"{company}\"}}'\n",
    "            )\n",
    "            script.append(\n",
    "                f'Thought: I found the answer.\\n'\n",
    "                f'Action: submit_answer\\n'\n",
    "                f'Action Input: {{\"answer\": \"{task.expected_answer}\"}}'\n",
    "            )\n",
    "        else:\n",
    "            # Wrong answer\n",
    "            script.append(\n",
    "                'Thought: I will guess.\\n'\n",
    "                'Action: submit_answer\\n'\n",
    "                'Action Input: {\"answer\": \"wrong\"}'\n",
    "            )\n",
    "\n",
    "    elif task.difficulty == \"medium\":\n",
    "        company = None\n",
    "        for name in COMPANY_DB:\n",
    "            if name in task.question.upper():\n",
    "                company = name\n",
    "                break\n",
    "        if company and solve:\n",
    "            data = COMPANY_DB[company]\n",
    "            script.append(\n",
    "                f'Thought: I need to look up {company}.\\n'\n",
    "                f'Action: search\\n'\n",
    "                f'Action Input: {{\"query\": \"{company}\"}}'\n",
    "            )\n",
    "            # Build a calculation expression based on expected answer\n",
    "            rev = data[\"revenue\"]\n",
    "            exp = data[\"expenses\"]\n",
    "            emp = data[\"employees\"]\n",
    "            if \"margin\" in task.question.lower():\n",
    "                expr = f\"({rev} - {exp}) / {rev}\"\n",
    "            elif \"per employee\" in task.question.lower():\n",
    "                expr = f\"{rev} / {emp}\"\n",
    "            else:  # profit\n",
    "                expr = f\"{rev} - {exp}\"\n",
    "            script.append(\n",
    "                f'Thought: I need to calculate.\\n'\n",
    "                f'Action: calculate\\n'\n",
    "                f'Action Input: {{\"expression\": \"{expr}\"}}'\n",
    "            )\n",
    "            script.append(\n",
    "                f'Thought: I have the answer.\\n'\n",
    "                f'Action: submit_answer\\n'\n",
    "                f'Action Input: {{\"answer\": \"{task.expected_answer}\"}}'\n",
    "            )\n",
    "        else:\n",
    "            script.append(\n",
    "                'Thought: I will guess.\\n'\n",
    "                'Action: submit_answer\\n'\n",
    "                'Action Input: {\"answer\": \"999\"}'\n",
    "            )\n",
    "\n",
    "    elif task.difficulty == \"hard\":\n",
    "        if solve:\n",
    "            # Find all companies mentioned\n",
    "            companies = [n for n in COMPANY_DB if n in task.question.upper()]\n",
    "            for c in companies:\n",
    "                script.append(\n",
    "                    f'Thought: Looking up {c}.\\n'\n",
    "                    f'Action: search\\n'\n",
    "                    f'Action Input: {{\"query\": \"{c}\"}}'\n",
    "                )\n",
    "            if \"calculate\" in task.available_tools and any(\n",
    "                kw in task.question.lower()\n",
    "                for kw in [\"combined\", \"total\", \"margin\", \"per employee\", \"profitable\"]\n",
    "            ):\n",
    "                # Add a calculation step\n",
    "                # Build the expression from company data\n",
    "                if \"combined revenue\" in task.question.lower() or \"total\" in task.question.lower():\n",
    "                    vals = [str(COMPANY_DB[c][\"revenue\"]) for c in companies]\n",
    "                    if \"employee\" in task.question.lower():\n",
    "                        vals = [str(COMPANY_DB[c][\"employees\"]) for c in companies]\n",
    "                    expr = \" + \".join(vals)\n",
    "                else:\n",
    "                    expr = \"0\"  # placeholder\n",
    "                script.append(\n",
    "                    f'Thought: Calculating.\\n'\n",
    "                    f'Action: calculate\\n'\n",
    "                    f'Action Input: {{\"expression\": \"{expr}\"}}'\n",
    "                )\n",
    "            script.append(\n",
    "                f'Thought: I have the answer.\\n'\n",
    "                f'Action: submit_answer\\n'\n",
    "                f'Action Input: {{\"answer\": \"{task.expected_answer}\"}}'\n",
    "            )\n",
    "        else:\n",
    "            # Fail by submitting wrong or timing out\n",
    "            script.append(\n",
    "                'Thought: I will guess.\\n'\n",
    "                'Action: submit_answer\\n'\n",
    "                'Action Input: {\"answer\": \"WRONG\"}'\n",
    "            )\n",
    "\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_agent_factory(\n",
    "    easy_solve_rate: float = 1.0,\n",
    "    medium_solve_rate: float = 0.57,\n",
    "    hard_solve_rate: float = 0.33,\n",
    "    seed: int = 42,\n",
    ") -> Callable[[AgentTask], MockAgent]:\n",
    "    \"\"\"Create a factory that produces scripted MockAgents with realistic\n",
    "    difficulty-stratified success rates.\n",
    "\n",
    "    Uses a seeded RNG to make solve/fail decisions deterministic.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    def factory(task: AgentTask) -> MockAgent:\n",
    "        rate_map = {\"easy\": easy_solve_rate, \"medium\": medium_solve_rate, \"hard\": hard_solve_rate}\n",
    "        solve = rng.random() < rate_map[task.difficulty]\n",
    "        script = make_agent_script_for_task(task, solve=solve)\n",
    "        return MockAgent(script)\n",
    "\n",
    "    return factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    tasks: list[AgentTask],\n",
    "    agent_factory_fn: Callable[[AgentTask], MockAgent],\n",
    "    registry: ToolRegistry,\n",
    ") -> EvalResults:\n",
    "    \"\"\"Run the full evaluation harness over a task suite.\n",
    "\n",
    "    For each task:\n",
    "        1. Create an agent via factory.\n",
    "        2. Run the agent loop.\n",
    "        3. Score the answer.\n",
    "        4. Classify the failure mode.\n",
    "\n",
    "    Aggregate metrics: accuracy, avg_steps, tool_usage, failure_modes.\n",
    "    \"\"\"\n",
    "    task_results: list[dict] = []\n",
    "    tool_usage: Counter = Counter()\n",
    "    failure_modes: Counter = Counter()\n",
    "    total_steps = 0\n",
    "\n",
    "    for task in tasks:\n",
    "        agent = agent_factory_fn(task)\n",
    "        traj = run_agent(task.question, registry, agent, max_steps=task.max_steps)\n",
    "        score = score_answer(traj.final_answer, task.expected_answer, task.answer_type)\n",
    "        failure = classify_failure(traj, score)\n",
    "\n",
    "        # Record tool usage\n",
    "        for step in traj.steps:\n",
    "            if step.action is not None:\n",
    "                tool_usage[step.action.tool_name] += 1\n",
    "        total_steps += len(traj.steps)\n",
    "        failure_modes[failure] += 1\n",
    "\n",
    "        task_results.append({\n",
    "            \"task_id\": task.task_id,\n",
    "            \"difficulty\": task.difficulty,\n",
    "            \"score\": score,\n",
    "            \"failure_mode\": failure,\n",
    "            \"num_steps\": len(traj.steps),\n",
    "            \"termination_reason\": traj.termination_reason,\n",
    "            \"trajectory\": traj,\n",
    "        })\n",
    "\n",
    "    accuracy = np.mean([r[\"score\"] for r in task_results])\n",
    "    avg_steps = total_steps / len(tasks) if tasks else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"avg_steps\": float(avg_steps),\n",
    "        \"tool_usage\": dict(tool_usage),\n",
    "        \"failure_modes\": dict(failure_modes),\n",
    "        \"num_tasks\": len(tasks),\n",
    "    }\n",
    "\n",
    "    return EvalResults(task_results=task_results, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_accuracy(\n",
    "    results: EvalResults,\n",
    "    stratify_by: str = \"difficulty\",\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute accuracy grouped by an attribute.\n",
    "\n",
    "    Args:\n",
    "        results: EvalResults from run_evaluation.\n",
    "        stratify_by: Key in task_results dict to group by.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping group name to accuracy.\n",
    "    \"\"\"\n",
    "    groups: dict[str, list[float]] = {}\n",
    "    for r in results.task_results:\n",
    "        key = r[stratify_by]\n",
    "        groups.setdefault(key, []).append(r[\"score\"])\n",
    "    return {k: float(np.mean(v)) for k, v in groups.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run evaluation ---\n",
    "\n",
    "registry = build_default_registry()\n",
    "factory = create_mock_agent_factory(easy_solve_rate=1.0, medium_solve_rate=0.57, hard_solve_rate=0.33, seed=42)\n",
    "results = run_evaluation(TASKS, factory, registry)\n",
    "\n",
    "print(\"Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Overall accuracy: {results.metrics['accuracy']:.2%}\")\n",
    "print(f\"Average steps:    {results.metrics['avg_steps']:.1f}\")\n",
    "print(f\"Total tasks:      {results.metrics['num_tasks']}\")\n",
    "print(f\"\\nTool usage:       {results.metrics['tool_usage']}\")\n",
    "print(f\"Failure modes:    {results.metrics['failure_modes']}\")\n",
    "\n",
    "strat = stratified_accuracy(results, \"difficulty\")\n",
    "print(f\"\\nStratified accuracy:\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    if diff in strat:\n",
    "        print(f\"  {diff:8s}: {strat[diff]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tests for Part 4 ---\n",
    "\n",
    "print(\"Part 4 Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: accuracy in [0, 1]\n",
    "assert 0.0 <= results.metrics[\"accuracy\"] <= 1.0\n",
    "print(f\"[PASS] accuracy = {results.metrics['accuracy']:.2%} is in [0, 1]\")\n",
    "\n",
    "# Test 2: avg_steps > 0\n",
    "assert results.metrics[\"avg_steps\"] > 0\n",
    "print(f\"[PASS] avg_steps = {results.metrics['avg_steps']:.1f} > 0\")\n",
    "\n",
    "# Test 3: failure categories cover all tasks\n",
    "total_failures = sum(results.metrics[\"failure_modes\"].values())\n",
    "assert total_failures == len(TASKS), f\"Failure modes sum {total_failures} != {len(TASKS)}\"\n",
    "print(f\"[PASS] failure modes cover all {len(TASKS)} tasks\")\n",
    "\n",
    "# Test 4: stratification sums check\n",
    "strat = stratified_accuracy(results, \"difficulty\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    assert diff in strat, f\"Missing difficulty '{diff}' in stratification\"\n",
    "    assert 0.0 <= strat[diff] <= 1.0\n",
    "print(\"[PASS] stratified accuracy covers all difficulties, values in [0, 1]\")\n",
    "\n",
    "# Test 5: easy accuracy >= hard accuracy (by design)\n",
    "assert strat[\"easy\"] >= strat[\"hard\"], \"Easy should be >= hard by design\"\n",
    "print(f\"[PASS] easy ({strat['easy']:.0%}) >= hard ({strat['hard']:.0%})\")\n",
    "\n",
    "# Test 6: classify_failure categories are valid\n",
    "valid_modes = {\"success\", \"wrong_answer\", \"timeout\", \"tool_error\", \"parse_error\"}\n",
    "for mode in results.metrics[\"failure_modes\"]:\n",
    "    assert mode in valid_modes, f\"Invalid failure mode: {mode}\"\n",
    "print(\"[PASS] all failure modes are valid categories\")\n",
    "\n",
    "# Test 7: per-task results have expected fields\n",
    "for r in results.task_results:\n",
    "    assert \"task_id\" in r\n",
    "    assert \"score\" in r\n",
    "    assert \"failure_mode\" in r\n",
    "    assert 0.0 <= r[\"score\"] <= 1.0\n",
    "print(\"[PASS] per-task results have expected fields\")\n",
    "\n",
    "print(\"\\nAll Part 4 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Scaffold vs Model Disentanglement\n",
    "\n",
    "The central question in agent evaluation: **how much of the score is due to the model vs the scaffold?**\n",
    "\n",
    "The scaffold includes:\n",
    "- Maximum number of steps (more steps = more chances to recover)\n",
    "- Prompt style (verbose instructions vs minimal)\n",
    "- Available tools (removing tools forces different strategies)\n",
    "\n",
    "We run ablation experiments across these dimensions and compute **scaffold sensitivity** (coefficient of variation of accuracy across configs). A high value means the scaffold has outsized influence on the benchmark score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AblationConfig:\n",
    "    \"\"\"Configuration for a scaffold ablation experiment.\"\"\"\n",
    "    name: str\n",
    "    max_steps: int\n",
    "    prompt_style: str  # 'standard' or 'verbose'\n",
    "    tools_available: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TOOLS = [\"search\", \"retrieve_document\", \"calculate\", \"submit_answer\"]\n",
    "\n",
    "ABLATION_CONFIGS: list[AblationConfig] = [\n",
    "    AblationConfig(\"steps-2\", max_steps=2, prompt_style=\"standard\", tools_available=ALL_TOOLS.copy()),\n",
    "    AblationConfig(\"steps-5\", max_steps=5, prompt_style=\"standard\", tools_available=ALL_TOOLS.copy()),\n",
    "    AblationConfig(\"steps-10\", max_steps=10, prompt_style=\"standard\", tools_available=ALL_TOOLS.copy()),\n",
    "    AblationConfig(\"verbose-prompt\", max_steps=10, prompt_style=\"verbose\", tools_available=ALL_TOOLS.copy()),\n",
    "    AblationConfig(\"no-calculate\", max_steps=10, prompt_style=\"standard\",\n",
    "                   tools_available=[\"search\", \"retrieve_document\", \"submit_answer\"]),\n",
    "]\n",
    "\n",
    "print(\"Ablation configs:\")\n",
    "for cfg in ABLATION_CONFIGS:\n",
    "    print(f\"  {cfg.name:16s} | max_steps={cfg.max_steps:2d} | prompt={cfg.prompt_style:8s} | tools={cfg.tools_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ablation_script(\n",
    "    task: AgentTask,\n",
    "    config: AblationConfig,\n",
    "    solve: bool = True,\n",
    ") -> list[str]:\n",
    "    \"\"\"Generate a MockAgent script adapted to an ablation config.\n",
    "\n",
    "    Key adaptations:\n",
    "    - If a required tool is not in config.tools_available, the agent cannot solve\n",
    "      tasks needing that tool.\n",
    "    - If max_steps is too low for the task, the script will exceed the limit.\n",
    "    - Verbose prompt style adds extra thought steps.\n",
    "    \"\"\"\n",
    "    # Check if the agent has the tools needed\n",
    "    needed_tools = set(task.available_tools)\n",
    "    available = set(config.tools_available)\n",
    "    missing_tools = needed_tools - available - {\"submit_answer\"}  # submit always available\n",
    "\n",
    "    if missing_tools and solve:\n",
    "        # Cannot solve -- submit wrong answer\n",
    "        return [\n",
    "            'Thought: I do not have the required tools.\\n'\n",
    "            'Action: submit_answer\\n'\n",
    "            'Action Input: {\"answer\": \"cannot compute\"}'\n",
    "        ]\n",
    "\n",
    "    base_script = make_agent_script_for_task(task, solve=solve)\n",
    "\n",
    "    # Verbose prompt adds a planning step at the beginning\n",
    "    if config.prompt_style == \"verbose\" and solve:\n",
    "        planning_step = (\n",
    "            f'Thought: Let me carefully plan my approach to: {task.question}\\n'\n",
    "            f'Action: search\\n'\n",
    "            f'Action Input: {{\"query\": \"planning\"}}'\n",
    "        )\n",
    "        base_script = [planning_step] + base_script\n",
    "\n",
    "    return base_script\n",
    "\n",
    "\n",
    "def create_ablation_agent_factory(\n",
    "    config: AblationConfig,\n",
    "    easy_solve_rate: float = 1.0,\n",
    "    medium_solve_rate: float = 0.57,\n",
    "    hard_solve_rate: float = 0.33,\n",
    "    seed: int = 42,\n",
    ") -> Callable[[AgentTask], MockAgent]:\n",
    "    \"\"\"Create an agent factory for a specific ablation config.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    def factory(task: AgentTask) -> MockAgent:\n",
    "        rate_map = {\"easy\": easy_solve_rate, \"medium\": medium_solve_rate, \"hard\": hard_solve_rate}\n",
    "        solve = rng.random() < rate_map[task.difficulty]\n",
    "        # Override max_steps on the task for this ablation\n",
    "        task_copy = copy.copy(task)\n",
    "        task_copy.max_steps = config.max_steps\n",
    "        script = make_ablation_script(task_copy, config, solve=solve)\n",
    "        return MockAgent(script)\n",
    "\n",
    "    return factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(\n",
    "    tasks: list[AgentTask],\n",
    "    configs: list[AblationConfig],\n",
    "    registry: ToolRegistry,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run evaluation for each ablation config and return a summary DataFrame.\n",
    "\n",
    "    Columns: config_name, accuracy, avg_steps, num_successes.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for config in configs:\n",
    "        factory = create_ablation_agent_factory(config, seed=42)\n",
    "\n",
    "        # Adapt tasks to config max_steps\n",
    "        adapted_tasks = []\n",
    "        for t in tasks:\n",
    "            t_copy = copy.copy(t)\n",
    "            t_copy.max_steps = config.max_steps\n",
    "            adapted_tasks.append(t_copy)\n",
    "\n",
    "        eval_results = run_evaluation(adapted_tasks, factory, registry)\n",
    "        num_successes = sum(\n",
    "            1 for r in eval_results.task_results if r[\"failure_mode\"] == \"success\"\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"config_name\": config.name,\n",
    "            \"accuracy\": eval_results.metrics[\"accuracy\"],\n",
    "            \"avg_steps\": eval_results.metrics[\"avg_steps\"],\n",
    "            \"num_successes\": num_successes,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_sensitivity(accuracies: list[float]) -> float:\n",
    "    \"\"\"Coefficient of variation of accuracy across scaffold configs.\n",
    "\n",
    "    CV = std / mean. High value means scaffold matters more than model.\n",
    "    Returns 0.0 if mean is 0 (to avoid division by zero).\n",
    "    \"\"\"\n",
    "    arr = np.array(accuracies)\n",
    "    mean_val = np.mean(arr)\n",
    "    if mean_val == 0:\n",
    "        return 0.0\n",
    "    return float(np.std(arr) / mean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run ablation experiments ---\n",
    "\n",
    "registry = build_default_registry()\n",
    "ablation_df = run_ablation(TASKS, ABLATION_CONFIGS, registry)\n",
    "\n",
    "print(\"Ablation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(ablation_df.to_string(index=False))\n",
    "\n",
    "sensitivity = scaffold_sensitivity(ablation_df[\"accuracy\"].tolist())\n",
    "print(f\"\\nScaffold sensitivity (CV): {sensitivity:.3f}\")\n",
    "if sensitivity > 0.15:\n",
    "    print(\"  -> HIGH: scaffold configuration dominates model differences\")\n",
    "elif sensitivity > 0.05:\n",
    "    print(\"  -> MODERATE: scaffold has meaningful impact\")\n",
    "else:\n",
    "    print(\"  -> LOW: model capability dominates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Tufte-style grouped bar chart ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "configs = ablation_df[\"config_name\"].tolist()\n",
    "accuracies = ablation_df[\"accuracy\"].tolist()\n",
    "\n",
    "# Minimal bar chart following Tufte principles:\n",
    "# high data-ink ratio, no chartjunk, direct labeling\n",
    "bars = ax.bar(\n",
    "    range(len(configs)),\n",
    "    accuracies,\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor=\"none\",\n",
    "    width=0.6,\n",
    ")\n",
    "\n",
    "# Direct labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{acc:.0%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_xticks(range(len(configs)))\n",
    "ax.set_xticklabels(configs, rotation=25, ha=\"right\", fontsize=9)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=11)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Tufte: remove top and right spines, minimize grid\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.0%}\"))\n",
    "\n",
    "# Add a reference line for the baseline config\n",
    "baseline_acc = ablation_df.loc[ablation_df[\"config_name\"] == \"steps-10\", \"accuracy\"].values[0]\n",
    "ax.axhline(y=baseline_acc, color=\"gray\", linestyle=\"--\", linewidth=0.8, alpha=0.6)\n",
    "ax.text(len(configs) - 0.5, baseline_acc + 0.02, f\"baseline ({baseline_acc:.0%})\",\n",
    "        fontsize=8, color=\"gray\", ha=\"right\")\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Scaffold Ablation: Accuracy across Configs  (CV = {sensitivity:.2f})\",\n",
    "    fontsize=12,\n",
    "    pad=12,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tests for Part 5 ---\n",
    "\n",
    "print(\"Part 5 Tests\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: All configs produced results\n",
    "assert len(ablation_df) == len(ABLATION_CONFIGS)\n",
    "print(f\"[PASS] All {len(ABLATION_CONFIGS)} configs produced results\")\n",
    "\n",
    "# Test 2: scaffold_sensitivity >= 0\n",
    "assert sensitivity >= 0.0\n",
    "print(f\"[PASS] scaffold_sensitivity = {sensitivity:.3f} >= 0\")\n",
    "\n",
    "# Test 3: All accuracy values are valid\n",
    "for _, row in ablation_df.iterrows():\n",
    "    assert 0.0 <= row[\"accuracy\"] <= 1.0, f\"Invalid accuracy: {row['accuracy']}\"\n",
    "    assert row[\"avg_steps\"] > 0, f\"avg_steps must be > 0\"\n",
    "    assert row[\"num_successes\"] >= 0\n",
    "print(\"[PASS] All accuracy/step/success values are valid\")\n",
    "\n",
    "# Test 4: steps-2 should have <= accuracy of steps-10 (fewer steps = harder)\n",
    "acc_2 = ablation_df.loc[ablation_df[\"config_name\"] == \"steps-2\", \"accuracy\"].values[0]\n",
    "acc_10 = ablation_df.loc[ablation_df[\"config_name\"] == \"steps-10\", \"accuracy\"].values[0]\n",
    "assert acc_2 <= acc_10, f\"steps-2 ({acc_2:.2f}) should be <= steps-10 ({acc_10:.2f})\"\n",
    "print(f\"[PASS] steps-2 ({acc_2:.0%}) <= steps-10 ({acc_10:.0%})\")\n",
    "\n",
    "# Test 5: no-calculate should have lower accuracy for medium tasks\n",
    "# (medium tasks need calculate tool)\n",
    "no_calc_acc = ablation_df.loc[ablation_df[\"config_name\"] == \"no-calculate\", \"accuracy\"].values[0]\n",
    "full_acc = ablation_df.loc[ablation_df[\"config_name\"] == \"steps-10\", \"accuracy\"].values[0]\n",
    "assert no_calc_acc <= full_acc, f\"no-calculate ({no_calc_acc:.2f}) should be <= full ({full_acc:.2f})\"\n",
    "print(f\"[PASS] no-calculate ({no_calc_acc:.0%}) <= full tools ({full_acc:.0%})\")\n",
    "\n",
    "# Test 6: scaffold_sensitivity with identical values should be 0\n",
    "assert scaffold_sensitivity([0.5, 0.5, 0.5]) == 0.0\n",
    "print(\"[PASS] scaffold_sensitivity([0.5, 0.5, 0.5]) == 0.0\")\n",
    "\n",
    "# Test 7: scaffold_sensitivity with all zeros should be 0\n",
    "assert scaffold_sensitivity([0.0, 0.0, 0.0]) == 0.0\n",
    "print(\"[PASS] scaffold_sensitivity handles all-zero edge case\")\n",
    "\n",
    "print(\"\\nAll Part 5 tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interview Tips: Agent Evaluation at Vals AI\n",
    "\n",
    "### Why agentic benchmarks are harder than static QA\n",
    "\n",
    "Static QA benchmarks (MMLU, HellaSwag) have a fixed input-output mapping. Agent benchmarks introduce **sequential decision-making** with feedback loops. This creates several challenges:\n",
    "\n",
    "1. **Path dependence**: A wrong early action can cascade into failure, even if the model is capable.\n",
    "2. **Non-determinism**: Even with temperature=0, tool outputs and parsing can vary.\n",
    "3. **Partial observability**: The agent only sees its own trajectory, not the full state.\n",
    "4. **Credit assignment**: Was a failure due to bad reasoning, bad tool use, or bad luck?\n",
    "\n",
    "### Scaffold confounds and how Vals standardizes\n",
    "\n",
    "The scaffold (max steps, prompt template, tool definitions, retry logic) is a massive confound. Two models with identical capabilities can score very differently under different scaffolds. Vals addresses this by:\n",
    "\n",
    "- Providing a **standardized harness** with fixed scaffold parameters per benchmark\n",
    "- Reporting scaffold sensitivity alongside model scores\n",
    "- Running ablation experiments to quantify scaffold contribution\n",
    "- Using identical tool definitions and system prompts across model comparisons\n",
    "\n",
    "The **coefficient of variation** (std/mean of accuracy across scaffold configs) is a useful summary statistic. If CV > 0.15, the scaffold is likely dominating the signal.\n",
    "\n",
    "### Reproducibility: practical considerations\n",
    "\n",
    "- **Seed management**: Fix numpy, torch, and random seeds. But LLM APIs are not fully deterministic.\n",
    "- **Temperature=0**: Necessary but not sufficient (some APIs still have nondeterminism).\n",
    "- **Retry policies**: Define how many retries on tool failure, parse failure, etc.\n",
    "- **Versioning**: Pin model versions, tool definitions, and prompt templates.\n",
    "- **Logging**: Record full trajectories, not just final scores. This enables post-hoc debugging.\n",
    "\n",
    "### Connection to Vals' Finance Agent benchmark\n",
    "\n",
    "The Finance Agent benchmark uses real-world financial tools (SEC_API for filing retrieval, ParseHTML for document processing). The architecture mirrors what we built here:\n",
    "\n",
    "- Tool registry with SEC-specific tools\n",
    "- ReAct-style agent loop with trajectory recording\n",
    "- Tasks graded on financial accuracy (numeric tolerance for dollar amounts)\n",
    "- Failure classification to distinguish model errors from API errors\n",
    "\n",
    "The key difference is that production benchmarks must handle **real API failures**, **rate limits**, and **non-deterministic tool outputs** -- which is why the harness design (retry policies, error classification, trajectory logging) matters as much as the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}