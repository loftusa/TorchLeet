{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Mixture of Factor Analyzers for LLM Activation Decomposition\n",
    "\n",
    "**Paper**: [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464) (Shafran et al., 2025)\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "Existing interpretability methods assume concepts live along **single global directions** (linear probes, SAE features, DiffMean). But many concepts have **nonlinear** structure—they're spread across clusters with different local orientations.\n",
    "\n",
    "**Mixture of Factor Analyzers (MFA)** models activation space as a collection of **Gaussian regions**, each with its own centroid and local low-rank subspace. Every activation decomposes into:\n",
    "\n",
    "1. **Centroid** $\\mu_k$: *which region* the activation belongs to (broad semantic category)\n",
    "2. **Local offset** $W_k z_k$: *variation within* that region (fine-grained distinctions)\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "- **Steering**: Centroid interpolation steers toward broad concepts; local offsets refine within a concept. MFA often outperforms SAEs on causal steering benchmarks.\n",
    "- **Interpretability**: MFA decompositions have ~96% interpretable feature mass (vs ~29% for SAEs).\n",
    "- **Structure discovery**: Reveals that concepts organize into multi-Gaussian neighborhoods (e.g., an \"emotions\" cluster contains sub-Gaussians for happiness, surprise, anger).\n",
    "\n",
    "### The Model\n",
    "\n",
    "**Factor Analysis (single component):**\n",
    "$$x = \\mu + Wz + \\epsilon, \\quad z \\sim \\mathcal{N}(0, I_R), \\quad \\epsilon \\sim \\mathcal{N}(0, \\Psi)$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{D \\times R}$ maps $R$ latent factors to $D$-dimensional activation space, and $\\Psi$ is diagonal noise.\n",
    "\n",
    "**Mixture of Factor Analyzers:**\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x \\mid \\mu_k, W_k W_k^\\top + \\Psi)$$\n",
    "\n",
    "Each component $k$ has its own centroid $\\mu_k$ and loading matrix $W_k$, but they share noise $\\Psi$.\n",
    "\n",
    "**Decomposition** of an activation $x$:\n",
    "$$x \\approx \\underbrace{\\mu_k}_{\\text{centroid}} + \\underbrace{W_k \\hat{z}_k}_{\\text{local offset}} + \\underbrace{\\epsilon}_{\\text{noise}}$$\n",
    "\n",
    "where $k = \\arg\\max_j R_j(x)$ is the most likely component, and $\\hat{z}_k$ is the posterior mean of the latent.\n",
    "\n",
    "### Exercise Overview\n",
    "\n",
    "You will:\n",
    "1. **Extract activations** from a small LLM\n",
    "2. **Implement Factor Analysis** (the single-component building block)\n",
    "3. **Implement MFA** (mixture of FAs with K-means initialization)\n",
    "4. **Decompose activations** into centroid + local offset + noise\n",
    "5. **Interpret components**: inspect centroids and loading directions via logit lens\n",
    "6. **Steer with MFA**: compare centroid intervention vs local offset intervention\n",
    "\n",
    "Estimated time: ~45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "## Part 1: Extract Activations\n",
    "\n",
    "We extract residual stream activations from a middle layer of SmolLM2-135M using `nnsight`. The paper uses Llama-3.1-8B and Gemma-2-2B with 100M activations; we use a much smaller setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = LanguageModel(model_name, device_map=device, dispatch=True)\n",
    "tokenizer = model.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "d_model = model.config.hidden_size\n",
    "target_layer = n_layers // 3  # Early-mid layer\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Layers: {n_layers}, d_model: {d_model}\")\n",
    "print(f\"Extracting from layer {target_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-acts",
   "metadata": {},
   "outputs": [],
   "source": "ds = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n\nall_activations = []\nall_tokens = []\nnum_texts = 300\nmax_len = 128\n\ntexts = []\nfor i, example in enumerate(ds):\n    if i >= num_texts:\n        break\n    texts.append(example[\"text\"][:512])\n\nprint(f\"Collected {len(texts)} texts, tokenizing...\")\n\nbatch_size = 20\nfor i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting activations\"):\n    batch_texts = texts[i:i + batch_size]\n    tokens = tokenizer(\n        batch_texts, return_tensors=\"pt\", padding=True,\n        truncation=True, max_length=max_len\n    )\n    input_ids = tokens[\"input_ids\"].to(device)\n    attention_mask = tokens[\"attention_mask\"].to(device)\n\n    with torch.no_grad():\n        with model.trace(input_ids, attention_mask=attention_mask):\n            hidden = model.model.layers[target_layer].output.save()\n\n    acts = hidden.float().cpu()\n    mask = attention_mask.cpu().bool()\n\n    for b in range(acts.shape[0]):\n        valid = mask[b].clone()\n        valid[0] = False\n        if valid.sum() > 0:\n            all_activations.append(acts[b, valid])\n            all_tokens.append(input_ids[b, valid].cpu())\n\nactivations = torch.cat(all_activations, dim=0).float()\ntoken_ids = torch.cat(all_tokens, dim=0)\n\nprint(f\"\\nCollected {activations.shape[0]:,} activations of dim {activations.shape[1]}\")\nprint(f\"Stats: mean={activations.mean():.3f}, std={activations.std():.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-unembedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unembedding matrix for logit lens later\n",
    "unembed = model.lm_head.weight.detach().cpu().float()  # (vocab_size, d_model)\n",
    "# Apply final layer norm for proper logit lens\n",
    "ln_weight = model.model.norm.weight.detach().cpu().float()\n",
    "\n",
    "def logit_lens(\n",
    "    direction: Float[Tensor, \"dim\"],\n",
    "    top_k: int = 10,\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"Project a direction through the unembedding to see what tokens it promotes.\"\"\"\n",
    "    # Apply RMSNorm-style scaling\n",
    "    normed = direction * ln_weight\n",
    "    logits = unembed @ normed  # (vocab_size,)\n",
    "    topk = logits.topk(top_k)\n",
    "    results = []\n",
    "    for idx, val in zip(topk.indices, topk.values):\n",
    "        tok = tokenizer.decode([idx.item()])\n",
    "        results.append((tok, val.item()))\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "print(\"Logit lens on mean activation:\")\n",
    "for tok, val in logit_lens(activations.mean(dim=0), top_k=5):\n",
    "    print(f\"  {val:7.2f}  '{tok}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: Implement Factor Analysis\n",
    "\n",
    "Factor Analysis (FA) is the single-component building block. The generative model is:\n",
    "\n",
    "$$x = \\mu + Wz + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $z \\sim \\mathcal{N}(0, I_R)$ are $R$ latent factors\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, \\Psi)$ with diagonal $\\Psi$ (per-dimension noise)\n",
    "- $W \\in \\mathbb{R}^{D \\times R}$ is the loading matrix\n",
    "\n",
    "The covariance of $x$ is $C = WW^\\top + \\Psi$.\n",
    "\n",
    "**Key insight from the paper**: The meaningful object is the **subspace** $\\text{span}(W)$, not individual columns of $W$ (since $W$ is invariant to orthogonal rotations: $W' = WQ$ for orthogonal $Q$ gives the same model).\n",
    "\n",
    "**Posterior of latent given observation**:\n",
    "$$\\hat{z} = (I_R + W^\\top \\Psi^{-1} W)^{-1} W^\\top \\Psi^{-1} (x - \\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "factor-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorAnalysis(nn.Module):\n",
    "    \"\"\"\n",
    "    Factor Analysis: x = mu + W @ z + epsilon\n",
    "    \n",
    "    z ~ N(0, I_R), epsilon ~ N(0, diag(psi))\n",
    "    Covariance: C = W @ W^T + diag(psi)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, rank: int):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.rank = rank\n",
    "\n",
    "        self.mu = nn.Parameter(torch.zeros(d))\n",
    "        self.W = nn.Parameter(torch.randn(d, rank) * 0.01)\n",
    "        # log_psi for positivity constraint\n",
    "        self.log_psi = nn.Parameter(torch.zeros(d))\n",
    "\n",
    "    @property\n",
    "    def psi(self) -> Float[Tensor, \"d\"]:\n",
    "        return self.log_psi.exp()\n",
    "\n",
    "    def covariance(self) -> Float[Tensor, \"d d\"]:\n",
    "        \"\"\"C = W @ W^T + diag(psi)\"\"\"\n",
    "        return self.W @ self.W.T + torch.diag(self.psi)\n",
    "\n",
    "    def log_prob(\n",
    "        self, x: Float[Tensor, \"batch d\"]\n",
    "    ) -> Float[Tensor, \"batch\"]:\n",
    "        \"\"\"\n",
    "        Log probability under this FA model: log N(x | mu, C).\n",
    "        \n",
    "        Use the Woodbury identity for efficient computation:\n",
    "        C^{-1} = Psi^{-1} - Psi^{-1} W (I + W^T Psi^{-1} W)^{-1} W^T Psi^{-1}\n",
    "        \n",
    "        And the matrix determinant lemma:\n",
    "        |C| = |I + W^T Psi^{-1} W| * |Psi|\n",
    "        \"\"\"\n",
    "        # TODO: Compute the log probability efficiently using Woodbury identity\n",
    "        # \n",
    "        # Steps:\n",
    "        # 1. Compute psi_inv = 1 / psi  (element-wise, since Psi is diagonal)\n",
    "        # 2. Compute M = I_R + W^T @ diag(psi_inv) @ W  (R x R matrix)\n",
    "        # 3. Compute log_det = log|M| + sum(log_psi)  (matrix determinant lemma)\n",
    "        # 4. Center data: dx = x - mu\n",
    "        # 5. Compute C^{-1} @ dx using Woodbury:\n",
    "        #    C_inv_dx = psi_inv * dx - psi_inv * (W @ M^{-1} @ W^T @ (psi_inv * dx))\n",
    "        # 6. Mahalanobis: mahal = sum(dx * C_inv_dx, dim=-1)\n",
    "        # 7. log_prob = -0.5 * (D * log(2pi) + log_det + mahal)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def posterior_z(\n",
    "        self, x: Float[Tensor, \"batch d\"]\n",
    "    ) -> Float[Tensor, \"batch rank\"]:\n",
    "        \"\"\"\n",
    "        Posterior mean of latent z given x.\n",
    "        \n",
    "        z_hat = (I_R + W^T Psi^{-1} W)^{-1} W^T Psi^{-1} (x - mu)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the posterior mean computation\n",
    "        # 1. psi_inv = 1 / psi\n",
    "        # 2. M = I_R + W^T @ diag(psi_inv) @ W\n",
    "        # 3. dx = x - mu\n",
    "        # 4. z_hat = M^{-1} @ W^T @ (psi_inv * dx)^T  -> transpose appropriately\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Factor Analysis\n",
    "torch.manual_seed(42)\n",
    "fa = FactorAnalysis(d=16, rank=3)\n",
    "test_x = torch.randn(32, 16)\n",
    "\n",
    "lp = fa.log_prob(test_x)\n",
    "assert lp.shape == (32,), f\"Wrong shape: {lp.shape}\"\n",
    "assert torch.isfinite(lp).all(), \"Non-finite log probs\"\n",
    "\n",
    "z_hat = fa.posterior_z(test_x)\n",
    "assert z_hat.shape == (32, 3), f\"Wrong shape: {z_hat.shape}\"\n",
    "\n",
    "# Verify reconstruction: x ≈ mu + W @ z_hat (approximately)\n",
    "x_recon = fa.mu + z_hat @ fa.W.T\n",
    "assert x_recon.shape == test_x.shape\n",
    "\n",
    "print(\"✓ Factor Analysis tests passed\")\n",
    "print(f\"  Log prob range: [{lp.min():.1f}, {lp.max():.1f}]\")\n",
    "print(f\"  Posterior z range: [{z_hat.min():.3f}, {z_hat.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Implement Mixture of Factor Analyzers\n",
    "\n",
    "MFA extends FA with $K$ components, each having its own centroid $\\mu_k$ and loading matrix $W_k$, but sharing noise $\\Psi$:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x \\mid \\mu_k, W_k W_k^\\top + \\Psi)$$\n",
    "\n",
    "**Responsibilities** (posterior component assignment):\n",
    "$$R_k(x) = \\frac{\\pi_k \\, \\mathcal{N}(x \\mid \\mu_k, C_k)}{\\sum_j \\pi_j \\, \\mathcal{N}(x \\mid \\mu_j, C_j)}$$\n",
    "\n",
    "The paper trains via gradient descent on negative log-likelihood (not EM), initialized with K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFA(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Factor Analyzers.\n",
    "    \n",
    "    K components, each with centroid mu_k and loading W_k.\n",
    "    Shared diagonal noise Psi.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, K: int, rank: int):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.K = K\n",
    "        self.rank = rank\n",
    "\n",
    "        # Component parameters\n",
    "        self.mus = nn.Parameter(torch.randn(K, d) * 0.1)       # centroids\n",
    "        self.Ws = nn.Parameter(torch.randn(K, d, rank) * 0.01)  # loadings\n",
    "\n",
    "        # Shared noise (log-scale for positivity)\n",
    "        self.log_psi = nn.Parameter(torch.zeros(d))\n",
    "\n",
    "        # Mixture weights (log-scale, softmax to get pi)\n",
    "        self.log_pi = nn.Parameter(torch.zeros(K))\n",
    "\n",
    "    @property\n",
    "    def psi(self) -> Float[Tensor, \"d\"]:\n",
    "        return self.log_psi.exp()\n",
    "\n",
    "    @property\n",
    "    def pi(self) -> Float[Tensor, \"K\"]:\n",
    "        return F.softmax(self.log_pi, dim=0)\n",
    "\n",
    "    def component_log_prob(\n",
    "        self, x: Float[Tensor, \"batch d\"], k: int\n",
    "    ) -> Float[Tensor, \"batch\"]:\n",
    "        \"\"\"\n",
    "        Log probability under component k: log N(x | mu_k, W_k W_k^T + Psi).\n",
    "        Uses Woodbury identity for efficiency.\n",
    "        \"\"\"\n",
    "        # TODO: Same as FA.log_prob but using self.mus[k] and self.Ws[k]\n",
    "        pass\n",
    "\n",
    "    def log_prob(\n",
    "        self, x: Float[Tensor, \"batch d\"]\n",
    "    ) -> Float[Tensor, \"batch\"]:\n",
    "        \"\"\"\n",
    "        Mixture log probability: log sum_k pi_k * N(x | mu_k, C_k)\n",
    "        \n",
    "        Use logsumexp for numerical stability:\n",
    "        log p(x) = logsumexp_k(log pi_k + log N(x | mu_k, C_k))\n",
    "        \"\"\"\n",
    "        # TODO: Compute log p(x) using logsumexp over components\n",
    "        # 1. For each k, compute log_pi_k + component_log_prob(x, k)\n",
    "        # 2. Stack into (batch, K) tensor\n",
    "        # 3. logsumexp over K dimension\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def responsibilities(\n",
    "        self, x: Float[Tensor, \"batch d\"]\n",
    "    ) -> Float[Tensor, \"batch K\"]:\n",
    "        \"\"\"\n",
    "        Posterior component probabilities: R_k(x) = p(k | x)\n",
    "        \n",
    "        R_k(x) = pi_k * N(x|mu_k,C_k) / sum_j pi_j * N(x|mu_j,C_j)\n",
    "        \n",
    "        In log space: log R_k = log pi_k + log N_k - logsumexp\n",
    "        \"\"\"\n",
    "        # TODO: Compute responsibilities using softmax in log-space\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def posterior_z(\n",
    "        self, x: Float[Tensor, \"batch d\"], k: int\n",
    "    ) -> Float[Tensor, \"batch rank\"]:\n",
    "        \"\"\"\n",
    "        Posterior mean of latent z for component k.\n",
    "        z_hat_k = (I_R + W_k^T Psi^{-1} W_k)^{-1} W_k^T Psi^{-1} (x - mu_k)\n",
    "        \"\"\"\n",
    "        # TODO: Implement (same formula as FA but with component-specific params)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def decompose(\n",
    "        self, x: Float[Tensor, \"batch d\"]\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Decompose activations into centroid + local offset + noise.\n",
    "        \n",
    "        For each x, find the most likely component k*, then:\n",
    "          centroid = mu_{k*}\n",
    "          local_offset = W_{k*} @ z_hat_{k*}\n",
    "          noise = x - centroid - local_offset\n",
    "        \n",
    "        Returns dict with: assignments, centroids, local_offsets, noise, z_hat\n",
    "        \"\"\"\n",
    "        # TODO: Implement the decomposition\n",
    "        # 1. Get responsibilities -> hard assignment k* = argmax\n",
    "        # 2. For each unique k*, compute posterior z and local offset\n",
    "        # 3. Residual = x - mu_{k*} - W_{k*} @ z_hat\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-mfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MFA\n",
    "torch.manual_seed(42)\n",
    "test_mfa = MFA(d=16, K=4, rank=3)\n",
    "test_x = torch.randn(32, 16)\n",
    "\n",
    "lp = test_mfa.log_prob(test_x)\n",
    "assert lp.shape == (32,), f\"Wrong log_prob shape: {lp.shape}\"\n",
    "\n",
    "resp = test_mfa.responsibilities(test_x)\n",
    "assert resp.shape == (32, 4), f\"Wrong responsibilities shape: {resp.shape}\"\n",
    "assert torch.allclose(resp.sum(dim=-1), torch.ones(32), atol=1e-5), \"Responsibilities must sum to 1\"\n",
    "\n",
    "z_hat = test_mfa.posterior_z(test_x, k=0)\n",
    "assert z_hat.shape == (32, 3), f\"Wrong posterior_z shape: {z_hat.shape}\"\n",
    "\n",
    "decomp = test_mfa.decompose(test_x)\n",
    "assert decomp[\"assignments\"].shape == (32,)\n",
    "# Check reconstruction: centroid + local_offset + noise ≈ x\n",
    "recon = decomp[\"centroids\"] + decomp[\"local_offsets\"] + decomp[\"noise\"]\n",
    "assert torch.allclose(recon, test_x, atol=1e-5), \"Decomposition must reconstruct x\"\n",
    "\n",
    "print(\"✓ MFA tests passed\")\n",
    "print(f\"  Component assignments: {decomp['assignments'].unique().tolist()}\")\n",
    "print(f\"  Responsibilities entropy: {-(resp * resp.log()).sum(-1).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "## Part 4: Train the MFA\n",
    "\n",
    "The paper initializes centroids with K-means, then trains all parameters via gradient descent on negative log-likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, C_k) \\right)$$\n",
    "\n",
    "We use a small $K$ here for tractability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_init(\n",
    "    data: Float[Tensor, \"n d\"],\n",
    "    K: int,\n",
    "    n_iters: int = 30,\n",
    ") -> Float[Tensor, \"K d\"]:\n",
    "    \"\"\"\n",
    "    K-means clustering to initialize MFA centroids.\n",
    "    Returns cluster centers.\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    # Random initialization\n",
    "    indices = torch.randperm(n)[:K]\n",
    "    centers = data[indices].clone()\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        # Assign to nearest center\n",
    "        dists = torch.cdist(data, centers)  # (n, K)\n",
    "        assignments = dists.argmin(dim=-1)  # (n,)\n",
    "\n",
    "        # Update centers\n",
    "        new_centers = torch.zeros_like(centers)\n",
    "        for k in range(K):\n",
    "            mask = assignments == k\n",
    "            if mask.sum() > 0:\n",
    "                new_centers[k] = data[mask].mean(dim=0)\n",
    "            else:\n",
    "                new_centers[k] = data[torch.randint(n, (1,))]\n",
    "        centers = new_centers\n",
    "\n",
    "    counts = [(assignments == k).sum().item() for k in range(K)]\n",
    "    print(f\"K-means cluster sizes: {counts}\")\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-mfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "K = 64          # Number of components (paper uses 1K-32K; we use fewer)\n",
    "rank = 10       # Latent rank per component (same as paper)\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "n_epochs = 40\n",
    "\n",
    "# Initialize MFA with K-means centroids\n",
    "torch.manual_seed(42)\n",
    "print(f\"Running K-means with K={K}...\")\n",
    "init_centers = kmeans_init(activations, K)\n",
    "\n",
    "mfa = MFA(d=d_model, K=K, rank=rank).to(device)\n",
    "with torch.no_grad():\n",
    "    mfa.mus.copy_(init_centers.to(device))\n",
    "\n",
    "n_params = sum(p.numel() for p in mfa.parameters())\n",
    "print(f\"\\nMFA: {n_params:,} params ({K} components, rank {rank})\")\n",
    "print(f\"  Centroids: {K} × {d_model} = {K * d_model:,}\")\n",
    "print(f\"  Loadings:  {K} × {d_model} × {rank} = {K * d_model * rank:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    TensorDataset(activations), batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "optimizer = torch.optim.Adam(mfa.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mfa.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for (x_batch,) in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "\n",
    "        # TODO: Compute negative log-likelihood loss\n",
    "        # loss = -mfa.log_prob(x_batch).mean()\n",
    "        \n",
    "        # TODO: Backprop and optimizer step\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        # Check component utilization\n",
    "        with torch.no_grad():\n",
    "            sample = activations[:2000].to(device)\n",
    "            resp = mfa.responsibilities(sample)\n",
    "            assignments = resp.argmax(dim=-1)\n",
    "            n_used = assignments.unique().shape[0]\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | NLL: {avg_loss:.2f} | Components used: {n_used}/{K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.5))\n",
    "ax.plot(losses, linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Negative Log-Likelihood\")\n",
    "ax.set_title(\"MFA Training\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "## Part 5: Interpret Components via Logit Lens\n",
    "\n",
    "The paper's key finding: MFA components are **naturally interpretable**.\n",
    "\n",
    "- **Centroids** ($\\mu_k$) encode broad thematic regions (e.g., \"genres\", \"emotions\", \"sports\")\n",
    "- **Loading columns** ($W_k$) encode fine-grained distinctions within a region (e.g., within \"genres\": fantasy vs. thriller vs. sitcom)\n",
    "\n",
    "We inspect components by projecting centroids and loading directions through the unembedding matrix (logit lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest components (most assigned activations)\n",
    "with torch.no_grad():\n",
    "    all_resp = []\n",
    "    for i in range(0, activations.shape[0], 2000):\n",
    "        batch = activations[i:i+2000].to(device)\n",
    "        all_resp.append(mfa.responsibilities(batch).cpu())\n",
    "    all_resp = torch.cat(all_resp, dim=0)\n",
    "    all_assignments = all_resp.argmax(dim=-1)\n",
    "\n",
    "# Count assignments per component\n",
    "counts = torch.zeros(K)\n",
    "for k in range(K):\n",
    "    counts[k] = (all_assignments == k).sum()\n",
    "\n",
    "top_components = counts.argsort(descending=True)[:10]\n",
    "print(\"Top 10 components by size:\")\n",
    "for rank_idx, k in enumerate(top_components):\n",
    "    k = k.item()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Component {k} ({counts[k].int().item()} activations, {100*counts[k]/len(all_assignments):.1f}%)\")\n",
    "    \n",
    "    # Centroid: what tokens does this region promote?\n",
    "    centroid = mfa.mus[k].detach().cpu()\n",
    "    print(f\"  Centroid top tokens: \", end=\"\")\n",
    "    for tok, val in logit_lens(centroid, top_k=8):\n",
    "        print(f\"'{tok.strip()}'({val:.1f})\", end=\"  \")\n",
    "    print()\n",
    "    \n",
    "    # Loading directions: what fine-grained distinctions?\n",
    "    W_k = mfa.Ws[k].detach().cpu()  # (d, rank)\n",
    "    for r in range(min(3, rank)):  # first 3 loading directions\n",
    "        direction = W_k[:, r]\n",
    "        # Show tokens promoted by +direction and -direction\n",
    "        pos_tokens = logit_lens(direction, top_k=5)\n",
    "        neg_tokens = logit_lens(-direction, top_k=5)\n",
    "        pos_str = \", \".join(f\"'{t.strip()}'\" for t, v in pos_tokens[:4])\n",
    "        neg_str = \", \".join(f\"'{t.strip()}'\" for t, v in neg_tokens[:4])\n",
    "        print(f\"  Loading {r}: [{neg_str}] ←→ [{pos_str}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show actual tokens assigned to a few components\n",
    "print(\"Tokens assigned to top components:\\n\")\n",
    "for k in top_components[:5]:\n",
    "    k = k.item()\n",
    "    mask = all_assignments == k\n",
    "    component_tokens = token_ids[mask]\n",
    "    # Sample up to 30 tokens\n",
    "    sample_idx = torch.randperm(component_tokens.shape[0])[:30]\n",
    "    decoded = [tokenizer.decode([t.item()]).strip() for t in component_tokens[sample_idx]]\n",
    "    # Show unique tokens\n",
    "    unique_toks = list(dict.fromkeys(decoded))[:20]\n",
    "    print(f\"Component {k} ({counts[k].int().item()} tokens):\")\n",
    "    print(f\"  {', '.join(repr(t) for t in unique_toks)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "## Part 6: Decompose Activations\n",
    "\n",
    "Every activation decomposes into three parts:\n",
    "\n",
    "$$x = \\underbrace{\\mu_k}_{\\text{centroid}} + \\underbrace{W_k \\hat{z}_k}_{\\text{local offset}} + \\underbrace{\\epsilon}_{\\text{residual noise}}$$\n",
    "\n",
    "The paper shows this decomposition has a simple 2-segment trajectory in PCA space (centroid, then local refinement), unlike SAEs which accumulate many small features.\n",
    "\n",
    "The paper also measures **Interpretability Fraction (IF)**: what fraction of the reconstruction magnitude comes from interpretable features. MFA achieves IF ≈ 0.96 vs SAE ≈ 0.29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decompose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose a batch of activations\n",
    "with torch.no_grad():\n",
    "    test_x = activations[:2000].to(device)\n",
    "    decomp = mfa.decompose(test_x)\n",
    "\n",
    "centroids = decomp[\"centroids\"].cpu()\n",
    "local_offsets = decomp[\"local_offsets\"].cpu()\n",
    "noise = decomp[\"noise\"].cpu()\n",
    "x_cpu = test_x.cpu()\n",
    "\n",
    "# Verify reconstruction\n",
    "recon = centroids + local_offsets + noise\n",
    "assert torch.allclose(recon, x_cpu, atol=1e-4), \"Decomposition must reconstruct x!\"\n",
    "\n",
    "# Measure contribution of each part (like the paper's IF metric)\n",
    "centroid_norm = centroids.norm(dim=-1).mean()\n",
    "offset_norm = local_offsets.norm(dim=-1).mean()\n",
    "noise_norm = noise.norm(dim=-1).mean()\n",
    "total_norm = x_cpu.norm(dim=-1).mean()\n",
    "\n",
    "print(\"Decomposition magnitude analysis:\")\n",
    "print(f\"  ||x||          = {total_norm:.3f}\")\n",
    "print(f\"  ||centroid||   = {centroid_norm:.3f} ({100*centroid_norm/total_norm:.1f}%)\")\n",
    "print(f\"  ||local_offset|| = {offset_norm:.3f} ({100*offset_norm/total_norm:.1f}%)\")\n",
    "print(f\"  ||noise||      = {noise_norm:.3f} ({100*noise_norm/total_norm:.1f}%)\")\n",
    "print(f\"\\nInterpretable fraction (centroid + offset): {100*(centroid_norm + offset_norm)/total_norm:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decomposition trajectories in PCA space\n",
    "# Paper Figure 4: MFA has a simple 2-segment path (origin -> centroid -> x)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_show = 200\n",
    "pca = PCA(n_components=2).fit(x_cpu[:1000].numpy())\n",
    "\n",
    "origin_2d = pca.transform(np.zeros((1, d_model)))\n",
    "centroids_2d = pca.transform(centroids[:n_show].numpy())\n",
    "full_2d = pca.transform(x_cpu[:n_show].numpy())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "# Draw trajectories: origin -> centroid -> full activation\n",
    "for i in range(n_show):\n",
    "    # Segment 1: origin to centroid (broad region)\n",
    "    ax.plot(\n",
    "        [origin_2d[0, 0], centroids_2d[i, 0]],\n",
    "        [origin_2d[0, 1], centroids_2d[i, 1]],\n",
    "        c=\"steelblue\", alpha=0.1, linewidth=0.5\n",
    "    )\n",
    "    # Segment 2: centroid to full (local refinement)\n",
    "    ax.plot(\n",
    "        [centroids_2d[i, 0], full_2d[i, 0]],\n",
    "        [centroids_2d[i, 1], full_2d[i, 1]],\n",
    "        c=\"coral\", alpha=0.15, linewidth=0.5\n",
    "    )\n",
    "\n",
    "ax.scatter(*origin_2d.T, c=\"black\", s=100, zorder=5, marker=\"*\", label=\"Origin\")\n",
    "ax.scatter(centroids_2d[:, 0], centroids_2d[:, 1], c=\"steelblue\", s=5, alpha=0.4, label=\"Centroids\")\n",
    "ax.scatter(full_2d[:, 0], full_2d[:, 1], c=\"coral\", s=5, alpha=0.4, label=\"Activations\")\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_title(\"MFA Decomposition Trajectories in PCA Space\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Blue: origin → centroid (which region?)\")\n",
    "print(\"Red:  centroid → activation (local variation within region)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-header",
   "metadata": {},
   "source": [
    "## Part 7: Steering with MFA\n",
    "\n",
    "The paper defines two types of interventions:\n",
    "\n",
    "**Centroid intervention** (steer toward a region):\n",
    "$$f_\\mu(x) = (1 - \\alpha) \\cdot x + \\alpha \\cdot \\mu_k$$\n",
    "\n",
    "This interpolates the activation toward the centroid of component $k$.\n",
    "\n",
    "**Local offset intervention** (refine within a region):\n",
    "$$f_w(x) = x + W_k v$$\n",
    "\n",
    "This adds a displacement along the component's loading directions.\n",
    "\n",
    "We'll demonstrate both and show that they produce different effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steering-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_steer(\n",
    "    x: Float[Tensor, \"batch d\"],\n",
    "    centroid: Float[Tensor, \"d\"],\n",
    "    alpha: float,\n",
    ") -> Float[Tensor, \"batch d\"]:\n",
    "    \"\"\"\n",
    "    Centroid intervention: interpolate toward a component's centroid.\n",
    "    f(x) = (1 - alpha) * x + alpha * mu_k\n",
    "    \"\"\"\n",
    "    # TODO: Implement centroid interpolation\n",
    "    pass\n",
    "\n",
    "\n",
    "def local_offset_steer(\n",
    "    x: Float[Tensor, \"batch d\"],\n",
    "    W_k: Float[Tensor, \"d rank\"],\n",
    "    v: Float[Tensor, \"rank\"],\n",
    "    alpha: float = 1.0,\n",
    ") -> Float[Tensor, \"batch d\"]:\n",
    "    \"\"\"\n",
    "    Local offset intervention: add displacement along loading directions.\n",
    "    f(x) = x + alpha * W_k @ v\n",
    "    \"\"\"\n",
    "    # TODO: Implement local offset steering\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steering-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an interesting component to steer toward\n",
    "# Use one of the top components\n",
    "target_k = top_components[0].item()\n",
    "\n",
    "centroid_k = mfa.mus[target_k].detach().cpu()\n",
    "W_k = mfa.Ws[target_k].detach().cpu()\n",
    "\n",
    "print(f\"Steering toward component {target_k}\")\n",
    "print(f\"Centroid promotes: \", end=\"\")\n",
    "for tok, val in logit_lens(centroid_k, top_k=8):\n",
    "    print(f\"'{tok.strip()}'({val:.1f})\", end=\"  \")\n",
    "print()\n",
    "\n",
    "# Take a batch of activations NOT assigned to this component\n",
    "other_mask = all_assignments != target_k\n",
    "other_acts = activations[other_mask][:500]\n",
    "\n",
    "# Centroid steering at different strengths\n",
    "print(f\"\\n--- Centroid Steering (interpolate toward mu_{target_k}) ---\")\n",
    "alphas = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "for alpha in alphas:\n",
    "    steered = centroid_steer(other_acts, centroid_k, alpha)\n",
    "    mean_steered = steered.mean(dim=0)\n",
    "    top_toks = logit_lens(mean_steered, top_k=5)\n",
    "    tok_str = \", \".join(f\"'{t.strip()}'\" for t, v in top_toks)\n",
    "    print(f\"  α={alpha:.1f}: {tok_str}\")\n",
    "\n",
    "# Local offset steering along first loading direction\n",
    "print(f\"\\n--- Local Offset Steering (along loading 0 of component {target_k}) ---\")\n",
    "# Get activations that ARE assigned to this component\n",
    "in_mask = all_assignments == target_k\n",
    "in_acts = activations[in_mask][:500]\n",
    "\n",
    "strengths = [-3.0, -1.0, 0.0, 1.0, 3.0]\n",
    "for s in strengths:\n",
    "    v = torch.zeros(rank)\n",
    "    v[0] = s  # push along first loading direction\n",
    "    steered = local_offset_steer(in_acts, W_k, v)\n",
    "    mean_steered = steered.mean(dim=0)\n",
    "    top_toks = logit_lens(mean_steered, top_k=5)\n",
    "    tok_str = \", \".join(f\"'{t.strip()}'\" for t, v in top_toks)\n",
    "    print(f\"  v[0]={s:+.1f}: {tok_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steering-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: centroid steering vs naive direction steering (DiffMean baseline)\n",
    "# DiffMean: steer along the difference in means between component k and everything else\n",
    "\n",
    "in_mean = activations[in_mask].mean(dim=0)\n",
    "out_mean = activations[other_mask].mean(dim=0)\n",
    "diff_mean_dir = in_mean - out_mean\n",
    "diff_mean_dir = diff_mean_dir / diff_mean_dir.norm()\n",
    "\n",
    "# Measure how \"on-manifold\" steered activations are\n",
    "# by computing their log-likelihood under the MFA\n",
    "test_acts = other_acts[:200].to(device)\n",
    "\n",
    "print(\"Steering comparison: MFA centroid vs DiffMean direction\")\n",
    "print(f\"{'Alpha':<8} {'Centroid NLL':>14} {'DiffMean NLL':>14}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for alpha in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    with torch.no_grad():\n",
    "        # Centroid steering\n",
    "        steered_c = centroid_steer(test_acts, centroid_k.to(device), alpha)\n",
    "        nll_c = -mfa.log_prob(steered_c).mean().item()\n",
    "\n",
    "        # DiffMean steering (additive, scaled by alpha * distance to centroid)\n",
    "        scale = alpha * (centroid_k.to(device) - test_acts.mean(dim=0)).norm()\n",
    "        steered_d = test_acts + scale * diff_mean_dir.to(device)\n",
    "        nll_d = -mfa.log_prob(steered_d).mean().item()\n",
    "\n",
    "    print(f\"{alpha:<8.1f} {nll_c:>14.2f} {nll_d:>14.2f}\")\n",
    "\n",
    "print(\"\\nLower NLL = more on-manifold. Centroid steering should stay more on-manifold\")\n",
    "print(\"because it interpolates between valid activation positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part8-header",
   "metadata": {},
   "source": [
    "## Part 8: Multi-Gaussian Concept Neighborhoods\n",
    "\n",
    "The paper finds that concepts aren't captured by single components but by **neighborhoods of nearby Gaussians**. For example, \"emotions\" might be a cluster of sub-Gaussians for happiness, surprise, anger, etc.\n",
    "\n",
    "We verify this by finding which components are neighbors in centroid space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neighborhoods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise distances between centroids\n",
    "with torch.no_grad():\n",
    "    centroids_all = mfa.mus.detach().cpu()\n",
    "    centroid_dists = torch.cdist(centroids_all, centroids_all)\n",
    "\n",
    "# For each of top-5 components, show their nearest neighbors\n",
    "print(\"Component neighborhoods (nearest centroids in activation space):\\n\")\n",
    "for k in top_components[:5]:\n",
    "    k = k.item()\n",
    "    dists_k = centroid_dists[k]\n",
    "    nearest = dists_k.argsort()[1:6]  # skip self\n",
    "\n",
    "    # Parent centroid\n",
    "    parent_toks = logit_lens(centroids_all[k], top_k=5)\n",
    "    parent_str = \", \".join(f\"'{t.strip()}'\" for t, v in parent_toks)\n",
    "    print(f\"Component {k}: {parent_str}\")\n",
    "\n",
    "    for neighbor in nearest:\n",
    "        n = neighbor.item()\n",
    "        d = dists_k[n].item()\n",
    "        neighbor_toks = logit_lens(centroids_all[n], top_k=5)\n",
    "        neighbor_str = \", \".join(f\"'{t.strip()}'\" for t, v in neighbor_toks)\n",
    "        print(f\"  → {n} (dist={d:.2f}): {neighbor_str}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component structure: PCA of centroids, colored by neighborhood\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_centroids = PCA(n_components=2).fit_transform(centroids_all.numpy())\n",
    "\n",
    "# Color by cluster (use hierarchical clustering on centroids)\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "Z = linkage(centroids_all.numpy(), method=\"ward\")\n",
    "cluster_labels = fcluster(Z, t=8, criterion=\"maxclust\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    pca_centroids[:, 0], pca_centroids[:, 1],\n",
    "    c=cluster_labels, cmap=\"tab10\",\n",
    "    s=counts.numpy() / counts.max().item() * 200 + 10,  # size by count\n",
    "    alpha=0.7, edgecolors=\"white\", linewidth=0.5\n",
    ")\n",
    "\n",
    "# Annotate top components\n",
    "for k in top_components[:8]:\n",
    "    k = k.item()\n",
    "    top_tok = logit_lens(centroids_all[k], top_k=1)[0][0].strip()\n",
    "    ax.annotate(\n",
    "        f\"{k}:'{top_tok}'\", (pca_centroids[k, 0], pca_centroids[k, 1]),\n",
    "        fontsize=7, ha=\"center\", va=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8, edgecolor=\"none\")\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"MFA Centroids in PCA Space ({K} components)\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Bubble size ∝ number of assigned activations.\")\n",
    "print(\"Color = hierarchical cluster of centroids.\")\n",
    "print(\"Nearby centroids with same color = concept neighborhoods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've implemented the core components of the MFA paper:\n",
    "\n",
    "1. **Factor Analysis** — the generative model $x = \\mu + Wz + \\epsilon$ with Woodbury-efficient log-likelihood\n",
    "2. **Mixture of Factor Analyzers** — K components with centroids, loadings, shared noise\n",
    "3. **Component assignment** — responsibilities $R_k(x)$ via Bayes' rule\n",
    "4. **Activation decomposition** — centroid + local offset + noise (3-part split)\n",
    "5. **Logit lens interpretation** — centroids encode broad themes, loadings encode fine distinctions\n",
    "6. **Two types of steering** — centroid interpolation (change region) vs local offset (refine within region)\n",
    "7. **Concept neighborhoods** — related Gaussians cluster together in centroid space\n",
    "\n",
    "### Key takeaways from the paper:\n",
    "\n",
    "- **Beyond single directions**: Concepts have nonlinear, multi-cluster structure. MFA captures this; linear probes and SAEs don't.\n",
    "- **Interpretability fraction**: MFA decompositions are ~96% interpretable (vs ~29% for SAEs) because the centroid + offset structure is inherently meaningful.\n",
    "- **Competitive steering**: MFA often outperforms SAEs on causal steering benchmarks, especially for broad concepts.\n",
    "- **Scaling**: The paper uses K=1K to 32K components. More components split broad Gaussians into finer sub-concepts rather than discovering entirely new structure.\n",
    "\n",
    "### Things we didn't cover:\n",
    "- RAVEL/MCQA localization benchmarks with DBM\n",
    "- Full causal steering evaluation with LLM-as-judge\n",
    "- Comparison at full scale (100M activations, 32K components)\n",
    "- Narrow vs broad Gaussian classification\n",
    "- The SAE decomposition trajectory comparison (Figure 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}