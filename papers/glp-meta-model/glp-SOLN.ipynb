{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Generative Latent Prior (GLP): Learning a Diffusion Model over LLM Activations\n",
    "\n",
    "**Paper**: [Learning a Generative Meta-Model of LLM Activations](https://arxiv.org/abs/2602.06964) (Luo et al., 2025)\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "Instead of using PCA or Sparse Autoencoders (SAEs) to analyze LLM internals, **train a diffusion model on residual stream activations**. This \"meta-model\" learns the activation manifold without imposing structural assumptions (linearity, sparsity).\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "1. **On-manifold steering**: When you steer an LLM by adding a direction vector to activations (e.g., \"make output more positive\"), large perturbations push activations off the natural manifold, causing gibberish. A diffusion model can project steered activations *back onto the manifold* (like SDEdit for images).\n",
    "\n",
    "2. **Interpretable meta-neurons**: The diffusion model's internal neurons learn to isolate semantic concepts into individual units—better than SAE features or raw LLM neurons.\n",
    "\n",
    "3. **Predictable scaling**: Diffusion loss follows a power law with compute, and downstream utility (steering quality, probe accuracy) tracks this loss.\n",
    "\n",
    "### Method: Flow Matching\n",
    "\n",
    "GLP uses **flow matching**, a variant of diffusion:\n",
    "\n",
    "- **Forward process**: Linearly interpolate between data and noise: $z_t = (1-t)z_0 + t\\epsilon$, where $t \\in [0,1]$\n",
    "- **Velocity target**: The network learns to predict $u = \\epsilon - z_0$ (the direction from data to noise)\n",
    "- **Reverse process**: Start from noise $z_1 \\sim \\mathcal{N}(0,I)$ and iteratively denoise using predicted velocity\n",
    "- **Architecture**: Stack of SwiGLU MLP blocks (no attention—each token is modeled independently)\n",
    "\n",
    "### On-Manifold Steering Algorithm\n",
    "\n",
    "1. Apply steering: $a_{\\text{edit}} = a + \\alpha \\cdot w$ (where $w$ is a steering direction)\n",
    "2. Add noise to $t_{\\text{start}}$: $a_{\\text{noisy}} = (1-t_{\\text{start}}) \\cdot a_{\\text{edit}} + t_{\\text{start}} \\cdot \\epsilon$\n",
    "3. Run diffusion denoising from $t_{\\text{start}}$ back to $t=0$ (NOT from $t=1$, so we preserve structure)\n",
    "4. This projects the steered activation back onto the learned manifold\n",
    "\n",
    "### Exercise Overview\n",
    "\n",
    "You will:\n",
    "1. **Extract activations** from a small LLM using `nnsight`\n",
    "2. **Implement flow matching** (forward process + velocity prediction loss)\n",
    "3. **Train a tiny GLP** on those activations\n",
    "4. **Implement on-manifold steering** (the SDEdit-style projection)\n",
    "5. **Probe meta-neurons** to see if they capture interpretable concepts\n",
    "\n",
    "Estimated time: ~45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "## Part 1: Extract Activations from a Small LLM\n",
    "\n",
    "We use `nnsight` to collect residual stream activations from a middle layer of a small language model. The paper uses ~1B activations; we'll use a much smaller set for tractability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = LanguageModel(model_name, device_map=device, dispatch=True)\n",
    "tokenizer = model.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "d_model = model.config.hidden_size\n",
    "target_layer = n_layers // 2  # Middle layer, as in the paper\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Layers: {n_layers}, d_model: {d_model}\")\n",
    "print(f\"Extracting from layer {target_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-acts",
   "metadata": {},
   "outputs": [],
   "source": "# Load a small text corpus\nds = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n\n# Collect activations\nall_activations = []\nall_tokens = []\nnum_texts = 200\nmax_len = 128\n\ntexts = []\nfor i, example in enumerate(ds):\n    if i >= num_texts:\n        break\n    texts.append(example[\"text\"][:512])  # truncate long texts\n\nprint(f\"Collected {len(texts)} texts, tokenizing...\")\n\nbatch_size = 20\nfor i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting activations\"):\n    batch_texts = texts[i:i + batch_size]\n    tokens = tokenizer(\n        batch_texts, return_tensors=\"pt\", padding=True,\n        truncation=True, max_length=max_len\n    )\n    input_ids = tokens[\"input_ids\"].to(device)\n    attention_mask = tokens[\"attention_mask\"].to(device)\n\n    with torch.no_grad():\n        with model.trace(input_ids, attention_mask=attention_mask):\n            # Grab residual stream output at the target layer\n            hidden = model.model.layers[target_layer].output.save()\n\n    acts = hidden.float().cpu()  # (batch, seq_len, d_model)\n    mask = attention_mask.cpu().bool()\n\n    # Flatten: keep only non-padding, non-BOS tokens (as in the paper)\n    for b in range(acts.shape[0]):\n        valid = mask[b].clone()\n        valid[0] = False  # skip BOS\n        if valid.sum() > 0:\n            all_activations.append(acts[b, valid])\n            all_tokens.append(input_ids[b, valid].cpu())\n\nactivations = torch.cat(all_activations, dim=0)  # (N, d_model)\ntoken_ids = torch.cat(all_tokens, dim=0)  # (N,)\n\nprint(f\"\\nCollected {activations.shape[0]:,} activation vectors of dim {activations.shape[1]}\")\nprint(f\"Activation stats: mean={activations.mean():.3f}, std={activations.std():.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize-acts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize activations (the paper does this for the steering algorithm)\n",
    "act_mean = activations.mean(dim=0)\n",
    "act_std = activations.std(dim=0)\n",
    "\n",
    "activations_normed = (activations - act_mean) / (act_std + 1e-8)\n",
    "\n",
    "print(f\"Normalized stats: mean={activations_normed.mean():.4f}, std={activations_normed.std():.4f}\")\n",
    "\n",
    "# Train/val split\n",
    "n = activations_normed.shape[0]\n",
    "n_val = min(2000, n // 10)\n",
    "perm = torch.randperm(n)\n",
    "val_acts = activations_normed[perm[:n_val]]\n",
    "train_acts = activations_normed[perm[n_val:]]\n",
    "train_token_ids = token_ids[perm[n_val:]]\n",
    "val_token_ids = token_ids[perm[:n_val]]\n",
    "\n",
    "print(f\"Train: {train_acts.shape[0]:,}, Val: {val_acts.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: Implement Flow Matching\n",
    "\n",
    "The core of GLP is a **flow matching** diffusion model. Unlike DDPM which predicts noise, flow matching predicts the *velocity* of a linear interpolation between data and noise.\n",
    "\n",
    "**Forward process** (adding noise):\n",
    "$$z_t = (1 - t) \\cdot z_0 + t \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I), \\quad t \\in [0, 1]$$\n",
    "\n",
    "**Velocity target** (what the network learns to predict):\n",
    "$$u = \\epsilon - z_0$$\n",
    "\n",
    "**Training loss** (MSE on predicted velocity):\n",
    "$$\\mathcal{L} = \\mathbb{E}_{z_0, \\epsilon, t} \\left[ \\| u_\\theta(z_t, t) - (\\epsilon - z_0) \\|^2 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flow-matching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_matching_forward(\n",
    "    z0: Float[Tensor, \"batch dim\"],\n",
    "    t: Float[Tensor, \"batch 1\"],\n",
    ") -> tuple[\n",
    "    Float[Tensor, \"batch dim\"],  # z_t (noisy sample)\n",
    "    Float[Tensor, \"batch dim\"],  # velocity target\n",
    "]:\n",
    "    \"\"\"\n",
    "    Flow matching forward process.\n",
    "    \n",
    "    Given clean data z0 and timesteps t, produce:\n",
    "      - z_t: the noisy interpolation\n",
    "      - velocity: the target for the denoiser to predict\n",
    "    \n",
    "    Args:\n",
    "        z0: Clean activation vectors [batch, dim]\n",
    "        t: Timesteps in [0, 1], shape [batch, 1]\n",
    "    \n",
    "    Returns:\n",
    "        (z_t, velocity_target)\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn_like(z0)\n",
    "    z_t = (1 - t) * z0 + t * epsilon\n",
    "    velocity = epsilon - z0\n",
    "    return z_t, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-flow-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test flow matching forward\n",
    "torch.manual_seed(42)\n",
    "test_z0 = torch.randn(4, 8)\n",
    "test_t = torch.zeros(4, 1)  # t=0 should return z0\n",
    "\n",
    "zt, vel = flow_matching_forward(test_z0, test_t)\n",
    "\n",
    "# At t=0, z_t should equal z0 (no noise added)\n",
    "# Note: velocity target doesn't depend on t, so we just check z_t\n",
    "assert zt.shape == test_z0.shape, f\"Wrong shape: {zt.shape}\"\n",
    "assert vel.shape == test_z0.shape, f\"Wrong velocity shape: {vel.shape}\"\n",
    "assert torch.allclose(zt, test_z0, atol=1e-6), \"At t=0, z_t should equal z0\"\n",
    "\n",
    "# At t=1, z_t should be pure noise (epsilon)\n",
    "test_t_one = torch.ones(4, 1)\n",
    "zt_one, vel_one = flow_matching_forward(test_z0, test_t_one)\n",
    "# z_t = 0 * z0 + 1 * epsilon = epsilon, and velocity = epsilon - z0\n",
    "# So z_t should equal vel + z0\n",
    "assert torch.allclose(zt_one, vel_one + test_z0, atol=1e-6), \"At t=1, z_t should equal epsilon = velocity + z0\"\n",
    "\n",
    "print(\"✓ Flow matching forward process tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Build the GLP Denoiser Network\n",
    "\n",
    "The paper uses a stack of **SwiGLU MLP blocks** with **timestep conditioning** via multiplicative modulation.\n",
    "\n",
    "Architecture:\n",
    "- Width = 2× activation dimension\n",
    "- MLP expansion = 2× width (so 4× activation dim)\n",
    "- Timestep conditioning: modulate the SwiGLU gate pre-activation\n",
    "- Residual connections\n",
    "- No attention (each token is independent)\n",
    "\n",
    "For this exercise, we'll build a smaller version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiglu-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"Embed scalar timestep into a vector using sinusoidal encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 2, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t: Float[Tensor, \"batch\"]) -> Float[Tensor, \"batch dim\"]:\n",
    "        # Sinusoidal encoding (same as positional encoding)\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-np.log(10000.0) * torch.arange(half, device=t.device) / half)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.mlp(emb)\n",
    "\n",
    "\n",
    "class SwiGLUBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU MLP block with timestep conditioning.\n",
    "    \n",
    "    The paper modulates the gate pre-activation multiplicatively with the timestep embedding.\n",
    "    \n",
    "    SwiGLU: output = (W_up @ x) * silu(W_gate @ x) \n",
    "    With timestep: output = (W_up @ x) * silu(W_gate @ x * (1 + t_scale))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, expansion: int = 4):\n",
    "        super().__init__()\n",
    "        hidden = dim * expansion\n",
    "\n",
    "        self.W_gate = nn.Linear(dim, hidden)\n",
    "        self.W_up = nn.Linear(dim, hidden)\n",
    "        self.W_down = nn.Linear(hidden, dim)\n",
    "        self.t_proj = nn.Linear(dim, hidden)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch dim\"],\n",
    "        t_emb: Float[Tensor, \"batch dim\"],\n",
    "    ) -> Float[Tensor, \"batch dim\"]:\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        gate = self.W_gate(x)\n",
    "        scale = self.t_proj(t_emb)\n",
    "        gate = gate * (1 + scale)  # timestep modulation\n",
    "        \n",
    "        up = self.W_up(x)\n",
    "        hidden = up * F.silu(gate)  # SwiGLU\n",
    "        out = self.W_down(hidden)\n",
    "        \n",
    "        return residual + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glp-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Generative Latent Prior: a flow-matching diffusion model for LLM activations.\n",
    "    \n",
    "    Architecture: input_proj -> N x SwiGLU blocks -> output_proj\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        act_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_blocks: int = 4,\n",
    "        expansion: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act_dim = act_dim\n",
    "\n",
    "        self.time_embed = TimestepEmbedding(hidden_dim)\n",
    "        self.input_proj = nn.Linear(act_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwiGLUBlock(hidden_dim, expansion) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, act_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z_t: Float[Tensor, \"batch act_dim\"],\n",
    "        t: Float[Tensor, \"batch\"],\n",
    "    ) -> Float[Tensor, \"batch act_dim\"]:\n",
    "        t_emb = self.time_embed(t)\n",
    "        h = self.input_proj(z_t)\n",
    "        for block in self.blocks:\n",
    "            h = block(h, t_emb)\n",
    "        h = self.output_norm(h)\n",
    "        return self.output_proj(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-glp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick shape test\n",
    "torch.manual_seed(0)\n",
    "test_glp = GLP(act_dim=d_model, hidden_dim=d_model * 2, n_blocks=4).to(device)\n",
    "n_params = sum(p.numel() for p in test_glp.parameters())\n",
    "print(f\"GLP parameters: {n_params:,}\")\n",
    "\n",
    "test_input = torch.randn(8, d_model, device=device)\n",
    "test_t = torch.rand(8, device=device)\n",
    "test_out = test_glp(test_input, test_t)\n",
    "\n",
    "assert test_out.shape == (8, d_model), f\"Wrong output shape: {test_out.shape}\"\n",
    "print(f\"✓ GLP output shape correct: {test_out.shape}\")\n",
    "del test_glp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "## Part 4: Train the GLP\n",
    "\n",
    "Training loop:\n",
    "1. Sample a batch of activations $z_0$\n",
    "2. Sample random timesteps $t \\sim \\text{Uniform}(0, 1)$\n",
    "3. Compute noisy samples $z_t$ and velocity targets via `flow_matching_forward`\n",
    "4. Predict velocity with the GLP network\n",
    "5. Minimize MSE between predicted and target velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-glp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (small for tractability)\n",
    "hidden_dim = d_model * 2\n",
    "n_blocks = 4\n",
    "batch_size = 512\n",
    "lr = 5e-4\n",
    "n_epochs = 30\n",
    "\n",
    "glp = GLP(act_dim=d_model, hidden_dim=hidden_dim, n_blocks=n_blocks).to(device)\n",
    "optimizer = torch.optim.AdamW(glp.parameters(), lr=lr, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "train_dataset = TensorDataset(train_acts)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "n_params = sum(p.numel() for p in glp.parameters())\n",
    "print(f\"Training GLP: {n_params:,} params, {len(train_loader)} batches/epoch\")\n",
    "print(f\"Activation dim: {d_model}, Hidden dim: {hidden_dim}, Blocks: {n_blocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    glp.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for (z0_batch,) in train_loader:\n",
    "        z0_batch = z0_batch.to(device)\n",
    "        bs = z0_batch.shape[0]\n",
    "\n",
    "        t = torch.rand(bs, 1, device=device)\n",
    "        z_t, velocity_target = flow_matching_forward(z0_batch, t)\n",
    "        velocity_pred = glp(z_t, t.squeeze(-1))\n",
    "        loss = F.mse_loss(velocity_pred, velocity_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_train = epoch_loss / n_batches\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    # Validation\n",
    "    glp.eval()\n",
    "    with torch.no_grad():\n",
    "        val_z0 = val_acts.to(device)\n",
    "        val_t = torch.rand(val_z0.shape[0], 1, device=device)\n",
    "        val_zt, val_vel = flow_matching_forward(val_z0, val_t)\n",
    "        val_pred = glp(val_zt, val_t.squeeze(-1))\n",
    "        val_loss = F.mse_loss(val_pred, val_vel).item()\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | Train loss: {avg_train:.4f} | Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.5))\n",
    "ax.plot(train_losses, label=\"Train\", linewidth=1.5)\n",
    "ax.plot(val_losses, label=\"Val\", linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Flow Matching Loss (MSE)\")\n",
    "ax.set_title(\"GLP Training\")\n",
    "ax.legend(frameon=False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "## Part 5: Generate Activations & Evaluate Quality\n",
    "\n",
    "**Reverse process** (sampling from noise):\n",
    "\n",
    "Start at $z_1 \\sim \\mathcal{N}(0, I)$ and iteratively step:\n",
    "$$z_{t'} = z_t + \\hat{u}_\\theta(z_t, t) \\cdot (t' - t)$$\n",
    "\n",
    "where we step from $t=1$ down to $t=0$ in small increments.\n",
    "\n",
    "We evaluate using the **Representation Fréchet Distance (FD)**: the Fréchet distance between the distributions of real and generated activations (analogous to FID for images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_glp(\n",
    "    model: GLP,\n",
    "    n_samples: int,\n",
    "    act_dim: int,\n",
    "    num_steps: int = 50,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Float[Tensor, \"n_samples act_dim\"]:\n",
    "    \"\"\"\n",
    "    Generate activation samples via the reverse diffusion process.\n",
    "    \n",
    "    Start from z_1 ~ N(0, I) and iteratively denoise to z_0.\n",
    "    \n",
    "    Euler method: z_{t'} = z_t + velocity_pred * (t' - t)\n",
    "    where we go from t=1.0 down to t=0.0\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    z = torch.randn(n_samples, act_dim, device=device)\n",
    "    schedule = torch.linspace(1.0, 0.0, num_steps + 1, device=device)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        t_curr = schedule[i]\n",
    "        t_next = schedule[i + 1]\n",
    "        dt = t_next - t_curr  # negative (denoising)\n",
    "        \n",
    "        t_batch = torch.full((n_samples,), t_curr, device=device)\n",
    "        velocity = model(z, t_batch)\n",
    "        z = z + velocity * dt\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frechet-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frechet_distance(\n",
    "    real: Float[Tensor, \"n dim\"],\n",
    "    fake: Float[Tensor, \"m dim\"],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Fréchet Distance between two sets of vectors.\n",
    "    \n",
    "    FD = ||mu_r - mu_f||^2 + Tr(C_r + C_f - 2*(C_r @ C_f)^{1/2})\n",
    "    \n",
    "    This is the activation-space analog of FID.\n",
    "    \"\"\"\n",
    "    mu_r = real.mean(dim=0)\n",
    "    mu_f = fake.mean(dim=0)\n",
    "    \n",
    "    # Covariance matrices\n",
    "    real_centered = real - mu_r\n",
    "    fake_centered = fake - mu_f\n",
    "    cov_r = (real_centered.T @ real_centered) / (real.shape[0] - 1)\n",
    "    cov_f = (fake_centered.T @ fake_centered) / (fake.shape[0] - 1)\n",
    "    \n",
    "    # Mean difference term\n",
    "    diff = mu_r - mu_f\n",
    "    mean_term = (diff @ diff).item()\n",
    "    \n",
    "    # Matrix square root via eigendecomposition\n",
    "    product = cov_r @ cov_f\n",
    "    eigvals = torch.linalg.eigvalsh(product)\n",
    "    eigvals = torch.clamp(eigvals, min=0)  # numerical stability\n",
    "    sqrt_product_trace = eigvals.sqrt().sum().item()\n",
    "    \n",
    "    fd = mean_term + cov_r.trace().item() + cov_f.trace().item() - 2 * sqrt_product_trace\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples and compute FD\n",
    "n_gen = min(2000, val_acts.shape[0])\n",
    "generated = sample_glp(glp, n_gen, d_model, num_steps=50, device=device).cpu()\n",
    "\n",
    "fd_generated = frechet_distance(val_acts[:n_gen], generated)\n",
    "\n",
    "# Baseline: FD between two halves of real data (lower bound)\n",
    "half = n_gen // 2\n",
    "fd_baseline = frechet_distance(val_acts[:half], val_acts[half:2*half])\n",
    "\n",
    "# FD of pure noise (upper bound)\n",
    "noise = torch.randn_like(generated)\n",
    "fd_noise = frechet_distance(val_acts[:n_gen], noise)\n",
    "\n",
    "print(f\"Fréchet Distance:\")\n",
    "print(f\"  Real vs Real (lower bound): {fd_baseline:.2f}\")\n",
    "print(f\"  Real vs GLP generated:      {fd_generated:.2f}\")\n",
    "print(f\"  Real vs Random noise:        {fd_noise:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization (cf. Figure 3 in the paper)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_viz = 1000\n",
    "real_sample = val_acts[:n_viz].numpy()\n",
    "gen_sample = generated[:n_viz].numpy()\n",
    "\n",
    "pca = PCA(n_components=2).fit(real_sample)\n",
    "real_2d = pca.transform(real_sample)\n",
    "gen_2d = pca.transform(gen_sample)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=5, c=\"steelblue\", label=\"Real\")\n",
    "axes[0].scatter(gen_2d[:, 0], gen_2d[:, 1], alpha=0.3, s=5, c=\"coral\", label=\"Generated\")\n",
    "axes[0].legend(frameon=False, markerscale=3)\n",
    "axes[0].set_title(\"PCA: Real vs Generated (50 steps)\")\n",
    "axes[0].set_xlabel(\"PC1\")\n",
    "axes[0].set_ylabel(\"PC2\")\n",
    "\n",
    "# Also show 1-step generation for comparison (should be worse)\n",
    "gen_1step = sample_glp(glp, n_viz, d_model, num_steps=1, device=device).cpu().numpy()\n",
    "gen_1_2d = pca.transform(gen_1step)\n",
    "\n",
    "axes[1].scatter(real_2d[:, 0], real_2d[:, 1], alpha=0.3, s=5, c=\"steelblue\", label=\"Real\")\n",
    "axes[1].scatter(gen_1_2d[:, 0], gen_1_2d[:, 1], alpha=0.3, s=5, c=\"coral\", label=\"Generated\")\n",
    "axes[1].legend(frameon=False, markerscale=3)\n",
    "axes[1].set_title(\"PCA: Real vs Generated (1 step)\")\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "axes[1].set_ylabel(\"PC2\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nWith more steps, generated samples should better cover the real distribution modes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "## Part 6: On-Manifold Steering\n",
    "\n",
    "This is the paper's key practical contribution. When you steer an LLM by adding a direction to activations:\n",
    "- Small perturbations → ok\n",
    "- Large perturbations → off-manifold → gibberish\n",
    "\n",
    "**GLP fix**: Initialize diffusion at an intermediate timestep $t_{\\text{start}}$ (not $t=1$) from the steered activation, then denoise. This is exactly **SDEdit** applied to activation space.\n",
    "\n",
    "The algorithm:\n",
    "1. Steer: $a_{\\text{edit}} = a + \\alpha \\cdot w$\n",
    "2. Standardize to zero mean / unit variance\n",
    "3. Add noise: $a_{\\text{noisy}} = (1 - t_{\\text{start}}) \\cdot a_{\\text{edit}} + t_{\\text{start}} \\cdot \\epsilon$\n",
    "4. Denoise from $t_{\\text{start}}$ → $0$ (preserves more of the edit than starting from $t=1$)\n",
    "5. Unstandardize back to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "on-manifold-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def on_manifold_steer(\n",
    "    model: GLP,\n",
    "    activations: Float[Tensor, \"batch dim\"],\n",
    "    steering_direction: Float[Tensor, \"dim\"],\n",
    "    alpha: float,\n",
    "    t_start: float = 0.5,\n",
    "    num_steps: int = 20,\n",
    "    act_mean: Float[Tensor, \"dim\"] | None = None,\n",
    "    act_std: Float[Tensor, \"dim\"] | None = None,\n",
    ") -> Float[Tensor, \"batch dim\"]:\n",
    "    \"\"\"\n",
    "    Steer activations and project back onto manifold using GLP.\n",
    "    \n",
    "    This is the SDEdit-style algorithm from Section 4 of the paper.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dev = activations.device\n",
    "    bs = activations.shape[0]\n",
    "    \n",
    "    # Step 1: Apply steering\n",
    "    acts_edit = activations + alpha * steering_direction\n",
    "    \n",
    "    # Step 2: Standardize\n",
    "    if act_mean is not None and act_std is not None:\n",
    "        acts_normed = (acts_edit - act_mean) / (act_std + 1e-8)\n",
    "    else:\n",
    "        acts_normed = acts_edit\n",
    "    \n",
    "    # Step 3: Add noise at t_start level\n",
    "    noise = torch.randn_like(acts_normed)\n",
    "    z = (1 - t_start) * acts_normed + t_start * noise\n",
    "    \n",
    "    # Step 4: Denoise from t_start to 0\n",
    "    schedule = torch.linspace(t_start, 0.0, num_steps + 1, device=dev)\n",
    "    for i in range(num_steps):\n",
    "        t_curr = schedule[i]\n",
    "        t_next = schedule[i + 1]\n",
    "        dt = t_next - t_curr\n",
    "        \n",
    "        t_batch = torch.full((bs,), t_curr, device=dev)\n",
    "        velocity = model(z, t_batch)\n",
    "        z = z + velocity * dt\n",
    "    \n",
    "    # Step 5: Unstandardize\n",
    "    if act_mean is not None and act_std is not None:\n",
    "        result = z * (act_std + 1e-8) + act_mean\n",
    "    else:\n",
    "        result = z\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steering-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: steer activations and compare naive vs on-manifold\n",
    "# We'll compute a simple steering direction using the DiffMean method\n",
    "# (difference in mean activations between two groups of tokens)\n",
    "\n",
    "# Split tokens into two groups by a simple heuristic:\n",
    "# tokens that tend to appear in questions vs. statements\n",
    "# We'll use a simpler proxy: high vs low activation norm\n",
    "\n",
    "norms = train_acts.norm(dim=-1)\n",
    "median_norm = norms.median()\n",
    "high_norm = train_acts[norms > median_norm]\n",
    "low_norm = train_acts[norms <= median_norm]\n",
    "\n",
    "# DiffMean steering direction\n",
    "steer_dir = (high_norm.mean(dim=0) - low_norm.mean(dim=0))\n",
    "steer_dir = steer_dir / steer_dir.norm()  # unit vector\n",
    "\n",
    "print(f\"Steering direction norm: {steer_dir.norm():.4f}\")\n",
    "print(f\"Mean activation norm: {train_acts.norm(dim=-1).mean():.4f}\")\n",
    "\n",
    "# Take a batch of test activations\n",
    "test_batch = val_acts[:200].to(device)\n",
    "\n",
    "# Sweep over steering strengths\n",
    "alphas = [0.0, 1.0, 3.0, 5.0, 10.0, 20.0]\n",
    "fd_naive = []\n",
    "fd_glp = []\n",
    "\n",
    "reference = val_acts[200:400]  # separate reference set for FD\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Naive steering (just add the direction)\n",
    "    naive_steered = test_batch + alpha * steer_dir.to(device)\n",
    "    naive_normed = (naive_steered.cpu() - act_mean) / (act_std + 1e-8)  # normalize for FD comparison\n",
    "    fd_n = frechet_distance(val_acts[200:400], naive_normed)\n",
    "    fd_naive.append(fd_n)\n",
    "\n",
    "    # On-manifold steering\n",
    "    if alpha > 0:\n",
    "        glp_steered = on_manifold_steer(\n",
    "            glp, test_batch, steer_dir.to(device), alpha,\n",
    "            t_start=0.5, num_steps=20,\n",
    "            act_mean=act_mean.to(device), act_std=act_std.to(device),\n",
    "        )\n",
    "        glp_normed = (glp_steered.cpu() - act_mean) / (act_std + 1e-8)\n",
    "        fd_g = frechet_distance(val_acts[200:400], glp_normed)\n",
    "    else:\n",
    "        fd_g = fd_n\n",
    "    fd_glp.append(fd_g)\n",
    "\n",
    "    print(f\"α={alpha:5.1f} | FD naive: {fd_n:8.2f} | FD GLP: {fd_g:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.5))\n",
    "ax.plot(alphas, fd_naive, \"o-\", label=\"Naive steering\", linewidth=1.5, markersize=5)\n",
    "ax.plot(alphas, fd_glp, \"s-\", label=\"GLP on-manifold\", linewidth=1.5, markersize=5)\n",
    "ax.set_xlabel(\"Steering strength (α)\")\n",
    "ax.set_ylabel(\"Fréchet Distance from real\")\n",
    "ax.set_title(\"Naive vs On-Manifold Steering\")\n",
    "ax.legend(frameon=False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: At large α, naive steering diverges from the real distribution.\")\n",
    "print(\"GLP on-manifold steering keeps activations closer to the learned manifold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-header",
   "metadata": {},
   "source": [
    "## Part 7: Probing Meta-Neurons\n",
    "\n",
    "The paper's other key finding: the GLP's *internal neurons* (\"meta-neurons\") learn to isolate semantic concepts better than SAE features or raw LLM neurons.\n",
    "\n",
    "To test this, we:\n",
    "1. Run real activations through the GLP's first few layers to get meta-neuron activations\n",
    "2. Define binary concept labels for our tokens (e.g., \"is this a number?\", \"is this punctuation?\")\n",
    "3. Measure how well individual meta-neurons predict each concept (1-D probe AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-meta-neurons",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_meta_neurons(\n",
    "    model: GLP,\n",
    "    activations: Float[Tensor, \"n dim\"],\n",
    "    t: float = 0.0,\n",
    "    layer_idx: int = 1,\n",
    ") -> Float[Tensor, \"n hidden_dim\"]:\n",
    "    \"\"\"\n",
    "    Extract intermediate representations (meta-neurons) from the GLP.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    activations = activations.to(dev)\n",
    "    \n",
    "    bs = activations.shape[0]\n",
    "    t_batch = torch.full((bs,), t, device=dev)\n",
    "    t_emb = model.time_embed(t_batch)\n",
    "    \n",
    "    h = model.input_proj(activations)\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        h = block(h, t_emb)\n",
    "        if i == layer_idx:\n",
    "            break\n",
    "    \n",
    "    return h.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-concepts",
   "metadata": {},
   "outputs": [],
   "source": "# Define binary token-level concepts to probe\ndef make_concept_labels(token_ids: Tensor, tokenizer) -> dict[str, Tensor]:\n    \"\"\"Create binary labels for various token-level concepts.\"\"\"\n    decoded = [tokenizer.decode([tid]) for tid in token_ids.tolist()]\n    \n    concepts = {}\n    \n    # Punctuation\n    punct_chars = set(\".,;:!?-()[]{}\\\"'/\")\n    concepts[\"punctuation\"] = torch.tensor(\n        [1.0 if any(c in punct_chars for c in tok.strip()) and len(tok.strip()) <= 2 else 0.0 for tok in decoded]\n    )\n    \n    # Starts with capital letter\n    concepts[\"capitalized\"] = torch.tensor(\n        [1.0 if tok.strip() and tok.strip()[0].isupper() else 0.0 for tok in decoded]\n    )\n    \n    # Contains a digit\n    concepts[\"has_digit\"] = torch.tensor(\n        [1.0 if any(c.isdigit() for c in tok) else 0.0 for tok in decoded]\n    )\n    \n    # Starts with space (word boundary)\n    concepts[\"word_start\"] = torch.tensor(\n        [1.0 if tok.startswith(\" \") or tok.startswith(\"Ġ\") else 0.0 for tok in decoded]\n    )\n    \n    # Short tokens (1-2 chars after stripping)\n    concepts[\"short_token\"] = torch.tensor(\n        [1.0 if len(tok.strip()) <= 2 else 0.0 for tok in decoded]\n    )\n    \n    for name, labels in concepts.items():\n        pos = labels.sum().int().item()\n        print(f\"  {name}: {pos}/{len(labels)} positive ({100*pos/len(labels):.1f}%)\")\n    \n    return concepts\n\nprint(\"Concept label distributions:\")\nconcept_labels = make_concept_labels(train_token_ids, tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probe-auc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def best_1d_probe_auc(\n",
    "    features: Float[Tensor, \"n dim\"],\n",
    "    labels: Float[Tensor, \"n\"],\n",
    ") -> tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Find the single feature (neuron) that best predicts the binary label.\n",
    "    Returns (best_auc, best_feature_idx).\n",
    "    \n",
    "    This is the 1-D probe from Section 5.1 of the paper.\n",
    "    \"\"\"\n",
    "    # Skip if label is trivial\n",
    "    if labels.sum() < 5 or (1 - labels).sum() < 5:\n",
    "        return 0.5, -1\n",
    "    \n",
    "    features_np = features.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    \n",
    "    best_auc = 0.5\n",
    "    best_idx = -1\n",
    "    \n",
    "    for i in range(features_np.shape[1]):\n",
    "        auc = roc_auc_score(labels_np, features_np[:, i])\n",
    "        # AUC can be < 0.5 if the neuron is anti-correlated; take max(auc, 1-auc)\n",
    "        auc = max(auc, 1 - auc)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_idx = i\n",
    "    \n",
    "    return best_auc, best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-probes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meta-neuron features\n",
    "meta_features = extract_meta_neurons(glp, train_acts.to(device), t=0.0, layer_idx=1).cpu()\n",
    "print(f\"Meta-neuron features shape: {meta_features.shape}\")\n",
    "\n",
    "# Compare: raw LLM activations vs GLP meta-neurons\n",
    "print(f\"\\n{'Concept':<15} {'Raw LLM AUC':>12} {'Meta-neuron AUC':>16}\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "results = {}\n",
    "for concept_name, labels in concept_labels.items():\n",
    "    # Raw LLM activations (normalized)\n",
    "    raw_auc, raw_idx = best_1d_probe_auc(train_acts, labels)\n",
    "    \n",
    "    # GLP meta-neurons\n",
    "    meta_auc, meta_idx = best_1d_probe_auc(meta_features, labels)\n",
    "    \n",
    "    results[concept_name] = {\"raw\": raw_auc, \"meta\": meta_auc}\n",
    "    print(f\"{concept_name:<15} {raw_auc:>12.3f} {meta_auc:>16.3f}\")\n",
    "\n",
    "raw_mean = np.mean([v[\"raw\"] for v in results.values()])\n",
    "meta_mean = np.mean([v[\"meta\"] for v in results.values()])\n",
    "print(\"-\" * 47)\n",
    "print(f\"{'Mean':<15} {raw_mean:>12.3f} {meta_mean:>16.3f}\")\n",
    "print(f\"\\nThe paper finds meta-neurons consistently outperform raw neurons as 1-D probes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-meta-neuron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a top meta-neuron: show highest-activating tokens\n",
    "best_concept = max(results, key=lambda k: results[k][\"meta\"])\n",
    "_, best_meta_idx = best_1d_probe_auc(meta_features, concept_labels[best_concept])\n",
    "\n",
    "neuron_vals = meta_features[:, best_meta_idx]\n",
    "top_k = 20\n",
    "top_indices = neuron_vals.topk(top_k).indices\n",
    "bottom_indices = neuron_vals.topk(top_k, largest=False).indices\n",
    "\n",
    "print(f\"Meta-neuron #{best_meta_idx} (best for '{best_concept}')\")\n",
    "print(f\"\\nTop-{top_k} activating tokens:\")\n",
    "for idx in top_indices:\n",
    "    tok = tokenizer.decode([train_token_ids[idx].item()])\n",
    "    print(f\"  {neuron_vals[idx]:.3f}  '{tok}'\")\n",
    "\n",
    "print(f\"\\nBottom-{top_k} activating tokens:\")\n",
    "for idx in bottom_indices:\n",
    "    tok = tokenizer.decode([train_token_ids[idx].item()])\n",
    "    print(f\"  {neuron_vals[idx]:.3f}  '{tok}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've implemented the core components of the GLP paper:\n",
    "\n",
    "1. **Flow matching** — the forward process ($z_t = (1-t)z_0 + t\\epsilon$) and velocity prediction loss\n",
    "2. **SwiGLU denoiser with timestep conditioning** — the meta-model architecture\n",
    "3. **Reverse sampling** — Euler integration from noise to clean activations\n",
    "4. **On-manifold steering** — SDEdit-style projection that preserves fluency at large steering strengths\n",
    "5. **Meta-neuron probing** — showing the GLP's internal features isolate concepts\n",
    "\n",
    "### Key takeaways from the paper:\n",
    "\n",
    "- **No structural assumptions**: Unlike SAEs (sparsity) or PCA (linearity), diffusion models learn the activation manifold flexibly\n",
    "- **Scaling laws transfer**: Diffusion loss → downstream utility (steering quality, probe accuracy) follows a predictable power law\n",
    "- **Practical**: On-manifold steering expands the Pareto frontier of concept-strength vs fluency\n",
    "- **Interpretable**: Meta-neurons outperform SAE features and raw neurons on 1-D probing tasks\n",
    "\n",
    "### Things we didn't cover (that the paper does):\n",
    "- Training on 1B+ activations with the full scaling study\n",
    "- Sentiment steering with LLM-as-judge evaluation\n",
    "- Persona elicitation (evil/curious/helpful)\n",
    "- Transfer from base model training to instruct model application\n",
    "- Delta LM loss evaluation (replacing activations and measuring perplexity increase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}