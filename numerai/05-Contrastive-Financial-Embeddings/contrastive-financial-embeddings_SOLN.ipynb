{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Financial Embeddings\n",
    "\n",
    "## Context\n",
    "Off-the-shelf text embeddings (NB02) capture general semantics but aren't optimized for financial signal. Contrastive learning fine-tunes embeddings so that texts about stocks with similar returns are close together, and texts about diverging stocks are far apart.\n",
    "\n",
    "## My Experience\n",
    "At Creyon Bio, I used contrastive learning to predict oligo toxicity from 3D electrostatic maps, pushing AUC from 0.73 to 0.88. Same framework applies here: learn a representation where the downstream signal (stock returns) is encoded in embedding similarity.\n",
    "\n",
    "## Pipeline\n",
    "Financial headlines + stock returns → Contrastive pairs (same return quintile = positive, different = negative) → Fine-tune encoder → Financial-signal-optimized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Training Data\n",
    "We need (headline, stock_return) pairs. The contrastive loss will learn embeddings where stocks with similar returns have similar headline embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset: headlines with associated stock returns\n",
    "# In production: real headlines from news APIs + real returns from market data\n",
    "np.random.seed(42)\n",
    "\n",
    "data = []\n",
    "# Generate headlines with implicit sentiment \u2192 return correlation\n",
    "templates_positive = [\n",
    "    \"{company} reports record quarterly revenue exceeding analyst expectations\",\n",
    "    \"{company} announces major partnership deal to expand market reach\",\n",
    "    \"{company} beats earnings estimates by wide margin in Q4\",\n",
    "    \"{company} raises full-year guidance citing strong demand\",\n",
    "    \"{company} wins major government contract worth $2 billion\",\n",
    "    \"{company} launches innovative product to strong customer reception\",\n",
    "    \"{company} expands into new markets with strategic acquisition\",\n",
    "    \"{company} reports surge in subscriber growth beating all estimates\",\n",
    "]\n",
    "templates_negative = [\n",
    "    \"{company} misses revenue expectations amid weakening demand\",\n",
    "    \"{company} announces layoffs affecting 10% of workforce\",\n",
    "    \"{company} faces regulatory investigation over business practices\",\n",
    "    \"{company} warns of lower guidance citing supply chain issues\",\n",
    "    \"{company} reports unexpected quarterly loss shocking analysts\",\n",
    "    \"{company} recalls products due to safety concerns\",\n",
    "    \"{company} loses market share to competitors in core business\",\n",
    "    \"{company} under pressure as key customers reduce orders\",\n",
    "]\n",
    "templates_neutral = [\n",
    "    \"{company} reports quarterly results in line with expectations\",\n",
    "    \"{company} maintains steady operations in challenging environment\",\n",
    "    \"{company} announces leadership transition as CEO retires\",\n",
    "    \"{company} completes previously announced restructuring plan\",\n",
    "]\n",
    "\n",
    "companies = [\"Acme Corp\", \"Beta Inc\", \"Gamma Tech\", \"Delta Health\", \"Epsilon Energy\",\n",
    "             \"Zeta Finance\", \"Eta Retail\", \"Theta Pharma\", \"Iota Systems\", \"Kappa Media\",\n",
    "             \"Lambda Auto\", \"Mu Logistics\", \"Nu Biotech\", \"Xi Mining\", \"Omicron Telecom\"]\n",
    "\n",
    "for company in companies:\n",
    "    # Positive headlines \u2192 higher returns\n",
    "    for template in np.random.choice(templates_positive, size=3, replace=False):\n",
    "        ret = np.random.normal(0.05, 0.02)  # positive return\n",
    "        data.append({\"company\": company, \"headline\": template.format(company=company), \n",
    "                     \"return\": ret, \"quintile\": None})\n",
    "    # Negative headlines \u2192 lower returns\n",
    "    for template in np.random.choice(templates_negative, size=3, replace=False):\n",
    "        ret = np.random.normal(-0.05, 0.02)\n",
    "        data.append({\"company\": company, \"headline\": template.format(company=company),\n",
    "                     \"return\": ret, \"quintile\": None})\n",
    "    # Neutral headlines \u2192 near-zero returns\n",
    "    for template in np.random.choice(templates_neutral, size=2, replace=False):\n",
    "        ret = np.random.normal(0.0, 0.02)\n",
    "        data.append({\"company\": company, \"headline\": template.format(company=company),\n",
    "                     \"return\": ret, \"quintile\": None})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Assign quintiles based on returns\n",
    "df[\"quintile\"] = pd.qcut(df[\"return\"], q=5, labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df)} headlines, {df['company'].nunique()} companies\")\n",
    "print(f\"\\nReturn distribution by quintile:\")\n",
    "print(df.groupby(\"quintile\")[\"return\"].agg([\"mean\", \"std\", \"count\"]).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Off-the-Shelf Embeddings\n",
    "Before fine-tuning, see how well vanilla embeddings cluster by return quintile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "baseline_embeddings = model.encode(df[\"headline\"].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"Baseline embedding shape: {baseline_embeddings.shape}\")\n",
    "\n",
    "# Visualize with PCA\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(baseline_embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(emb_2d[:, 0], emb_2d[:, 1], c=df[\"quintile\"], cmap=\"RdYlGn\", \n",
    "                     alpha=0.7, s=60, edgecolors='black', linewidth=0.3)\n",
    "plt.colorbar(scatter, label=\"Return Quintile (0=worst, 4=best)\")\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\")\n",
    "ax.set_title(\"Baseline Embeddings (all-MiniLM-L6-v2) Colored by Return Quintile\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contrastive Learning Setup\n",
    "We use a simple contrastive loss: minimize distance between same-quintile pairs, maximize distance between different-quintile pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePairDataset(Dataset):\n",
    "    \"\"\"Generate pairs for contrastive learning.\n",
    "    Positive pairs: same return quintile\n",
    "    Negative pairs: different return quintile\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, quintiles, n_pairs=1000):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.quintiles = np.array(quintiles)\n",
    "        self.n_pairs = n_pairs\n",
    "        \n",
    "        # Pre-compute indices per quintile for efficient sampling\n",
    "        self.quintile_indices = {}\n",
    "        for q in range(5):\n",
    "            self.quintile_indices[q] = np.where(self.quintiles == q)[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 50% positive pairs, 50% negative pairs\n",
    "        is_positive = np.random.random() > 0.5\n",
    "        \n",
    "        anchor_idx = np.random.randint(len(self.embeddings))\n",
    "        anchor_q = self.quintiles[anchor_idx]\n",
    "        \n",
    "        if is_positive:\n",
    "            # Same quintile\n",
    "            pair_idx = np.random.choice(self.quintile_indices[anchor_q])\n",
    "            label = 1.0\n",
    "        else:\n",
    "            # Different quintile (prefer distant quintiles)\n",
    "            other_qs = [q for q in range(5) if q != anchor_q]\n",
    "            pair_q = np.random.choice(other_qs)\n",
    "            pair_idx = np.random.choice(self.quintile_indices[pair_q])\n",
    "            label = 0.0\n",
    "        \n",
    "        return self.embeddings[anchor_idx], self.embeddings[pair_idx], torch.tensor(label)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"Small projection head to fine-tune embedding space.\"\"\"\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, output_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.net(x), dim=-1)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive loss with cosine similarity.\"\"\"\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, emb1, emb2, label):\n",
    "        similarity = F.cosine_similarity(emb1, emb2)\n",
    "        # Positive pairs: maximize similarity\n",
    "        # Negative pairs: minimize similarity (push below margin)\n",
    "        loss_pos = label * (1 - similarity)\n",
    "        loss_neg = (1 - label) * torch.clamp(similarity - self.margin, min=0)\n",
    "        return (loss_pos + loss_neg).mean()\n",
    "\n",
    "\n",
    "print(\"Contrastive learning components defined:\")\n",
    "print(\"  - ContrastivePairDataset: generates positive/negative pairs from quintile labels\")\n",
    "print(\"  - ProjectionHead: 384 \\u2192 128 \\u2192 64 dimensional projection\")\n",
    "print(\"  - ContrastiveLoss: cosine similarity with margin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ContrastivePairDataset(baseline_embeddings, df[\"quintile\"].values, n_pairs=5000)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "projection = ProjectionHead(input_dim=384, hidden_dim=128, output_dim=64)\n",
    "criterion = ContrastiveLoss(margin=0.3)\n",
    "optimizer = torch.optim.Adam(projection.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    for emb1, emb2, labels in loader:\n",
    "        proj1 = projection(emb1)\n",
    "        proj2 = projection(emb2)\n",
    "        loss = criterion(proj1, proj2, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/20, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(losses, 'b-', linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Contrastive Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare: Baseline vs Contrastive Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project embeddings through trained projection head\n",
    "with torch.no_grad():\n",
    "    contrastive_embeddings = projection(torch.tensor(baseline_embeddings, dtype=torch.float32)).numpy()\n",
    "\n",
    "# PCA for visualization\n",
    "pca_baseline = PCA(n_components=2).fit_transform(baseline_embeddings)\n",
    "pca_contrastive = PCA(n_components=2).fit_transform(contrastive_embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Baseline\n",
    "sc1 = axes[0].scatter(pca_baseline[:, 0], pca_baseline[:, 1], c=df[\"quintile\"], \n",
    "                       cmap=\"RdYlGn\", alpha=0.7, s=60, edgecolors='black', linewidth=0.3)\n",
    "axes[0].set_title(\"Baseline (all-MiniLM-L6-v2)\")\n",
    "axes[0].set_xlabel(\"PC1\")\n",
    "axes[0].set_ylabel(\"PC2\")\n",
    "\n",
    "# Contrastive\n",
    "sc2 = axes[1].scatter(pca_contrastive[:, 0], pca_contrastive[:, 1], c=df[\"quintile\"],\n",
    "                       cmap=\"RdYlGn\", alpha=0.7, s=60, edgecolors='black', linewidth=0.3)\n",
    "axes[1].set_title(\"After Contrastive Fine-Tuning\")\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "axes[1].set_ylabel(\"PC2\")\n",
    "\n",
    "plt.colorbar(sc2, ax=axes[1], label=\"Return Quintile\")\n",
    "plt.suptitle(\"Embedding Space: Baseline vs Contrastive\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Silhouette score: how well do quintile labels match embedding clusters?\n",
    "sil_baseline = silhouette_score(baseline_embeddings, df[\"quintile\"], sample_size=min(300, len(df)))\n",
    "sil_contrastive = silhouette_score(contrastive_embeddings, df[\"quintile\"], sample_size=min(300, len(df)))\n",
    "\n",
    "# Same-quintile vs different-quintile similarity\n",
    "def avg_similarity_by_group(embeddings, labels):\n",
    "    \"\"\"Compute average cosine similarity for same-group and different-group pairs.\"\"\"\n",
    "    sim = cosine_similarity(embeddings)\n",
    "    same_sims = []\n",
    "    diff_sims = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i+1, len(labels)):\n",
    "            if labels[i] == labels[j]:\n",
    "                same_sims.append(sim[i, j])\n",
    "            else:\n",
    "                diff_sims.append(sim[i, j])\n",
    "    return np.mean(same_sims), np.mean(diff_sims)\n",
    "\n",
    "same_base, diff_base = avg_similarity_by_group(baseline_embeddings, df[\"quintile\"].values)\n",
    "same_cont, diff_cont = avg_similarity_by_group(contrastive_embeddings, df[\"quintile\"].values)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"{'Metric':<35} {'Baseline':>10} {'Contrastive':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Silhouette Score':<35} {sil_baseline:>10.4f} {sil_contrastive:>12.4f}\")\n",
    "print(f\"{'Same-quintile avg similarity':<35} {same_base:>10.4f} {same_cont:>12.4f}\")\n",
    "print(f\"{'Diff-quintile avg similarity':<35} {diff_base:>10.4f} {diff_cont:>12.4f}\")\n",
    "print(f\"{'Similarity gap (same - diff)':<35} {same_base-diff_base:>10.4f} {same_cont-diff_cont:>12.4f}\")\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "metrics = [\"Silhouette\", \"Same-Q Sim\", \"Diff-Q Sim\", \"Gap\"]\n",
    "baseline_vals = [sil_baseline, same_base, diff_base, same_base - diff_base]\n",
    "contrastive_vals = [sil_contrastive, same_cont, diff_cont, same_cont - diff_cont]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, baseline_vals, width, label=\"Baseline\", color=\"steelblue\", alpha=0.7)\n",
    "ax.bar(x + width/2, contrastive_vals, width, label=\"Contrastive\", color=\"coral\", alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_title(\"Embedding Quality: Baseline vs Contrastive Fine-Tuned\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Interview Talking Points\n",
    "\n",
    "### Connection to My Experience\n",
    "- **Creyon Bio**: Used contrastive learning on 3D electrostatic maps to predict oligo toxicity. Pushed AUC from 0.73 to 0.88.\n",
    "- **Same framework**: Instead of electrostatic maps \u2192 toxicity, we have financial text \u2192 returns. The contrastive objective is identical.\n",
    "- **DPO experience**: My recent DPO fine-tuning work (Qwen 32B) uses a similar preference-based optimization \u2014 DPO is conceptually a contrastive method.\n",
    "\n",
    "### Strengths\n",
    "- **Domain-adapted**: Embeddings are optimized for the actual downstream task (return prediction)\n",
    "- **Beyond sentiment**: Captures whatever textual patterns correlate with returns, not just positive/negative\n",
    "- **Composable**: Can use contrastive embeddings as input to any downstream model\n",
    "\n",
    "### Weaknesses & Considerations\n",
    "- **Requires labeled data**: Need (text, return) pairs, which means historical market data\n",
    "- **Temporal leakage risk**: Must use strict temporal train/test splits\n",
    "- **Overfitting**: With small datasets, the projection head can memorize rather than generalize\n",
    "\n",
    "### For Numerai\n",
    "- Contrastive embeddings trained on returns that have been **factor-neutralized** would directly optimize for Numerai's scoring metric\n",
    "- Could combine with graph features (NB04): contrastive loss where graph-neighbors should have similar embeddings\n",
    "\n",
    "### Extensions (TODO)\n",
    "- [ ] Use triplet loss instead of pairwise contrastive\n",
    "- [ ] Fine-tune the encoder itself (not just a projection head) with LoRA\n",
    "- [ ] Train on factor-neutralized returns for Numerai-specific optimization\n",
    "- [ ] Add hard negative mining (most confusing cross-quintile pairs)\n",
    "- [ ] Compare InfoNCE, NT-Xent, and supervised contrastive losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}