{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning an LLM for Financial Feature Extraction\n",
    "\n",
    "## Context\n",
    "This is the most directly job-relevant notebook. The Numerai \"Quantitative AI Researcher\" role involves fine-tuning LLMs on financial corpora to create features. This notebook demonstrates the approach using LoRA for parameter-efficient fine-tuning.\n",
    "\n",
    "## My Experience\n",
    "- **Creyon Bio**: Fine-tuned large protein models for splice-site prediction. Pre-training, fine-tuning, and benchmarking.\n",
    "- **DPO Fine-tuning** (this week): Custom Trainer, Dataset, and DataCollator classes for DPO on Qwen 32B with LoRA.\n",
    "- **TorchLeet**: Implemented LoRA from scratch, understand low-rank adaptation at the mathematical level.\n",
    "\n",
    "## Pipeline\n",
    "Financial text corpus \u2192 LoRA fine-tune causal LM \u2192 Extract embeddings \u2192 Compare vanilla vs fine-tuned features\n",
    "\n",
    "## Hypothesis\n",
    "A general LLM (Gemma-3, Llama, Mistral) has broad language understanding but no financial specialization. By fine-tuning on financial text, we adapt the model's representations to encode financial-relevant patterns \u2014 producing better features for stock prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Base Model\n",
    "We use Gemma-3-270M-IT (270M params) for fast iteration. The same approach scales to Llama-3, Mistral, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_name = \"google/gemma-3-270m-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nbase_model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\nprint(f\"Base model: {model_name}\")\nprint(f\"Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Financial Text Corpus\n",
    "In production: SEC filings, financial news, earnings call transcripts, analyst reports.\n",
    "Here we use synthetic financial text that mimics 10-K language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic financial text corpus for fine-tuning\n",
    "# Mimics SEC filing language, earnings reports, and financial news\n",
    "financial_texts = [\n",
    "    \"The company reported total revenue of $89.5 billion for the fiscal year, representing a 12% increase year-over-year driven by strong performance in the cloud computing segment and continued growth in digital advertising revenue.\",\n",
    "    \"Operating expenses increased to $45.2 billion due to higher research and development spending, increased headcount in artificial intelligence divisions, and expanded data center infrastructure investments across three new regions.\",\n",
    "    \"Net income attributable to common shareholders was $21.3 billion, or $5.67 per diluted share, compared to $18.9 billion, or $4.89 per diluted share, in the prior year period.\",\n",
    "    \"The Board of Directors declared a quarterly dividend of $0.82 per share and authorized an additional $50 billion share repurchase program, reflecting confidence in the company's long-term growth prospects.\",\n",
    "    \"Gross margin contracted by 150 basis points to 42.3% primarily due to unfavorable product mix shift toward lower-margin hardware products and increased component costs from supply chain constraints.\",\n",
    "    \"The company completed the acquisition of a leading cybersecurity firm for $28 billion in an all-stock transaction, expanding its enterprise security offerings and adding approximately 4,000 employees.\",\n",
    "    \"Free cash flow for the quarter was $15.8 billion, up 18% from the prior year, driven by improved working capital management and higher operating income partially offset by increased capital expenditures.\",\n",
    "    \"Management expects revenue growth in the range of 8-10% for the coming fiscal year, with operating margins expected to expand by 50-75 basis points as restructuring benefits are fully realized.\",\n",
    "    \"The company's debt-to-equity ratio improved to 0.45 from 0.62 in the prior year as the company used excess cash flow to retire $12 billion in long-term debt obligations.\",\n",
    "    \"Research and development expenses totaled $22.1 billion, representing 24.7% of total revenue, as the company increased investment in generative AI, autonomous systems, and quantum computing initiatives.\",\n",
    "    \"The company recorded a goodwill impairment charge of $4.2 billion related to its consumer electronics segment, reflecting lower-than-expected market demand and increased competitive pressures.\",\n",
    "    \"International revenue grew 15% on a constant currency basis, with particularly strong performance in Asia-Pacific markets where the company expanded its distribution network and launched localized products.\",\n",
    "    \"The effective tax rate was 18.5%, benefiting from the recognition of research and development tax credits and income earned in lower-tax jurisdictions through the company's global operating structure.\",\n",
    "    \"Customer retention rates improved to 94.2% from 91.8% in the prior year, driven by enhanced product features, improved customer support, and the introduction of a premium loyalty program.\",\n",
    "    \"The company issued $8 billion in investment-grade bonds with maturities ranging from 5 to 30 years, taking advantage of favorable interest rate conditions to refinance existing debt at lower rates.\",\n",
    "    \"Same-store sales increased 3.2% for the quarter, with digital channel revenue growing 28% year-over-year as the company's omnichannel strategy continued to gain traction with consumers.\",\n",
    "    \"Inventory levels increased 22% from the prior year as the company built safety stock to mitigate ongoing supply chain disruptions and prepare for anticipated seasonal demand.\",\n",
    "    \"The company announced a strategic restructuring plan expected to result in annual cost savings of $2.5 billion by fiscal year 2025, including workforce reductions of approximately 7% of total employees.\",\n",
    "    \"Cloud infrastructure revenue reached $18.3 billion for the quarter, up 29% year-over-year, as enterprise customers accelerated their digital transformation initiatives and adopted AI-powered cloud services.\",\n",
    "    \"The company's patent portfolio expanded to over 45,000 active patents globally, providing significant intellectual property protection across its core technology platforms and emerging innovation areas.\",\n",
    "    \"Working capital requirements increased by $3.1 billion due to extended payment terms offered to key enterprise customers and higher accounts receivable balances from the rapid growth in subscription services.\",\n",
    "    \"The audit committee identified material weaknesses in internal controls over financial reporting related to revenue recognition processes, requiring management to implement remediation measures.\",\n",
    "    \"Backlog increased to $142 billion, up 8% from the prior quarter, providing strong revenue visibility for the next 18-24 months across defense, space, and commercial aviation segments.\",\n",
    "    \"The company's credit rating was upgraded to AA- by Standard and Poor's, reflecting improved profitability, reduced leverage, and strong competitive positioning in its core markets.\",\n",
    "    \"Capital expenditures totaled $12.4 billion for the fiscal year, primarily directed toward new manufacturing facilities, data center expansion, and automation of existing production lines.\",\n",
    "]\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "dataset = Dataset.from_dict({\"text\": financial_texts})\n",
    "print(f\"Financial corpus: {len(financial_texts)} passages\")\n",
    "print(f\"Total tokens (approx): {sum(len(t.split()) for t in financial_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply LoRA Adapters\n",
    "LoRA (Low-Rank Adaptation) freezes the base model and adds small trainable matrices. This is parameter-efficient and avoids catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    from peft import LoraConfig, get_peft_model, TaskType\n\n    # LoRA configuration\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=8,                    # Rank of the low-rank matrices\n        lora_alpha=16,          # Scaling factor\n        lora_dropout=0.1,       # Dropout for regularization\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Gemma-3's attention projection layers\n    )\n\n    # Apply LoRA to the model\n    ft_model = get_peft_model(base_model, lora_config)\n    \n    trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in ft_model.parameters())\n    print(f\"Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.2%})\")\n    print(f\"LoRA rank: {lora_config.r}\")\n    print(f\"Target modules: {lora_config.target_modules}\")\n    \n    HAS_PEFT = True\nexcept ImportError:\n    print(\"peft not installed. Run: uv add peft\")\n    print(\"Skipping LoRA fine-tuning \\u2014 showing concept only.\")\n    HAS_PEFT = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tune on Financial Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PEFT:\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    def add_labels(examples):\n",
    "        examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "        return examples\n",
    "    \n",
    "    tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/tmp/numerai-financial-lm\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model=ft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning with LoRA on financial corpus...\")\n",
    "    train_result = trainer.train()\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "    print(\"Fine-tuning complete!\")\n",
    "else:\n",
    "    print(\"Skipping fine-tuning (peft not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Embeddings: Vanilla vs Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_embeddings(model, tokenizer, texts, layer=-1):\n    \"\"\"Extract last-token hidden states from a specific layer.\"\"\"\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n            outputs = model(**inputs, output_hidden_states=True)\n            # Use specified layer, last token\n            hidden = outputs.hidden_states[layer][0, -1, :].numpy()\n            embeddings.append(hidden)\n    return np.stack(embeddings)\n\n# Test texts for comparison\ntest_texts = [\n    \"Revenue increased 15% driven by strong product demand\",\n    \"Company reported significant quarterly loss\",\n    \"New CEO appointment signals strategic direction change\",\n    \"Dividend increased reflecting strong cash flow generation\",\n    \"Regulatory investigation into accounting practices\",\n    \"Market share gains in cloud computing segment\",\n    \"Supply chain disruptions impacting production targets\",\n    \"Record free cash flow enables debt reduction\",\n    \"Workforce reduction announced as part of restructuring\",\n    \"Patent portfolio strengthens competitive moat\",\n]\ntest_labels = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1]  # 1=positive, 0=negative\n\n# Extract embeddings from both models\nvanilla_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m-it\", output_hidden_states=True)\nvanilla_emb = extract_embeddings(vanilla_model, tokenizer, test_texts)\n\nif HAS_PEFT:\n    ft_emb = extract_embeddings(ft_model, tokenizer, test_texts)\nelse:\n    # Simulate fine-tuned embeddings with small perturbation for illustration\n    ft_emb = vanilla_emb + np.random.normal(0, 0.1, vanilla_emb.shape)\n\nprint(f\"Vanilla embeddings shape: {vanilla_emb.shape}\")\nprint(f\"Fine-tuned embeddings shape: {ft_emb.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "pca_v = PCA(n_components=2).fit_transform(vanilla_emb)\n",
    "pca_f = PCA(n_components=2).fit_transform(ft_emb)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "labels_arr = np.array(test_labels)\n",
    "\n",
    "for ax, emb_2d, title in [(axes[0], pca_v, \"Vanilla Gemma-3\"), (axes[1], pca_f, \"LoRA Fine-Tuned\")]:\n",
    "    for label, color, name in [(1, 'green', 'Positive'), (0, 'red', 'Negative')]:\n",
    "        mask = labels_arr == label\n",
    "        ax.scatter(emb_2d[mask, 0], emb_2d[mask, 1], c=color, label=name, \n",
    "                   s=100, edgecolors='black', linewidth=0.5, alpha=0.7)\n",
    "        for i in np.where(mask)[0]:\n",
    "            ax.annotate(test_texts[i][:25] + \"...\", (emb_2d[i, 0], emb_2d[i, 1]),\n",
    "                       fontsize=6, alpha=0.7, ha='center', va='bottom')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Embedding Space: Vanilla vs Financial Fine-Tuned\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity structures\n",
    "sim_vanilla = cosine_similarity(vanilla_emb)\n",
    "sim_ft = cosine_similarity(ft_emb)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, sim, title in [(axes[0], sim_vanilla, \"Vanilla\"), (axes[1], sim_ft, \"Fine-Tuned\")]:\n",
    "    im = ax.imshow(sim, cmap='RdYlBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_title(title)\n",
    "    # Label with +/-\n",
    "    labels_str = [\"+\" if l == 1 else \"-\" for l in test_labels]\n",
    "    ax.set_xticks(range(len(labels_str)))\n",
    "    ax.set_yticks(range(len(labels_str)))\n",
    "    ax.set_xticklabels(labels_str)\n",
    "    ax.set_yticklabels(labels_str)\n",
    "\n",
    "plt.colorbar(im, ax=axes, label=\"Cosine Similarity\")\n",
    "plt.suptitle(\"Similarity Structure: Do Positive/Negative Texts Cluster?\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Interview Talking Points\n",
    "\n",
    "### My Direct Experience\n",
    "- **Creyon Bio**: Fine-tuned large protein language models for splice-site prediction. Same pipeline: domain corpus \u2192 LoRA \u2192 task-relevant features.\n",
    "- **DPO this week**: Built custom Trainer, Dataset, DataCollator for Qwen 32B DPO fine-tuning. Handled left-padding, response-length weighting, and label masking.\n",
    "- **TorchLeet**: Implemented LoRA from scratch (rank decomposition of weight updates).\n",
    "\n",
    "### Key Architectural Decisions for Numerai\n",
    "1. **Base model choice**: Llama-3-8B vs Mistral-7B vs FinBERT. Probing (NB06) can inform this.\n",
    "2. **Corpus construction**: Mix of SEC filings, news, earnings calls. Point-in-time correctness is critical.\n",
    "3. **LoRA vs full fine-tuning**: LoRA for efficiency; full fine-tuning only if compute budget allows.\n",
    "4. **Layer freezing strategy**: Freeze early layers (syntax), fine-tune middle/late layers (semantics).\n",
    "5. **Evaluation**: Compare fine-tuned features against vanilla on Numerai Signals scoring metric.\n",
    "\n",
    "### Scaling Considerations\n",
    "- **Common Crawl**: Petabytes of text. Need to filter \u2192 process \u2192 fine-tune in streaming fashion.\n",
    "- **Infrastructure**: My NDIF paper used Ray GCS + AWS object storage + VLLM. Same stack applies.\n",
    "- **Distributed training**: Experience with DDP and multi-GPU from Creyon Bio and NDIF.\n",
    "\n",
    "### Extensions (TODO)\n",
    "- [ ] Fine-tune on real SEC filings from EDGAR\n",
    "- [ ] Compare LoRA ranks (4, 8, 16, 32) \u2014 which is optimal for financial domain adaptation?\n",
    "- [ ] Multi-task fine-tuning: CausalLM + sentiment classification head\n",
    "- [ ] Curriculum learning: general financial text \u2192 sector-specific text \u2192 stock-specific text\n",
    "- [ ] Compare embeddings from fine-tuned model across probing layers (combine with NB06)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}