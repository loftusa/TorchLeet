{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Crawl \u2192 Financial Features Pipeline\n",
    "\n",
    "## Context\n",
    "This notebook directly addresses the Numerai job description, which mentions \"processing Common Crawl data\" as a core responsibility. Common Crawl is the largest publicly available web crawl dataset \u2014 petabytes of HTML from billions of web pages.\n",
    "\n",
    "The challenge: **extract financially relevant text from this haystack, map it to stock tickers, and produce clean features \u2014 all while maintaining strict point-in-time correctness.**\n",
    "\n",
    "## My Infrastructure Experience\n",
    "- **NDIF/NNsight** (ICLR 2025): Built Ray GCS Service backend with AWS object storage and VLLM for inference. Scaled to handle thousands of concurrent model probing requests.\n",
    "- **NeuroData Lab**: Optimized a diffusion MRI pipeline with Kubernetes, Docker, and AWS Batch. Halved runtime, cut cloud costs 40%.\n",
    "- **Creyon Bio**: Scaled data processing pipelines for protein sequence analysis.\n",
    "\n",
    "## Pipeline Architecture\n",
    "```\n",
    "Common Crawl (WARC files, ~3TB/month)\n",
    "    \u2192 Filter: financial content classifier\n",
    "    \u2192 Parse: extract clean text from HTML\n",
    "    \u2192 Entity Link: map text \u2192 stock tickers\n",
    "    \u2192 Feature Extract: sentiment, embeddings, linguistic features (NB01-07)\n",
    "    \u2192 Temporal Aggregate: daily text \u2192 weekly features (match Numerai eras)\n",
    "    \u2192 Output: per-ticker feature vectors for Numerai Signals\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Common Crawl Data\n",
    "\n",
    "Common Crawl stores web pages in WARC (Web ARChive) format. Each monthly crawl contains:\n",
    "- ~3 billion web pages\n",
    "- ~200-400 TB compressed\n",
    "- Available for free on S3: `s3://commoncrawl/`\n",
    "\n",
    "Each WARC record contains:\n",
    "- URL of the crawled page\n",
    "- HTTP response headers\n",
    "- HTML content\n",
    "- Crawl timestamp (critical for point-in-time correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Common Crawl WARC records\n",
    "# In production: use warcio library to read actual WARC files from S3\n",
    "# Example: aws s3 ls s3://commoncrawl/crawl-data/CC-MAIN-2024-10/segments/\n",
    "simulated_warc_records = [\n",
    "    {\n",
    "        \"url\": \"https://www.reuters.com/technology/apple-reports-record-q1-revenue-2024-01-25/\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T03:14:22Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><head><title>Apple Reports Record Q1 Revenue</title></head><body><article><h1>Apple Reports Record Q1 Revenue Driven by iPhone 15</h1><p>Apple Inc reported record first-quarter revenue of $119.6 billion on Thursday, driven by strong iPhone 15 sales and growing services revenue. The tech giant beat analyst expectations of $117.9 billion.</p><p>CEO Tim Cook said the company saw particular strength in emerging markets, with India revenue hitting an all-time record.</p></article></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.bloomberg.com/news/nvidia-datacenter-revenue-surge\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T05:22:11Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><article><h1>NVIDIA Revenue Surges 400% as AI Demand Explodes</h1><p>NVIDIA Corp reported that data center revenue increased 409% year-over-year to $18.4 billion, far exceeding analyst estimates. The chipmaker's GPUs have become essential infrastructure for training large language models.</p><p>CEO Jensen Huang announced the next-generation Blackwell GPU architecture.</p></article></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://finance.yahoo.com/news/tesla-recalls-vehicles\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T08:45:33Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><div class='article'><h1>Tesla Recalls 2 Million Vehicles Over Autopilot Safety</h1><p>Tesla Inc is recalling approximately 2 million vehicles in the United States to install new safeguards in its Autopilot driver-assistance system, according to NHTSA.</p></div></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.recipe-blog.com/best-chocolate-cake/\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T02:11:05Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><h1>Best Chocolate Cake Recipe</h1><p>This rich chocolate cake uses premium cocoa powder and buttermilk for an incredibly moist texture.</p></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=AAPL\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T12:00:00Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><h1>EDGAR Company Search</h1><p>Apple Inc (AAPL) - Latest filings available.</p></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.cnbc.com/jpmorgan-earnings-q4\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T06:33:22Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><article><h1>JPMorgan Reports Strong Q4 Earnings</h1><p>JPMorgan Chase & Co reported fourth-quarter profit that beat expectations as the largest U.S. bank benefited from higher interest rates and strong trading revenue. Net income rose to $9.3 billion from $3.57 billion.</p></article></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.espn.com/nfl/story/playoff-results\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T01:00:00Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><h1>NFL Playoff Results</h1><p>The Kansas City Chiefs advanced to the conference championship with a thrilling overtime victory.</p></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.ft.com/content/microsoft-openai-investment\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T09:15:44Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><article><h1>Microsoft Deepens OpenAI Partnership with $10B Investment</h1><p>Microsoft Corp has agreed to invest an additional $10 billion in OpenAI, strengthening its position in the artificial intelligence race against Google and Amazon.</p></article></body></html>\",\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.investopedia.com/terms/p/price-earnings-ratio.asp\",\n",
    "        \"crawl_timestamp\": \"2024-01-26T04:20:00Z\",\n",
    "        \"content_type\": \"text/html\",\n",
    "        \"html\": \"<html><body><h1>Price-to-Earnings Ratio (P/E)</h1><p>The price-to-earnings ratio is a financial metric used to value a company. It is calculated by dividing the stock price by earnings per share.</p></body></html>\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Simulated WARC records: {len(simulated_warc_records)}\")\n",
    "for r in simulated_warc_records:\n",
    "    domain = urlparse(r[\"url\"]).netloc\n",
    "    print(f\"  [{domain}] {r['url'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step 1: Filter for Financial Content\n",
    "Most of Common Crawl is irrelevant (recipes, sports, etc). We need a fast filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial content filtering strategy:\n",
    "# Level 1: URL-based filtering (fast, no parsing needed)\n",
    "# Level 2: Keyword-based filtering (requires text extraction)\n",
    "# Level 3: ML classifier (most accurate, most expensive)\n",
    "\n",
    "FINANCIAL_DOMAINS = {\n",
    "    \"reuters.com\", \"bloomberg.com\", \"cnbc.com\", \"ft.com\", \"wsj.com\",\n",
    "    \"finance.yahoo.com\", \"marketwatch.com\", \"seekingalpha.com\", \"sec.gov\",\n",
    "    \"businessinsider.com\", \"barrons.com\", \"economist.com\", \"investopedia.com\",\n",
    "    \"thestreet.com\", \"morningstar.com\", \"fool.com\", \"benzinga.com\",\n",
    "}\n",
    "\n",
    "FINANCIAL_URL_PATTERNS = [\n",
    "    r\"finance\", r\"business\", r\"market\", r\"stock\", r\"invest\",\n",
    "    r\"earnings\", r\"revenue\", r\"economy\", r\"trading\",\n",
    "]\n",
    "\n",
    "FINANCIAL_KEYWORDS = {\n",
    "    \"revenue\", \"earnings\", \"profit\", \"loss\", \"stock\", \"shares\", \"dividend\",\n",
    "    \"quarterly\", \"fiscal\", \"billion\", \"million\", \"analyst\", \"forecast\",\n",
    "    \"market\", \"trading\", \"investor\", \"shareholder\", \"CEO\", \"IPO\",\n",
    "    \"acquisition\", \"merger\", \"SEC\", \"filing\", \"guidance\", \"margin\",\n",
    "    \"growth\", \"decline\", \"beat\", \"miss\", \"estimate\", \"consensus\",\n",
    "}\n",
    "\n",
    "def is_financial_url(url):\n",
    "    \"\"\"Level 1: Fast URL-based filter.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.replace(\"www.\", \"\")\n",
    "    \n",
    "    # Check known financial domains\n",
    "    if domain in FINANCIAL_DOMAINS:\n",
    "        return True, \"domain_match\"\n",
    "    \n",
    "    # Check URL patterns\n",
    "    url_lower = url.lower()\n",
    "    for pattern in FINANCIAL_URL_PATTERNS:\n",
    "        if re.search(pattern, url_lower):\n",
    "            return True, \"url_pattern\"\n",
    "    \n",
    "    return False, \"no_match\"\n",
    "\n",
    "def is_financial_content(text, threshold=3):\n",
    "    \"\"\"Level 2: Keyword-based content filter.\"\"\"\n",
    "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "    matches = words & FINANCIAL_KEYWORDS\n",
    "    return len(matches) >= threshold, matches\n",
    "\n",
    "def filter_pipeline(record):\n",
    "    \"\"\"Multi-level financial content filter.\"\"\"\n",
    "    # Level 1: URL check (cheapest)\n",
    "    url_match, url_reason = is_financial_url(record[\"url\"])\n",
    "    if not url_match:\n",
    "        # Level 2: Content check (need to parse HTML first)\n",
    "        text = re.sub(r'<[^>]+>', ' ', record[\"html\"])  # strip HTML\n",
    "        content_match, keywords = is_financial_content(text)\n",
    "        if not content_match:\n",
    "            return False, \"filtered_non_financial\"\n",
    "        return True, f\"content_keywords: {keywords}\"\n",
    "    return True, f\"url: {url_reason}\"\n",
    "\n",
    "# Apply filter\n",
    "print(\"Filtering WARC records:\")\n",
    "print(\"-\" * 80)\n",
    "for record in simulated_warc_records:\n",
    "    passed, reason = filter_pipeline(record)\n",
    "    domain = urlparse(record[\"url\"]).netloc\n",
    "    status = \"PASS\" if passed else \"SKIP\"\n",
    "    print(f\"  [{status}] {domain:<35} | {reason}\")\n",
    "\n",
    "passed_records = [r for r in simulated_warc_records if filter_pipeline(r)[0]]\n",
    "print(f\"\\nPassed: {len(passed_records)}/{len(simulated_warc_records)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 2: Parse HTML \u2192 Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    \"\"\"Extract clean text from HTML. In production, use BeautifulSoup or trafilatura.\"\"\"\n",
    "    # Remove script and style tags\n",
    "    text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Clean whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_title(html):\n",
    "    \"\"\"Extract title from HTML.\"\"\"\n",
    "    match = re.search(r'<title[^>]*>(.*?)</title>', html, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    match = re.search(r'<h1[^>]*>(.*?)</h1>', html, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return re.sub(r'<[^>]+>', '', match.group(1)).strip()\n",
    "    return \"\"\n",
    "\n",
    "# Parse passed records\n",
    "parsed_records = []\n",
    "for record in passed_records:\n",
    "    parsed = {\n",
    "        \"url\": record[\"url\"],\n",
    "        \"crawl_timestamp\": record[\"crawl_timestamp\"],\n",
    "        \"title\": extract_title(record[\"html\"]),\n",
    "        \"text\": extract_text(record[\"html\"]),\n",
    "        \"domain\": urlparse(record[\"url\"]).netloc.replace(\"www.\", \"\"),\n",
    "    }\n",
    "    parsed_records.append(parsed)\n",
    "\n",
    "parsed_df = pd.DataFrame(parsed_records)\n",
    "print(\"Parsed financial content:\")\n",
    "for _, row in parsed_df.iterrows():\n",
    "    print(f\"\\n  [{row['domain']}] {row['title']}\")\n",
    "    print(f\"  Text: {row['text'][:120]}...\")\n",
    "    print(f\"  Crawled: {row['crawl_timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 3: Entity Linking (Text \u2192 Stock Tickers)\n",
    "Map company mentions to stock tickers. This is a critical and non-trivial step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity linking: company name \u2192 stock ticker\n",
    "# In production: use a comprehensive database + fuzzy matching + NER\n",
    "ENTITY_DB = {\n",
    "    # Company name variants \u2192 canonical ticker\n",
    "    \"apple\": \"AAPL\", \"apple inc\": \"AAPL\",\n",
    "    \"nvidia\": \"NVDA\", \"nvidia corp\": \"NVDA\",\n",
    "    \"tesla\": \"TSLA\", \"tesla inc\": \"TSLA\",\n",
    "    \"microsoft\": \"MSFT\", \"microsoft corp\": \"MSFT\",\n",
    "    \"jpmorgan\": \"JPM\", \"jpmorgan chase\": \"JPM\", \"jp morgan\": \"JPM\",\n",
    "    \"google\": \"GOOGL\", \"alphabet\": \"GOOGL\",\n",
    "    \"amazon\": \"AMZN\", \"amazon.com\": \"AMZN\",\n",
    "    \"meta\": \"META\", \"facebook\": \"META\",\n",
    "    \"openai\": None,  # Private company \u2014 not in Numerai's universe\n",
    "}\n",
    "\n",
    "def link_entities(text):\n",
    "    \"\"\"Map company mentions in text to stock tickers.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    found_tickers = set()\n",
    "    found_companies = []\n",
    "    \n",
    "    # Sort by length (longest first) to avoid partial matches\n",
    "    for company, ticker in sorted(ENTITY_DB.items(), key=lambda x: -len(x[0])):\n",
    "        if company in text_lower and ticker is not None:\n",
    "            found_tickers.add(ticker)\n",
    "            found_companies.append((company, ticker))\n",
    "    \n",
    "    return list(found_tickers), found_companies\n",
    "\n",
    "# Link entities in parsed records\n",
    "for i, row in parsed_df.iterrows():\n",
    "    tickers, companies = link_entities(row[\"text\"])\n",
    "    parsed_df.at[i, \"tickers\"] = tickers\n",
    "    parsed_df.at[i, \"n_tickers\"] = len(tickers)\n",
    "\n",
    "print(\"Entity Linking Results:\")\n",
    "for _, row in parsed_df.iterrows():\n",
    "    tickers = row[\"tickers\"] if isinstance(row[\"tickers\"], list) else []\n",
    "    print(f\"  [{', '.join(tickers) if tickers else 'NONE'}] {row['title'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 4: Point-in-Time Correctness (CRITICAL)\n",
    "This is the most important engineering constraint. Features must only use information available at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point-in-time correctness demonstration\n",
    "# \n",
    "# KEY RULE: Use the CRAWL TIMESTAMP, not the publication date\n",
    "# Why? The crawl timestamp is when the information was definitely available\n",
    "# Publication dates can be:\n",
    "#   - Missing or unreliable\n",
    "#   - Backdated\n",
    "#   - Different from when the content was actually accessible\n",
    "#\n",
    "# Numerai eras are weekly (Friday close). Features for era T must only use\n",
    "# data crawled BEFORE era T's prediction deadline.\n",
    "\n",
    "def assign_to_era(crawl_timestamp, era_schedule):\n",
    "    \"\"\"Assign a document to the latest era it's available for.\"\"\"\n",
    "    crawl_dt = datetime.fromisoformat(crawl_timestamp.replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n",
    "    for era_date in sorted(era_schedule, reverse=True):\n",
    "        if crawl_dt <= era_date:\n",
    "            continue\n",
    "        return era_date\n",
    "    return None\n",
    "\n",
    "# Simulate weekly eras (Fridays)\n",
    "era_dates = [datetime(2024, 1, 19), datetime(2024, 1, 26), datetime(2024, 2, 2)]\n",
    "\n",
    "print(\"Point-in-Time Assignment:\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in parsed_df.iterrows():\n",
    "    era = assign_to_era(row[\"crawl_timestamp\"], era_dates)\n",
    "    era_str = era.strftime(\"%Y-%m-%d\") if era else \"BEFORE_FIRST_ERA\"\n",
    "    print(f\"  Crawled: {row['crawl_timestamp'][:19]} \\u2192 Era: {era_str}\")\n",
    "    print(f\"    {row['title'][:60]}\")\n",
    "print()\n",
    "print(\"RULE: Document crawled on Jan 26 is available for era Jan 26, NOT Jan 19\")\n",
    "print(\"This prevents look-ahead bias \\u2014 the most common mistake in financial ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 5: Temporal Aggregation\n",
    "Aggregate text features per ticker per era, with exponential decay weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal aggregation with exponential decay\n",
    "# Recent documents are weighted more heavily than older ones\n",
    "\n",
    "def exponential_decay_weight(days_old, half_life=7):\n",
    "    \"\"\"Weight that halves every `half_life` days.\"\"\"\n",
    "    return 2 ** (-days_old / half_life)\n",
    "\n",
    "# Demonstrate decay function\n",
    "days = np.arange(0, 60)\n",
    "weights_7d = [exponential_decay_weight(d, half_life=7) for d in days]\n",
    "weights_14d = [exponential_decay_weight(d, half_life=14) for d in days]\n",
    "weights_30d = [exponential_decay_weight(d, half_life=30) for d in days]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(days, weights_7d, label=\"Half-life: 7 days\", linewidth=2)\n",
    "ax.plot(days, weights_14d, label=\"Half-life: 14 days\", linewidth=2)\n",
    "ax.plot(days, weights_30d, label=\"Half-life: 30 days\", linewidth=2)\n",
    "ax.set_xlabel(\"Days Since Document Crawled\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_title(\"Exponential Decay Weighting for Temporal Aggregation\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Design choice: half-life should match the signal horizon\")\n",
    "print(\"  - News sentiment: 7-day half-life (decays fast)\")\n",
    "print(\"  - SEC filing features: 30-90 day half-life (changes slowly)\")\n",
    "print(\"  - Numerai targets are 20-60 day returns \\u2192 14-30 day half-life is reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling calculations for Common Crawl processing\n",
    "\n",
    "# Monthly Common Crawl stats (approximate)\n",
    "monthly_warc_records = 3_000_000_000  # 3 billion pages\n",
    "avg_html_size_kb = 50\n",
    "monthly_raw_size_tb = monthly_warc_records * avg_html_size_kb / 1e9  # ~150 TB\n",
    "\n",
    "# After financial filtering (estimated 0.5-2% of web is financial)\n",
    "financial_pct = 0.01  # 1% conservative estimate\n",
    "monthly_financial_records = monthly_warc_records * financial_pct\n",
    "monthly_financial_size_tb = monthly_raw_size_tb * financial_pct\n",
    "\n",
    "# Numerai stock universe\n",
    "numerai_stocks = 5000\n",
    "us_stocks = 3000  # rough US portion\n",
    "\n",
    "# Processing costs (rough estimates)\n",
    "filter_cost_per_1m = 0.50  # URL filtering is cheap\n",
    "parse_cost_per_1m = 2.00   # HTML parsing\n",
    "ner_cost_per_1m = 10.00    # Named entity recognition\n",
    "embed_cost_per_1m = 50.00  # Sentence embedding inference (GPU)\n",
    "\n",
    "total_records_millions = monthly_financial_records / 1e6\n",
    "total_cost = (filter_cost_per_1m + parse_cost_per_1m + ner_cost_per_1m + embed_cost_per_1m) * total_records_millions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMMON CRAWL SCALING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMonthly Common Crawl:\")\n",
    "print(f\"  Total pages:      {monthly_warc_records/1e9:.1f} billion\")\n",
    "print(f\"  Raw size:          ~{monthly_raw_size_tb:.0f} TB\")\n",
    "print(f\"\\nAfter Financial Filtering ({financial_pct:.1%}):\")\n",
    "print(f\"  Financial pages:   {monthly_financial_records/1e6:.0f} million\")\n",
    "print(f\"  Financial size:    ~{monthly_financial_size_tb:.1f} TB\")\n",
    "print(f\"\\nEstimated Monthly Processing Cost:\")\n",
    "print(f\"  URL filtering:     ${filter_cost_per_1m * total_records_millions:,.0f}\")\n",
    "print(f\"  HTML parsing:      ${parse_cost_per_1m * total_records_millions:,.0f}\")\n",
    "print(f\"  NER/Entity link:   ${ner_cost_per_1m * total_records_millions:,.0f}\")\n",
    "print(f\"  Embedding (GPU):   ${embed_cost_per_1m * total_records_millions:,.0f}\")\n",
    "print(f\"  \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\")\n",
    "print(f\"  Total:             ${total_cost:,.0f}/month\")\n",
    "print(f\"\\nTarget: {numerai_stocks:,} stocks with weekly features\")\n",
    "print(f\"  \\u2192 {monthly_financial_records/numerai_stocks:.0f} articles per stock per month\")\n",
    "\n",
    "# Architecture diagram (text-based)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502                   Common Crawl (S3)                      \\u2502\n",
    "\\u2502              s3://commoncrawl/crawl-data/                \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 WARC files (~3B pages/month)\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Stage 1: URL Filter (Ray / Spark)                \\u2502\n",
    "\\u2502  \\u2022 Domain whitelist (reuters, bloomberg, etc.)            \\u2502\n",
    "\\u2502  \\u2022 URL pattern matching                                   \\u2502\n",
    "\\u2502  \\u2022 Drops ~98% of records                                  \\u2502\n",
    "\\u2502  \\u2022 Cost: ~$0.50/M records                                \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 ~30M financial pages\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Stage 2: HTML Parse + Content Filter             \\u2502\n",
    "\\u2502  \\u2022 trafilatura / BeautifulSoup                           \\u2502\n",
    "\\u2502  \\u2022 Keyword-based content classifier                       \\u2502\n",
    "\\u2502  \\u2022 Extract: title, text, metadata, crawl timestamp       \\u2502\n",
    "\\u2502  \\u2022 Cost: ~$2/M records                                   \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 ~15M clean text documents\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Stage 3: Entity Linking (GPU)                    \\u2502\n",
    "\\u2502  \\u2022 spaCy NER / fine-tuned NER model                     \\u2502\n",
    "\\u2502  \\u2022 Company name \\u2192 Ticker mapping                         \\u2502\n",
    "\\u2502  \\u2022 Fuzzy matching for name variants                      \\u2502\n",
    "\\u2502  \\u2022 Cost: ~$10/M records                                  \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 ~10M ticker-linked documents\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Stage 4: Feature Extraction (GPU)                \\u2502\n",
    "\\u2502  \\u2022 FinBERT sentiment (NB01)                              \\u2502\n",
    "\\u2502  \\u2022 Sentence embeddings (NB02)                            \\u2502\n",
    "\\u2502  \\u2022 LM linguistic features (NB03)                         \\u2502\n",
    "\\u2502  \\u2022 Fine-tuned LLM embeddings (NB07)                      \\u2502\n",
    "\\u2502  \\u2022 Cost: ~$50/M records (GPU inference)                  \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 Per-document feature vectors\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Stage 5: Temporal Aggregation                    \\u2502\n",
    "\\u2502  \\u2022 Group by (ticker, era_week)                           \\u2502\n",
    "\\u2502  \\u2022 Exponential decay weighting                           \\u2502\n",
    "\\u2502  \\u2022 Statistics: mean, std, min, max, count                \\u2502\n",
    "\\u2502  \\u2022 Point-in-time enforcement                             \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u252c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "                       \\u2502 Per-ticker, per-era feature vectors\n",
    "                       \\u25bc\n",
    "\\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\n",
    "\\u2502          Numerai Signals Submission                       \\u2502\n",
    "\\u2502  \\u2022 5,000 tickers \\u00d7 N features                           \\u2502\n",
    "\\u2502  \\u2022 Rank-normalized to [0, 1]                             \\u2502\n",
    "\\u2502  \\u2022 Weekly submission                                      \\u2502\n",
    "\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Interview Talking Points\n",
    "\n",
    "### Why I Can Do This\n",
    "- **NDIF** (ICLR 2025): I built a distributed inference platform using Ray, AWS, and VLLM that handles thousands of concurrent probing requests. Common Crawl processing is a similar distributed data pipeline.\n",
    "- **NeuroData MRI Pipeline**: Optimized a Kubernetes + AWS Batch pipeline for processing neuroimaging data. Halved runtime, cut cloud costs 40%. Same principles apply: parallelism, fault tolerance, cost optimization.\n",
    "- **Scale mindset**: I've worked with petabyte-scale data (neuroimaging), distributed training, and production ML systems.\n",
    "\n",
    "### Key Engineering Challenges\n",
    "1. **Deduplication**: Common Crawl has many duplicate pages across monthly crawls. Need MinHash/SimHash dedup.\n",
    "2. **Language filtering**: Most of CC is English, but need to handle multilingual content for global stocks.\n",
    "3. **Rate limiting**: If supplementing CC with live API calls, respect rate limits.\n",
    "4. **Storage**: Processed features for 5,000 stocks x 52 weeks x N features = manageable, but raw text is huge.\n",
    "5. **Monitoring**: Need observability for a pipeline this complex (logging, alerting, data quality checks).\n",
    "\n",
    "### Technology Stack I'd Use\n",
    "| Component | Technology | Why |\n",
    "|-----------|-----------|-----|\n",
    "| Orchestration | Ray / Spark | Distributed processing, fault tolerance |\n",
    "| Storage | S3 + Parquet | Columnar format, cheap storage |\n",
    "| Compute | AWS Batch / EC2 | Spot instances for cost savings |\n",
    "| GPU inference | VLLM / TGI | Batched inference for embeddings |\n",
    "| Monitoring | W&B / MLflow | Already experienced with both |\n",
    "| Scheduling | Airflow / Prefect | Weekly pipeline runs |\n",
    "\n",
    "### Extensions (TODO)\n",
    "- [ ] Download and process a real CC WARC segment (warcio + boto3)\n",
    "- [ ] Implement MinHash deduplication for cross-crawl dedup\n",
    "- [ ] Build a financial content classifier (fine-tune BERT on financial vs non-financial)\n",
    "- [ ] Add comprehensive entity linking with fuzzy matching (fuzzywuzzy + company database)\n",
    "- [ ] Estimate actual AWS costs for monthly processing\n",
    "- [ ] Set up an Airflow DAG for weekly pipeline execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}