{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Crawl \u2192 Financial Features Pipeline\n",
    "\n",
    "## Context\n",
    "This notebook directly addresses the Numerai job description, which mentions \"processing Common Crawl data\" as a core responsibility. Common Crawl is the largest publicly available web crawl dataset \u2014 petabytes of HTML from billions of web pages.\n",
    "\n",
    "The challenge: **extract financially relevant text from this haystack, map it to stock tickers, and produce clean features \u2014 all while maintaining strict point-in-time correctness.**\n",
    "\n",
    "## My Infrastructure Experience\n",
    "- **NDIF/NNsight** (ICLR 2025): Built Ray GCS Service backend with AWS object storage and VLLM for inference. Scaled to handle thousands of concurrent model probing requests.\n",
    "- **NeuroData Lab**: Optimized a diffusion MRI pipeline with Kubernetes, Docker, and AWS Batch. Halved runtime, cut cloud costs 40%.\n",
    "- **Creyon Bio**: Scaled data processing pipelines for protein sequence analysis.\n",
    "\n",
    "## Pipeline Architecture\n",
    "```\n",
    "Common Crawl (WARC files, ~3TB/month)\n",
    "    \u2192 Filter: financial content classifier\n",
    "    \u2192 Parse: extract clean text from HTML\n",
    "    \u2192 Entity Link: map text \u2192 stock tickers\n",
    "    \u2192 Feature Extract: sentiment, embeddings, linguistic features (NB01-07)\n",
    "    \u2192 Temporal Aggregate: daily text \u2192 weekly features (match Numerai eras)\n",
    "    \u2192 Output: per-ticker feature vectors for Numerai Signals\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Common Crawl Data\n",
    "\n",
    "Common Crawl stores web pages in WARC (Web ARChive) format. Each monthly crawl contains:\n",
    "- ~3 billion web pages\n",
    "- ~200-400 TB compressed\n",
    "- Available for free on S3: `s3://commoncrawl/`\n",
    "\n",
    "Each WARC record contains:\n",
    "- URL of the crawled page\n",
    "- HTTP response headers\n",
    "- HTML content\n",
    "- Crawl timestamp (critical for point-in-time correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulated Common Crawl WARC records\n# In production: use warcio library to read actual WARC files from S3\n# Example: aws s3 ls s3://commoncrawl/crawl-data/CC-MAIN-2024-10/segments/\n\n# TODO: implement\n..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step 1: Filter for Financial Content\n",
    "Most of Common Crawl is irrelevant (recipes, sports, etc). We need a fast filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Financial content filtering strategy:\n# Level 1: URL-based filtering (fast, no parsing needed)\n# Level 2: Keyword-based filtering (requires text extraction)\n# Level 3: ML classifier (most accurate, most expensive)\ndef is_financial_url(url):\n    \"\"\"Level 1: Fast URL-based filter.\"\"\"\n    ...\n\ndef is_financial_content(text, threshold=3):\n    \"\"\"Level 2: Keyword-based content filter.\"\"\"\n    ...\n\ndef filter_pipeline(record):\n    \"\"\"Multi-level financial content filter.\"\"\"\n    ...\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 2: Parse HTML \u2192 Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_text(html):\n    \"\"\"Extract clean text from HTML. In production, use BeautifulSoup or trafilatura.\"\"\"\n    ...\n\ndef extract_title(html):\n    \"\"\"Extract title from HTML.\"\"\"\n    ...\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 3: Entity Linking (Text \u2192 Stock Tickers)\n",
    "Map company mentions to stock tickers. This is a critical and non-trivial step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Entity linking: company name \u2192 stock ticker\n# In production: use a comprehensive database + fuzzy matching + NER\ndef link_entities(text):\n    \"\"\"Map company mentions in text to stock tickers.\"\"\"\n    ...\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 4: Point-in-Time Correctness (CRITICAL)\n",
    "This is the most important engineering constraint. Features must only use information available at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Point-in-time correctness demonstration\n# \n# KEY RULE: Use the CRAWL TIMESTAMP, not the publication date\n# Why? The crawl timestamp is when the information was definitely available\n# Publication dates can be:\n#   - Missing or unreliable\n#   - Backdated\n#   - Different from when the content was actually accessible\n#\n# Numerai eras are weekly (Friday close). Features for era T must only use\n# data crawled BEFORE era T's prediction deadline.\ndef assign_to_era(crawl_timestamp, era_schedule):\n    \"\"\"Assign a document to the latest era it's available for.\"\"\"\n    ...\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 5: Temporal Aggregation\n",
    "Aggregate text features per ticker per era, with exponential decay weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Temporal aggregation with exponential decay\n# Recent documents are weighted more heavily than older ones\ndef exponential_decay_weight(days_old, half_life=7):\n    \"\"\"Weight that halves every `half_life` days.\"\"\"\n    ...\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scaling calculations for Common Crawl processing\n\n# TODO: implement\n..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Interview Talking Points\n",
    "\n",
    "### Why I Can Do This\n",
    "- **NDIF** (ICLR 2025): I built a distributed inference platform using Ray, AWS, and VLLM that handles thousands of concurrent probing requests. Common Crawl processing is a similar distributed data pipeline.\n",
    "- **NeuroData MRI Pipeline**: Optimized a Kubernetes + AWS Batch pipeline for processing neuroimaging data. Halved runtime, cut cloud costs 40%. Same principles apply: parallelism, fault tolerance, cost optimization.\n",
    "- **Scale mindset**: I've worked with petabyte-scale data (neuroimaging), distributed training, and production ML systems.\n",
    "\n",
    "### Key Engineering Challenges\n",
    "1. **Deduplication**: Common Crawl has many duplicate pages across monthly crawls. Need MinHash/SimHash dedup.\n",
    "2. **Language filtering**: Most of CC is English, but need to handle multilingual content for global stocks.\n",
    "3. **Rate limiting**: If supplementing CC with live API calls, respect rate limits.\n",
    "4. **Storage**: Processed features for 5,000 stocks x 52 weeks x N features = manageable, but raw text is huge.\n",
    "5. **Monitoring**: Need observability for a pipeline this complex (logging, alerting, data quality checks).\n",
    "\n",
    "### Technology Stack I'd Use\n",
    "| Component | Technology | Why |\n",
    "|-----------|-----------|-----|\n",
    "| Orchestration | Ray / Spark | Distributed processing, fault tolerance |\n",
    "| Storage | S3 + Parquet | Columnar format, cheap storage |\n",
    "| Compute | AWS Batch / EC2 | Spot instances for cost savings |\n",
    "| GPU inference | VLLM / TGI | Batched inference for embeddings |\n",
    "| Monitoring | W&B / MLflow | Already experienced with both |\n",
    "| Scheduling | Airflow / Prefect | Weekly pipeline runs |\n",
    "\n",
    "### Extensions (TODO)\n",
    "- [ ] Download and process a real CC WARC segment (warcio + boto3)\n",
    "- [ ] Implement MinHash deduplication for cross-crawl dedup\n",
    "- [ ] Build a financial content classifier (fine-tune BERT on financial vs non-financial)\n",
    "- [ ] Add comprehensive entity linking with fuzzy matching (fuzzywuzzy + company database)\n",
    "- [ ] Estimate actual AWS costs for monthly processing\n",
    "- [ ] Set up an Airflow DAG for weekly pipeline execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}